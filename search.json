[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "",
    "text": "Syllabus"
  },
  {
    "objectID": "index.html#land-acknowledgement",
    "href": "index.html#land-acknowledgement",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Land Acknowledgement",
    "text": "Land Acknowledgement\nAlthough our students come from many locations around the world, we wish to recognize the land on which the University of Toronto was built. This land has historically been and still is the the home of the Huron-Wendat, the Seneca, and the Mississaugas of the Credit River.\nThere is a First Nations House for Indigenous Student Services on campus. Please refer to their web page for more resources and information about honouring our land and their services for students."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Course Overview",
    "text": "Course Overview\nThis course covers statistical and data analysis, reproducible quantitative methods, and scientific computing in R to answer questions in ecology and evolutionary biology. Statistical and data analysis, modeling, and computing are essential skills for all biologists. This course is designed to meet a growing demand for reproducible, openly accessible, analytically thorough, and well-documented science. Students will learn to analyze and visualize data, develop mathematical models, and document their research using the R programming language. No prerequisite programming experience is required.\nPrerequisites: BIO220H1 and one of EEB225H1, STA288H1, or STA220H1\n\nTime\nTue and Thu 2:10 - 4:00 PM EST.\n\n\nClass Locations\nRW109 (Ramsay Wright first floor computer lab), St. George Campus.\n\n\n\nOffice hours (in EST)\n\n\n\n\n\n\nMete\nWeds 11-12pm\nESC3044\n\n\nVicki\nMon 11-12pm\nESC3044\n\n\nZoe\nThurs 4-5pm\nRW109\n\n\nJessie\nTues 4-5pm\nRW109\n\n\n\n\n\nContact protocol\nPlease address all course-related and project issues to both Vicki and Mete, with the exception that questions regarding assignments should be addressed to Zoe and Jessie. Prefix the subject matter with “EEB313”. If you do not receive a reply within 48 hours (excluding weekends), please send us a reminder."
  },
  {
    "objectID": "index.html#diversity-and-inclusion-statement",
    "href": "index.html#diversity-and-inclusion-statement",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Diversity and inclusion statement",
    "text": "Diversity and inclusion statement\nAs students, you all have something unique and special to offer to science. It is our intent that students from all backgrounds and perspectives be well served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that students bring to this class be recognized as a resource, strength, and benefit.\nDiversity can refer to multiple ways that we identify ourselves, including but not limited to race, national origin, language, cultural heritage, physical ability, neurodiversity, age, sexual orientation, gender identity, religion, and socio-economic class. Each of these varied, and often intersecting, identities, along with many others not mentioned here, shape the perspectives we bring to this class, to this department, and to the greater EEB community. We will work to promote diversity, equity, and inclusion not only because diversity fuels excellence and innovation, but because we want to pursue justice.\nWe expect that everybody in this class will respect each other, and demonstrate diligence in understanding how other people’s perspectives, behaviors, and worldviews may be different from their own. Racist, sexist, colonialist, homophobic, transphobic, and other abusive and discriminatory behavior and language will not be tolerated in this class and will result in disciplinary action, such as removal from class session or revocation of group working privileges. Please consult the University of Toronto Code of Student Conduct for details on unacceptable conduct and possible sanctions.\nPlease let us know if something said or done in this class, by either a member of the teaching team or other students, is particularly troubling or causes discomfort or offense. While our intention may not be to cause discomfort or offense, the impact of what happens throughout the course is not to be ignored and is something that we consider to be very important and deserving of attention. If and when this occurs, there are several ways to alleviate some of the discomfort or hurt you may experience:\n\nDiscuss the situation privately with a member of the teaching team. We are always open to listening to students’ experiences, and want to work with students to find acceptable ways to process and address the issue.\nNotify us of the issue through another source such as a trusted faculty member or a peer. If for any reason you do not feel comfortable discussing the issue directly with us, we encourage you to seek out another, more comfortable avenue to address the issue.\nContact the Anti-Racism and Cultural Diversity Office to report an incident and receive complaint resolution support, which may include consultations and referrals.\n\nWe acknowledge our imperfections while we also fully commit to the work, inside and outside of our classrooms, of building and sustaining a community that increasingly embraces these core values. Your suggestions and feedback are encouraged and appreciated. Please let us know ways to improve the effectiveness of the course for you personally or for other students or student groups."
  },
  {
    "objectID": "index.html#wellness-statement",
    "href": "index.html#wellness-statement",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Wellness statement",
    "text": "Wellness statement\nWe on the teaching team value your health and wellness. In order to succeed in this class, in university, and beyond, you must balance your work with rest, exercise, and attention to your mental and physical health. Working until exhaustion is NOT a badge of honor. If you are finding it difficult to balance your health and well-being with your work in this class, please do not hesitate to let us know. We are happy to help connect you with resources and services on campus and also to make accommodations to our course plan as needed. Our inboxes are always open, and we are also available for virtual chats by appointment. You have our support, and we believe in you."
  },
  {
    "objectID": "index.html#absence-policy",
    "href": "index.html#absence-policy",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Absence policy",
    "text": "Absence policy\nIf you are feeling unwell, please do not come to class. Instead, take the time to recover fully. Please let us know if you are feeling sick - you will not be penalized for missing a lecture, and we will do our best to ensure that you are up-to-date with class materials when you return."
  },
  {
    "objectID": "index.html#accessibility-needs",
    "href": "index.html#accessibility-needs",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Accessibility needs",
    "text": "Accessibility needs\nIf you require accommodations for a disability, or have any accessibility concerns about the course or course materials, please notify your course instructors (Mete and Vicki), or contact Accessibility Services, as soon as possible regarding accommodations."
  },
  {
    "objectID": "index.html#course-learning-outcomes",
    "href": "index.html#course-learning-outcomes",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Course learning outcomes",
    "text": "Course learning outcomes\n\nDevelop proficiency in the programming language R.\nUse R to apply statistical tools to analyze and interpret data.\nDevelop an understanding of mathematical models.\nDevelop proficiency in using the command line and Git.\nIntegrate appropriate techniques to analyze a variety of data types and formats.\nLearn and use techniques and best practices for reproducible, high-quality science.\nLearn how to work as part of a research team to produce a scientific product.\n\n\nLecture schedule\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nInstructor\n\n\n\n\n1\nSep 7\nIntro to course, programming, RStudio\nVicki\n\n\n2\nSep 12\nR Markdown, project workflows\nVicki\n\n\n2\nSep 14\nBase R: Assignment, vectors, functions\nMete\n\n\n3\nSep 19\nData frames, intro to dplyr.\nVicki\n\n\n3\nSep 21\nData wrangling in dplyr\nVicki\n\n\n4\nSep 26\nData visualization in ggplot\nVicki\n\n\n4\nSep 28\nExploratory data analysis\nZoe\n\n\n5\nOct 03\nIntroduction to statistical inference\nMete\n\n\n5\nOct 05\nSimple linear models and generalized linear models\nMete\n\n\n6\nOct 10\nMixed models\nMete\n\n\n6\nOct 12\nMixed models & model selection\nVicki/Mete\n\n\n7\nOct 17\nMultivariate statistics\nJessie\n\n\n7\nOct 19\nIntro to command line and GitHub\nMete/Vicki\n\n\n8\nOct 24\nModel averaging & linear model review\nMete\n\n\n8\nOct 26\nMathematical models in ecology and evolution I\nMete\n\n\n9\nOct 31\nMathematical models in ecology and evolution II\nMete\n\n\n9\nNov 02\nWrap-up, review\nZoe/Jessie\n\n\n10\nNov 07\nReading break\n-\n\n\n10\nNov 09\nReading break\n-\n\n\n11\nNov 14\nProject work\n\n\n\n11\nNov 16\nProject work\n\n\n\n12\nNov 21\nProject work\n\n\n\n12\nNov 23\nProject work\n\n\n\n13\nNov 28\nProject work\n\n\n\n13\nNov 30\nProject work\n\n\n\n14\nDec 05\nGroup presentations\nEveryone\n\n\n\n\n\nLecture readings\nYou will find a list of recommended readings posted under “Resources”. Since there are no exams in this class, you will not be tested on the readings directly. However, we highly recommend that you go through these readings as they were chosen to help you understand lecture material better (e.g., provides context for data that was used) as well as serve as resources for you if you wish to pursue any specific topic further. We also compiled a list of open-access R and statistics resources for your reference. You can find this list in the Readings folder on Quercus.\n\n\nAssessment schedule\n\n\n\n\n\n\n\n\n\n\nAssignment\nType\nSubmitted on\nDue date\nMarks\n\n\n\n\nBasic R and dplyr\nIndividual\nQuercus\nSep 28\n8\n\n\nProject proposal\nGroup\nGitHub\nOct 03\n4\n\n\ndplyr and tidy data\nIndividual\nQuercus\nOct 05\n8\n\n\nData exploration\nIndividual\nQuercus\nOct 12\n8\n\n\nLM, GLM, & LMM\nIndividual\nQuercus\nOct 19\n8\n\n\nCommand Line\nIndividual\nGitHub\nOct 26\n8\n\n\nMid-project update\nGroup\nGitHub\nNov 02\n6\n\n\nChallenge assignment\nIndividual\nGitHub\nNov 17\n20\n\n\nPresentation\nGroup\nIn-class\nDec 05\n10\n\n\nFinal report\nGroup\nGitHub\nDec 08\n20\n\n\n\nThere are 100 marks in total. Your final course mark will be the sum of your assignment scores, which will be translated to a letter grade according to the official grading scale of the Faculty of Arts and Science.\nAssignments will be distributed and submitted in the R Markdown format via Quercus. Assignments will be handed out on Thursdays after class and are due at 8:00 PM on the following Thursday.\nThe Challenge Assignment is equivalent to a take home exam. The format will be the same as the other assignments, but this assignment is designed challenge you to go a little beyond what was taught in class. It will be distributed on 9:00 AM on Nov 13, and it will be due 11:59 PM on Nov 17. Students are welcome to work in a group on this assignment, but each student must submit their own original work. No extensions will be granted on this assignment except under the same extra-ordinary circumstances akin to those under which an exam might be deferred. We only expect you to do your best!\nAs per our stance on supporting student’s mental health, we are happy to accommodate a 72-hour extension for one of the assignments, no questions asked. Otherwise, except under extenuating circumstances, there will be a penalty of 5% per day (including weekends) for all late submissions. If you foresee needing an extension, please email both Vicki and Mete as soon as possible. This policy does not apply to the Challenge Assignment, Presentation and Final Report."
  },
  {
    "objectID": "index.html#improving-your-writing-skills",
    "href": "index.html#improving-your-writing-skills",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Improving your writing skills",
    "text": "Improving your writing skills\nEffective communication is crucial in science. The University of Toronto provides services to help you improve your writing, from general advices on effective writing to writing centers and writing courses. The Faculty of Arts & Science also offers an English Language Learning (ELL) program, which provides free individualized instruction in English skills. Take advantage of these!"
  },
  {
    "objectID": "index.html#academic-integrity",
    "href": "index.html#academic-integrity",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "Academic integrity",
    "text": "Academic integrity\nYou should be aware of the University of Toronto Code of Behaviour on Academic Matters. Also see How Not to Plagiarize. Notably, it is NOT appropriate to use large sections from internet sources, and inserting a few words here and there does not make it an original piece of writing. Be careful in using internet sources – most online material are not reviewed and there are many errors out there. Make sure you read material from many sources (published, peer-reviewed, trusted internet sources) and that you write an original text using this information. Always cite your sources. In case of doubt about plagiarism, talk to your instructors and TAs. Please make sure that what you submit for the final project does not overlap with what you submit for other classes, such as the 4th-year research project.\n\nOn the use of Generative AI\nThe knowing use of generative artificial intelligence tools, including ChatGPT and other AI writing and coding assistants, for the completion of, or to support the completion of, the assignments, the challenge assignment, or the final project is prohibited and may be considered an academic offense."
  },
  {
    "objectID": "index.html#fas-student-engagement-programs",
    "href": "index.html#fas-student-engagement-programs",
    "title": "EEB313: Quantitative Methods in R for Biology",
    "section": "FAS student engagement programs",
    "text": "FAS student engagement programs\nThere are a few programs on campus aimed at increasing student engagement with their coursework and keeping them socially connected. Recognized Study Groups are voluntary, peer-led study groups of up to 8 students enrolled in the same course. Meet to Complete are online drop-in study sessions for A&S undergrads. These are worth checking out if you are interested in participating in a study group."
  },
  {
    "objectID": "about-us.html#course-instructors",
    "href": "about-us.html#course-instructors",
    "title": "The 2023 teaching team",
    "section": "Course instructors",
    "text": "Course instructors\nMete Yuksel (mete.yuksel@mail.utoronto.ca)\nMete is a 2nd year PhD student in the Dept. Ecology and Evolutionary Biology at UTSG, and is co-advised by Matt Osmond & Nicole Mideo. He uses mathematical models to address questions in evolutionary genetics and ecology. He is currently developing theory to understand patterns, causes, and consequences of pathogen recombination. Mete was an undergraduate math & statistics student at the University of Idaho. There, he worked on the ecology of gene drive interventions against vectored diseases, understanding how continuous spatial structure can affect species coexistence, and forecasting the dynamics of Chinook salmon in the Willamette River system. He loves modeling, statistics, and programming in R (among other languages!) - and is thrilled to be teaching this course! Outside of science, he enjoys listening to podcasts, cooking, and cycling between Toronto neighborhoods in search of coffee.\nVicki Zhang (vm.zhang@mail.utoronto.ca)\nVicki is a PhD Candidate in Peter Kotanen’s lab at UTM. She studies invasive species at the edge of the Arctic, and how climate change and anthropogenic stressors might affect the persistence and spread of invasive species in Canada’s tundra and boreal forests. Vicki does fieldwork in Churchill, Manitoba (the polar bear capital of the world) in the summer, but does a lot of data analyses and modelling on R throughout the fall and winter semesters. She took EEB313 in 2018, and it was her favourite undergraduate course because everything about R just clicked, and she learned so much about coding, statistics, and modelling (her previous encounters with R and stats went very poorly). Now, she spends a lot of time making sure that her data is clean and her code is fast - it’s so satisfying when every line finally runs! For Vicki, the best way to work in R is with a big cup of coffee, soft classical music in the background, and her cat Remi napping next to her."
  },
  {
    "objectID": "about-us.html#teaching-assistants",
    "href": "about-us.html#teaching-assistants",
    "title": "The 2023 teaching team",
    "section": "Teaching assistants",
    "text": "Teaching assistants\nZoe Humphries (zoe.humphries@mail.utoronto.ca)\nZoë is a PhD student in the Wright and Barrett labs at UTSG. She studies the genome of a weedy plant to better understand how transposable elements affect sex chromosome evolution. She taught herself how to use R during her honours thesis and fell in love because it was much kinder than Lisp or C++ and, most importantly, because aesthetics. Zoë makes plots from aggressively large genomic data sets and spends a lot of time literally bash-ing her data into a file small enough for her computer to load into RStudio. For Zoë, the best way to work in R is while patting her puppies.\nJessie Wang (jae.wang@mail.utoronto.ca)\nJessie is a PhD student in the Frederickson lab at UTSG. She studies plant-microbe interactions using high-throughput experimentation in duckweeds. She fell in love with R during her time as an undergraduate and took EEB313 in 2020, simultaneously sharpening her coding skills while conducting research alone in the lab. Jessie loves to spend too much money on fancy coffee as she types away, making sure her code is well-annotated and her figures look beautiful. Outside of work, she enjoys caring for her many houseplants and aquariums, finding new delicious eats, and admiring other people’s pets."
  },
  {
    "objectID": "downloadingR.html#introduction",
    "href": "downloadingR.html#introduction",
    "title": "Downloading R",
    "section": "Introduction",
    "text": "Introduction\nThis course makes extensive use of R and RStudio. If you have any issues installing R or RStudio, let’s resolve them in advance so that you can make the most of our time together! As you follow these instructions, don’t worry if you don’t understand exactly what is going on - we will go through it all again during the first lecture! The goal here is to ensure that your computer is set up and ready for action. If you can’t complete these steps, please email Vicki and Mete as soon as possible."
  },
  {
    "objectID": "downloadingR.html#tech-requirements",
    "href": "downloadingR.html#tech-requirements",
    "title": "Downloading R",
    "section": "Tech Requirements",
    "text": "Tech Requirements\nFirst, make sure that your personal computer meets meets these requirements:\n\n1024×768 screen resolution,\n1.5GHz CPU,\n2GB RAM,\n10GB free disk space,\na recent version of Windows, Mac OS, or Linux is installed.\n\nNext, check that a robust internet browser such as Firefox, Safari or Google is installed. (Internet Explorer will not work.) Finally, make that a PDF viewer (e.g., Adobe Acrobat, Preview) is installed. It is fine if no PDF viewer is installed if you can view PDFs on your web browser."
  },
  {
    "objectID": "downloadingR.html#download-r-and-rstudio",
    "href": "downloadingR.html#download-r-and-rstudio",
    "title": "Downloading R",
    "section": "Download R and RStudio",
    "text": "Download R and RStudio\n\nDownload R, a free software environment for statistical computing and graphics from CRAN, the Comprehensive R Archive Network. We recommend you install a precompiled binary distribution for your operating system – use the links up at the top of the CRAN page!\n\nNote: MacBook users with an Apple Silicon chip (e.g., M1 or M2) should install the “arm64” version of R, while MacBook users with an Intel chip should install the regular (64-bit) version of R. You can check your laptop’s hardware specifications by clicking the Apple icon (top left corner) \\&gt; About This Mac.\n\nInstall RStudio, a graphical user interface (GUI) for R. Click the link under “2: Install RStudio”. RStudio comes with a text editor, so there is no immediate need to install a separate stand-alone editor.\n\nIf R is already installed, ensure that the R version is 4.0 or higher. You can do this by opening RStudio, where you should see a multi-section window like below. Locate the quadrant named “Console”, and put your cursor at the start of the prompt indicated by the &gt; symbol and where the red arrow is pointing. Type or copy sessionInfo() - make sure that only the I at the start of Info is capitalized and you are including the round brackets. Press enter to run this command and R should return an output to you. The first line shows what version of R is installed. Ensure that the R version installed is at least 4.0."
  },
  {
    "objectID": "downloadingR.html#download-packages",
    "href": "downloadingR.html#download-packages",
    "title": "Downloading R",
    "section": "Download Packages",
    "text": "Download Packages\n\nTinyTex\nThere is one package we have to install first before we can create PDF reports, which will be necessary for assignments and the project. Copy and paste into the console (where the \\&gt; symbol is) the two lines of code below to install a package called tinytex.\n\ninstall.packages(\"tinytex\") \ntinytex::install_tinytex()\n\n\n\nAll packages\n\nCopy and paste the below code into your console.\n\n\ninstall.packages(c(\"tidyverse\", \"here\", \"knitr\", \"datasauRus\", \"car\",\n                   \"lme4\", \"lmerTest\", \"ggalt\", \"MuMIn\", \"sjmisc\",\n                   \"lsmeans\", \"ggfortify\", \"vegan\"), dependencies = TRUE)\n\nDuring installation, if you ever get the below message, click “No”.\n\nIf you get the message “Do you want to install from sources the packages which need compilation? (Yes/no/cancel)” in the Console, type “Yes” and press enter.\n\nCheck that these packages have been installed correctly. To do this, go to the bottom right pane and click the tab for “Packages”. If you can search for and find the below packages, then they have been installed! They do not need to be checked off. Alternatively, go to the Console and type library(lme4) to verify that the lme4 package is installed. An error along the lines “there is no package called lme4” will be returned if the package is not installed."
  },
  {
    "objectID": "lec00-rstudio.html#lesson-preamble",
    "href": "lec00-rstudio.html#lesson-preamble",
    "title": "1  Getting started with RStudio and R Notebook",
    "section": "Lesson preamble",
    "text": "Lesson preamble\n\n1.0.1 Lesson objectives\n\nIntroduce students to the RStudio interface\nIntroduce the Markdown syntax and how to use it within the R Notebook\n\n1.0.2 Learning outline\n\nExplore RStudio interface (20 mins)\nRMarkdown (20 mins)\nGenerating reports (10 mins)\nKnit to PDF and submit on Quercus (10 mins)"
  },
  {
    "objectID": "lec00-rstudio.html#working-with-computers",
    "href": "lec00-rstudio.html#working-with-computers",
    "title": "1  Getting started with RStudio and R Notebook",
    "section": "Working with computers",
    "text": "Working with computers\nBefore we get into more practical matters, we want to provide a brief background to the idea of working with computers. Essentially, computer work is about humans communicating with a computer by modulating flows of current in the hardware in order to get the computer to carry out advanced calculations that we are unable to efficiently compute ourselves. Early examples of human computer communication were quite primitive and included physically disconnecting a wire and connecting it again in a different spot. Luckily, we are not doing this anymore; instead we have programs with graphical user interfaces with menus and buttons that enable more efficient human to computer communication.\n\n1.0.3 Graphical user interfaces vs. text based user interfaces\nAn example of such a program that many of you are familiar with is spreadsheet software such as Microsoft Excel. Here, all the functionality of the program is accessible via hierarchical menus, and clicking buttons sends instructions to the computer, which then responds and sends the results back to your screen. For instance, I can click a button to send the instruction of coloring this cell yellow, and the computer interprets my instructions and then displays the results on the screen.\nSpreadsheet software is great for viewing and entering small data sets and creating simple visualizations fast. However, it can be tricky to design figures, create automatic reproducible analysis workflows, perform advanced calculations, and reliably clean data sets. Even when using a spreadsheet program to record data, it is often beneficial to have some some basic programming skills to facilitate the analyses of those data.\nTyping commands directly instead of searching for them in menus is a more efficient approach to communicate with the computer and a powerful way of doing data analysis. This is initially intimidating for almost all people, but if you compare it to learning a new language, the process becomes more intuitive: when learning a language, you would initially string together sentences by looking up individual words in the dictionary. As you improve, you will only reference the dictionary occasionally since you already know most of the words. Eventually, you will throw the dictionary out altogether because it is faster and more precise to speak directly. In contrast, graphical programs force you to look up every word in the dictionary every time, even if you already know what to say.\n\n\n1.0.4 RStudio and the R Notebook\nRStudio includes the R console, but also many other convenient functionalities, which makes it easier to get started and to work with R. When you launch RStudio, you will see four panels. Starting at the top left and going clockwise, these panels are:\n\nThe text editor panel. This is where we can write scripts, i.e. putting several commands of code together and saving them as a text document so that they are accessible for later and so that we can execute them all at once by running the script instead of typing them in one by one.\nThe environment panel, which shows us all the files and objects we currently loaded into R.\nThe files-plots-help panel. This panel shows the files in the current directory (the folder we are working out of), any plots we make later, and also documentation for various packages and functions. Here, the documentation is formatted in a way that is easier to read and also provides links to the related sections.\nThe console is another space we can input code, only now the code is executed immediately and doesn’t get saved at the end.\n\nTo change the appearance of your RStudio, navigate to Tools &gt; Global Options &gt; Appearance. You can change the the font and size, and the editor theme. The default is “Textmate”, but if you like dark mode, I recommend “Tomorrow Night Bright”. You can also change how your panels are organized. I like to have my Console and history below my Source, and that way I can see my working environment next to my code. That way, I know if an error I am getting is because I am missing an object or I renamed something oddly. Let’s change that now. I recommend playing around with the appearance if you prefer a different layout or colour scheme. Do what makes you the most productive!\nAnother very useful thing with RStudio is that you have access to some excellent cheat sheets in PDF format straight from the menu: Help -&gt; Cheatsheets!\nIn the RStudio interface, we will be writing code in a format called the R Notebook. As the name entails, this interface works like a notebook for code, as it allows us to save notes about what the code is doing, the code itself, and any output we get, such as plots and tables, all together in the same document.\nWhen we are in the Notebook, the text we write is normal plain text, just as if we would be writing it in a text document. If we want to execute some R code, we need to insert a code chunk.\nYou insert a code chunk by either clicking the “Insert” button or pressing Ctrl/Command + Alt + i simultaneously. You could also type out the surrounding backticks, but this would take longer. To run a code chunk, you press the green arrow, or Ctrl/Command + Shift + Enter.\n\n1+1\n#&gt; [1] 2\n\nAs you can see, the output appears right under the code block.\nThis is a great way to perform explore your data, since you can do your analysis and write comments and conclusions right under it all in the same document. A powerful feature of this workflow is that there is no extra time needed for code documentation and note-taking, since you’re doing your analyses and taking notes at the same time. This makes it great for both taking notes at lectures and to have as a reference when you return to your code in the future."
  },
  {
    "objectID": "lec00-rstudio.html#r-markdown",
    "href": "lec00-rstudio.html#r-markdown",
    "title": "1  Getting started with RStudio and R Notebook",
    "section": "1.1 R Markdown",
    "text": "1.1 R Markdown\nThe text format we are using in the R Notebook is called R Markdown. This format allows us to combine R code with the Markdown text format, which enables the use of certain characters to specify headings, bullet points, quotations and even citations. A simple example of how to write in Markdown is to use a single asterisk or underscore to emphasize text (*emphasis*) and two asterisks or underscores to strongly emphasize text (**strong emphasis**). When we convert our R Markdown text to other file formats, these will show up as italics and bold typeface, respectively. If you have used WhatsApp, you might already be familiar with this style of writing. In case you haven’t seen it before, you have just learned something about WhatsApp in your quantitative methods class…\nTo learn more about R Markdown, you can read the cheat sheets in RStudio and RStudio Markdown reference online.\n\n1.1.1 Saving data and generating reports\nTo save our notes, code, and graphs, all we have to do is to save the R Notebook file, and the we can open it in RStudio next time again. However, if we want someone else to look at this, we can’t always just send them the R Notebook file, because they might not have RStudio installed. Another great feature of R Notebooks is that it is really easy to export them to HTML, MS word, or PDF documents with figures and professional typesetting. There are actually many academic papers that are written entirely in this format and it is great for assignments and reports. (You might even use it to communicate with your collaborators!) Since R Notebook files convert to HTML, it is also easy to publish simple and good-looking websites in it, in which code chunks are embedded nicely within the text.\nLet’s try to create a document in R.\nFirst, let’s set up the YAML block. This is found at the top of your document, and it is where you specify the title of your document, what kind of output you want, and a few other things such as author list and date.\n\n---\ntitle: \"Your title here\"\nauthor: \"Your name here\"\ndate: \"Insert date\"\n---\n\nThen, let’s type some notes and code together!\n\n# Attempt 1\n\n## Here goes!\n\n\n1+2+3+4\n#&gt; [1] 10\n\nx &lt;- seq(0,100,1)\n\nplot(x, type = \"l\")\n\n\n\n\n\n\n\n\nLet’s see what this looks like. To create the output document, we poetically say that we will knit our R Markdown into the HTML document. Luckily, it is much simpler than actually knitting something. Simply press the Knit button here and the new document will be created.\nAs you can see in the knitted document, the title showed up as we would expect, the lines with pound sign(s) in front of them were converted into headers and we can see both the code and its output! So the plots are generated directly in the report without us having to cut and paste images! If we change something in the code, we don’t have to find the new images and paste it in again, the correct one will appear right in your code.\nWhen you quit, R will ask you if you want to save the workspace (that is, all of the variables you have defined in this session); in general, you should say “no” to avoid clutter and unintentional confusion of results from different sessions. Note: When you say “yes” to saving your workspace, it is saved in a hidden file named .RData. By default, when you open a new R session in the same directory, this workspace is loaded and a message informing you so is printed: [Previously saved workspace restored]."
  },
  {
    "objectID": "lec00-rstudio.html#exercise",
    "href": "lec00-rstudio.html#exercise",
    "title": "1  Getting started with RStudio and R Notebook",
    "section": "Exercise",
    "text": "Exercise\n\n1.1.2 Knitting and Submitting on Quercus\nPractice knitting and uploading your file to Quercus!\nClick the dropdown “Knit” button at the top of the screen, and click “PDF”.\nNote: for assignments, submit PDF versions. If you are having trouble rendering your knitted file, you can submit HTML formats, or your .Rmd file as a last resort. Note that, if you are unable to knit your assignment, chances are there is an error. Make sure to double-check your code!\nHead on over to Quercus and submit your knitted PDF to “Assignment 0”."
  },
  {
    "objectID": "lec01-markdown-workflows.html#lesson-preamble",
    "href": "lec01-markdown-workflows.html#lesson-preamble",
    "title": "2  Markdown, project workflows",
    "section": "2.1 Lesson preamble",
    "text": "2.1 Lesson preamble\n\n2.1.1 Lesson objectives\n\nLearn about the Markdown syntax and how to use it within the R Notebook.\nSet up the R Notebook\nSet up an R Project\nUnderstand how to follow the conventional data science workflow\n\n\n\n2.1.2 Learning outline\n\nExplore RStudio interface (10 mins)\nReproducible science with Markdown (40 mins)\nProject-Oriented Workflow (20 mins)\nReproducible Science (10 mins)\nThe Scientific Method (10 mins)\nFind your groups! (10 mins)"
  },
  {
    "objectID": "lec01-markdown-workflows.html#markdown",
    "href": "lec01-markdown-workflows.html#markdown",
    "title": "2  Markdown, project workflows",
    "section": "2.2 Markdown",
    "text": "2.2 Markdown\nLast class, we introduced Markdown, the text format that RStudio uses. In fact, if you are following along this far, you should have already been typing your notes and code in a .Rmd file, or a RMarkdown file.\nLet’s introduce more Markdown features! We encourage groups to try completing their group projects in RMarkdown, but more about that later.\n\n2.2.1 In-line code\nWe talked about using RMarkdown for notes and inline code last class. This is extremely useful for when you need to update objects or rerun analyses. You can also comment out code within the code chunk.\nAs a reminder, the keyboard shortcut to insert a code chunk is:\n\ncommand + option + i (Mac)\nctrl + alt + i (Windows)\n\n\nx &lt;- 5\ny &lt;- 13\n\n# 5 plus 13\nx + y\n\n[1] 18\n\n\n\n2.2.1.1 R code chunk tricks\n\neval=TRUE evaluates the code chunk (runs it); eval=FALSE does not (will just print the chunk)\necho=TRUE prints the code chunk; echo=FALSE hides it but will still evaluate it\n\nthis is useful if you need to save something as an object but you don’t necessarily need it to print\n\nwarning=FALSE will hide warnings; message=FALSE will hide messages from output\n\nonly the case when eval=TRUE\n\n\n\n## eval=TRUE, echo=FALSE prints output, but does not print code chunk\nlibrary(ggplot2)\nqplot(mpg, wt, data = mtcars)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n2.2.1.2 Changing height and width of images/figures\nYou can also indicate the sizes of figures by using fig.width and fig.height in your code chunk.\n\n## in this R chuck, fig.width=2, fig.height=2\nqplot(mpg, wt, data = mtcars)\n\n\n\n\n\n\n\n2.2.2 Lists\nI’ve already used these! You can use either * or - to indicate unordered lists. To indent, hit tab. Make sure there’s a space before your items in the list.\nUnordered Lists (bullet points)\n\nfirst item\nsecond item\n\nsub-level item (two tabs)\n\nthird item\n\nFor ordered lists, use numbers. Again, hit tab to add sublists.\nOrdered Lists (numbers)\n\nOrdered item\nOrdered item\n\nOrdered sub-item\n\nItem (will automatically re-number)\n\n\n\n2.2.3 Tables\nkable displays r code chunks with tables nicely\nlibrary(knitr)\nkable(head(mtcars))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n\n\nIn comparison:\nlibrary(knitr)\nhead(mtcars) ## does not nicely format the table\n               mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1\n\n\n2.2.4 Images and Links\nUse the following format to add images:\n![Description/Caption](/path/to/image.png)\nFor example: \nAs an aside, let’s talk about rubber duck debugging. It is a simple but effective technique used by scientists to debug code. By explaining the code line-by-line to an inanimate object, such as a rubber duck, programmers can break down the code into smaller, more manageable pieces, and potentially identify the root cause of the problem. Or, explain it to a friend in the course! They can also help you find typos and other errors\nAnd use the following format to add weblinks: [Description/Caption](weblink).\nThe EEB website lives here!\n\n\n2.2.5 White Space and New Page\nUse \\vspace{2pt} to add white space, and use \\newpage to add a page break.\n\n\n2.2.6 In-line citations and bibliography\nExport a .bib file (e.g., from Zotero and Mendeley). Then, add the .bib to your YAML header.\n---\ntitle: My Report\noutput: html_document\nbibliography: bibliography.bib\n---\nNow, you can cite directly in your report, like below:\nSome fact[@Smith2018]\nSome fact supported by multiple papers [@Smith2018; @Tran1997]\n\n\n2.2.7 Footnotes\nThis is what a footnote looks like.1 Here is another.2"
  },
  {
    "objectID": "lec01-markdown-workflows.html#project-oriented-workflow",
    "href": "lec01-markdown-workflows.html#project-oriented-workflow",
    "title": "2  Markdown, project workflows",
    "section": "2.3 Project-Oriented Workflow",
    "text": "2.3 Project-Oriented Workflow\n\n2.3.1 Projects\nMaterial adapted from Posit and R-bloggers.\nRStudio projects make it straightforward to divide your work into multiple contexts, each with their own working directory, workspace, history, and source documents.\nTo create a new project in the RStudio IDE, use the Create Project command (top right). You can link your new project to an existing directory, or create a new directory entirely. Since we already have some RMarkdown written, let’s add our project to an existing directory.\nWhen a new project is created RStudio, it creates a project file (with an .Rproj extension) within the project directory. This file contains various project options (discussed below) and can also be used as a shortcut for opening the project directly from the filesystem. It also creates a hidden directory (named .Rproj.user) where project-specific temporary files (e.g., auto-saved source documents, window-state, etc.) are stored. This directory is also automatically added to .Rbuildignore, .gitignore, etc. if required.\nNow, whenever you open your project, it loads your project into RStudio and displays its name in the Projects toolbar (which is located on the far right side of the main toolbar). It even takes you back to where you were working last, so it opens all the scripts and files that were last open! You can also save your history, the objects in your environment, and an image of what your project looked like at the last time you closed it, but this can take up a lot of space if you do this each time.\nYou can work with more than one RStudio project at a time by simply opening each project in its own window of RStudio.\n\n\n2.3.2 Relative File Pathways\nThe goal of the here package is to enable easy file referencing in project-oriented workflows. In contrast to using setwd(), which is fragile and dependent on the way you organize your files, here uses the top-level directory of a project to easily build paths to files.\n\nlibrary(here)\n\nhere() starts at /Users/vicki.zhang/Documents/UofT/PhD/Teaching/2023-2024/eeb313.github.io\n\nhere()\n\n[1] \"/Users/vicki.zhang/Documents/UofT/PhD/Teaching/2023-2024/eeb313.github.io\"\n\n\nYou can build a path relative to the top-level directory in order to read or write a file:\n\nhere(\"1_lectures\", \"csv\", \"portal_data.csv\")\n\n[1] \"/Users/vicki.zhang/Documents/UofT/PhD/Teaching/2023-2024/eeb313.github.io/1_lectures/csv/portal_data.csv\"\n\n\nThese relative paths work regardless of where the associated source file lives inside your project, like analysis projects with data and reports in different subdirectories.\n\n\n2.3.3 Demo Project\nThere is no a single “right” folder structure for analysis projects. However, this is a tried-and-true simple method that works (backed up by many data scientists!).\nCreate a folder on your computer that holds all the files relevant to that particular piece of work. This parent folder will contain the project’s .Rproj and all other subfolders, so it should have the same name as your project. Pick a good one. Spending an extra 5 minutes will save you from regrets in the future. The name should be short, concise, written in lower-case, and not containing any special symbols. One can apply similar strategies as for naming packages.\nUse subdirectories to organize data, code, figures, notes, etc.\n\nThe folder data typically contains two subfolders, namely, raw and processed. The content of raw directory is data files of any kind, such as .csv, SAS, Excel, text and database files, etc. The content of this folder is read-only, so that no scripts should change the original files or create new ones inside it. For this purpose, the processed directory is used: all processed, cleaned, and tidied datasets are saved here. It is a good practice to save files in R specific format, rather than in .csv, since the saving in .csv is a less efficient way of storing data (both in terms of space and time of reading/writing).\nThe folder figures is the place where you may store plots, diagrams, and other figures. There is not much to say about it. Common extensions of such files are .eps, .png, .pdf, etc. Again, file names in this folder should be meaningful (the name img1.png does not represent anything).\nAll reports live in a directory with the corresponding name reports. These reports can be of any formats, such as LaTeX, Markdown, R Markdown, Jupyter Notebooks, etc. Currently, more and more people prefer rich documents with text and executable code to LaTeX and such.\nPerhaps the most important folder is analyses or scripts. Here, you keep all your R scripts and codes. That is the exact place to use prefix numbers, if files should be run in a particular order. If you have files in other scripted languages (e.g., Python), it is better to keep them in this folder as well. There can also an important subfolder called deprecated. Whenever you want to remove one or the other script, it is a good idea to move it to deprecated at first iteration. I recommend never deleting your code until you are sure that you have finished everything you wanted to do, even if the code doesn’t work (going back to it after a break usually helps!).\n\n\n\n\nVicki’s demo of her file directory.\n\n\nThis convention guarantees that the project is self-contained, and it can be moved around on your computer or onto other computers and will still “just work”. This is a really important convention that creates reliable, reproducible science across different computers or users and over time.\n\n\n2.3.4 Documenting session info\nAs an overview of our environment, we can also use the sessionInfo() command. This is a good practice to have at the end of your code to document which packages you used and what version they were.\n\nsessionInfo()\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] here_1.0.1    knitr_1.44    ggplot2_3.4.3\n\nloaded via a namespace (and not attached):\n [1] rstudioapi_0.15.0 magrittr_2.0.3    tidyselect_1.2.0  munsell_0.5.0    \n [5] colorspace_2.1-0  R6_2.5.1          rlang_1.1.1       fastmap_1.1.1    \n [9] fansi_1.0.4       dplyr_1.1.3       tools_4.1.1       grid_4.1.1       \n[13] gtable_0.3.4      xfun_0.40         utf8_1.2.3        cli_3.6.1        \n[17] withr_2.5.1       htmltools_0.5.6   rprojroot_2.0.3   digest_0.6.33    \n[21] tibble_3.2.1      lifecycle_1.0.3   farver_2.1.1      htmlwidgets_1.6.2\n[25] vctrs_0.6.3       glue_1.6.2        evaluate_0.21     rmarkdown_2.25   \n[29] labeling_0.4.3    compiler_4.1.1    pillar_1.9.0      generics_0.1.3   \n[33] scales_1.2.1      jsonlite_1.8.7    pkgconfig_2.0.3  \n\n\nNotice that we have some base packages active even though we did not explicitly call for them."
  },
  {
    "objectID": "lec01-markdown-workflows.html#reproducible-science",
    "href": "lec01-markdown-workflows.html#reproducible-science",
    "title": "2  Markdown, project workflows",
    "section": "2.4 Reproducible Science",
    "text": "2.4 Reproducible Science\n\nComputational reproducibility\n\ncode, software, etc.\n\nScientific reproducibility\n\ndata (able to run the same statistical analysis on the same data)\nmethod, details about how the data was collected\nN.B., field work - hard to reproduce method\n\nStatistical reproducibility\n\n\nWhy does reproducibility matter in science?\n\nMust trust findings to build on scientific progress!\n\nWhat do you think about when you hear the term “open science”?\n\nOpen science: everything about a project (data, stats, code, funding, etc.) is open and available to viewing\n\nHow does open science affect collaboration and the progress of science?\n\nWhat are the barriers to reproducibility?\n\npeople won’t care\npeople might want to steal results\nresearchers do not want to share (embarrassed, code is rushed, etc.)\n\n\n2.4.1 Reproducibility in Data Science\nMaterial from this section adapted from R for Data Science.\nThe main tools of data science are: importing, tidying, transforming, and visualizing data. We will go through all of these steps of data science so that you can tackle real datasets.\n\n\n\nData Science Workflow\n\n\n\nBefore you can transform and visualize your data, you need to first get your data into R.\nThen, you’ll learn about tidy data, a consistent way of storing your data that makes transformation, visualization, and modelling easier. You’ll learn the underlying principles, and how to get your data into a tidy form.\nYou’ll learn the key verbs that allow you to select important variables, filter out key observations, create new variables, and compute summaries.\nFinally, we will visualize by making elegant and informative plots that help you understand data.\n\nNote that if you are interested in doing a modelling project, these steps are different. Talk to Vicki and Mete if you want to tackle mathematical models!"
  },
  {
    "objectID": "lec01-markdown-workflows.html#the-scientific-method",
    "href": "lec01-markdown-workflows.html#the-scientific-method",
    "title": "2  Markdown, project workflows",
    "section": "2.5 The Scientific Method",
    "text": "2.5 The Scientific Method\n\n2.5.1 Steps in scientific process\n\nIdentify research question(s).\nLook into what the previous literature shows.\nCreate one or more hypotheses or objectives.\nWrite up an outline or expected approach to answering those questions/objectives (analysis and presentation plan):\n\n\nHow will the data be obtained and what is the data (i.e. the source)?\nWhat statistical/mathematical techniques have previous researchers used? Will you use them? Are they appropriate (optional, may need expert)? ( How will the results/data be presented or visualized (possible tables/figures)?\n\n\nRun the planned analyses (or additional ones that come may up).\nVisualize or present all results from the analyses.\nInterpret the results and how they fit with the previous literature.\nDraw conclusions based on the hypotheses/objectives.\nDisseminate your results (in blogs, pre-print archives, journals, conferences)\n\n\n\n\nSimplified diagram of the scientific method\n\n\n\n\n2.5.2 Basics of succeeding as a team\nFinal assignment is in a group and to succeed, you need to understand team dynamics:\n\nCommunication is vital to work together and to achieve the goal\nTeams go through various stages\nNeed consensus for group norms, goals, duties/responsibilities, and conduct/behaviour\nImportant that everyone has a stake in the project\nRotate roles (specifically for the leader/facilitator)\n\n\n\n2.5.3 Roles and responsibilities\n\nLeader/Facilitator’s duty:\n\nGoal is to keep things running smoothly, focused on the task, and on track for time\nKeep everyone on topic and on task; stay aware of the time\n(Try to) mediate or resolve any conflicts (there will always be some type of conflict; how it’s dealt with is what matters)\n(Try to) encourage everyone to participate and allow everyone a chance at talking\n\nRecorder’s duty:\n\nGoal is to write/type down main or important points raised or discussed when team is meeting\nKeep notes and files organized and orderly\n\nOrganizer’s duty:\n\nArrange for next meeting time and location\n\nSend reminders to members a day or two before meeting\n\nMake and email a simple agenda of tasks to do or to discuss\n\n\n\n\n2.5.4 Code of Conduct\n\nVital to establishing boundaries and expectations of being a team member\n\nHow do you want each member to treat each other?\nHow do you deal with conflict?\nWhat is acceptable and unacceptable behaviour?\n\nThese are outlined in the code of conduct\nMostly its common sense (be kind and respectful)\n\nBut its important that you as a team write out what everyone wants and agrees to\n\n\n\n2.5.4.1 Example Code of Conduct\nCheck out the Contributor Convenent and the UofT Coders Code of Conduct"
  },
  {
    "objectID": "lec01-markdown-workflows.html#projects-1",
    "href": "lec01-markdown-workflows.html#projects-1",
    "title": "2  Markdown, project workflows",
    "section": "2.6 Projects!",
    "text": "2.6 Projects!\n\n2.6.1 Exercise after groups are formed\nAs a group, complete these tasks:\n\nGet into your groups\nIntroduce one another:\n\n\nName, year of study\nOther stuff (e.g., interests, plans for next years, etc.)\n\n\nCreate a “team name”\nAssign roles to each other:\n\n\nThese will be rotated: Leader/Facilitator, Recorder, Organizer\nDiscuss how and when the roles will be rotated.\n\n\nDiscuss and brainstorm code of conduct (record it).\nTake a minute to think about your own skills\n\n\nShare these with your group (record them).\n\n\nDiscuss how responsibilities of each member will be decided on (record it).\n\n\n\n2.6.2 GitHub\nMake a GitHub account."
  },
  {
    "objectID": "lec01-markdown-workflows.html#footnotes",
    "href": "lec01-markdown-workflows.html#footnotes",
    "title": "2  Markdown, project workflows",
    "section": "",
    "text": "This is the first footnote.↩︎\nThis is the second footnote.↩︎"
  },
  {
    "objectID": "lec02-base-r.html#lesson-preamble",
    "href": "lec02-base-r.html#lesson-preamble",
    "title": "3  Base R: assignment, vectors, functions, and loops",
    "section": "3.1 Lesson Preamble",
    "text": "3.1 Lesson Preamble\n\n3.1.1 Learning Objectives\n\nDefine the following terms as they relate to R: call, function, arguments, options.\nDo simple arithmetic operations in R using values and objects.\nCall functions and use arguments to change their default options.\nUnderstand the logic and use of if else statements.\nDefine our own functions.\nCreate for and while loops.\nInspect the content of vectors and manipulate their content.\n\n3.1.2 Learning outline\n\nCreating objects/variables in R\nIf else statements\nUsing and writing functions\nVectors and data types\nSubsetting vectors\nMissing data\nLoops and vectorization\n\n\nNote: Parts of this lecture were originally created by combining contributions to Data Carpentry and has been modified to align with the aims of EEB313.\n\n\nprint(\"hello! today we are talking about vectors, functions, and the like.\")\n\n[1] \"hello! today we are talking about vectors, functions, and the like.\""
  },
  {
    "objectID": "lec02-base-r.html#clear-your-worksapce",
    "href": "lec02-base-r.html#clear-your-worksapce",
    "title": "3  Base R: assignment, vectors, functions, and loops",
    "section": "3.2 Clear your worksapce",
    "text": "3.2 Clear your worksapce\nWhen using Rstudio, it is best practice to turn off automatic save and restore of global workspace. To do this, go to the “Tools” menu in Rstudio, select “Global Options”, and make sure the “Restore .RData into workspace at startup” box is not selected For good measure, set the “Save workspace to .RData on exit” to “Never”. The command to clear your workspace in a script is\n\nrm(list=ls())\n\nToday we will go through some R basics, including how to create objects, assign values, define functions, and use for and while loops to iteratively preform calculations."
  },
  {
    "objectID": "lec02-base-r.html#creating-objects-in-r",
    "href": "lec02-base-r.html#creating-objects-in-r",
    "title": "3  Base R: assignment, vectors, functions, and loops",
    "section": "3.3 Creating objects in R",
    "text": "3.3 Creating objects in R\nAs we saw in our first class, you can get output from R simply by typing math in the console:\n\n3 + 5\n\n[1] 8\n\n12 / 7\n\n[1] 1.714286\n\n\nHowever, to do more complex calcualtions, we need to assign values to objects.\n\nx &lt;- 3\ny &lt;- x + 5\ny\n\n[1] 8\n\n\nYou can name an object in R almost anything you want:\n\njoel &lt;- 3\njoel + 5\n\n[1] 8\n\nTRUE &lt;- 3\n\nError in TRUE &lt;- 3: invalid (do_set) left-hand side to assignment\n\n### not allowed to overwrite logical operators\n\nT &lt;- 3 \n### for some reason this is allowed, but problematic\n### T and TRUE are often used interchangeably\n\nThere are some names that cannot be used because they are they are reserved for commands, operators, functions, etc. in base R (e.g., while, TRUE). See ?Reserved for a list these names. Even if it’s allowed, it’s best to not use names of functions that already exist in R (e.g., c, T, mean, data, df, weights). When in doubt, check the help or use tab completion to see if the name is already in use.\n\n3.3.0.1 Challenge\nWe have created two variables, joel and x. What is their sum? The sum of joel six times?\n\njoel + x\n\n[1] 6\n\njoel + joel + joel + joel + joel + joel\n\n[1] 18"
  },
  {
    "objectID": "lec02-base-r.html#some-tips-on-naming-objects",
    "href": "lec02-base-r.html#some-tips-on-naming-objects",
    "title": "3  Base R: assignment, vectors, functions, and loops",
    "section": "3.4 Some tips on naming objects",
    "text": "3.4 Some tips on naming objects\n\nObjects can be given any name: x, current_temperature, thing, or subject_id.\nYou want your object names to be explicit and not too long.\nObject names cannot start with a number: x2 is valid, but 2x is not valid.\nR is also case sensitive: joel is different from Joel.\n\nIt is recommended to use nouns for variable names, and verbs for function names. It’s important to be consistent in the styling of your code (where you put spaces, how you name variables, etc.). Using a consistent coding style1 makes your code clearer to read for your future self and your collaborators. RStudio will format code for you if you highlight a section of code and press Ctrl/Cmd + Shift + a.\n\n3.4.1 Preforming calculations\nWhen assigning a value to an object, R does not print anything. You can force R to print the value by using parentheses or by typing the object name:\n\nweight_kg &lt;- 55    # doesn't print anything\n(weight_kg &lt;- 55)  # putting parentheses around the call prints the value of `weight_kg`\n\n[1] 55\n\nweight_kg          # and so does typing the name of the object\n\n[1] 55\n\n\nThe variable weight_kg is stored in the computer’s memory where R can access it, and we can start doing arithmetic with it efficiently. For instance, we may want to convert this weight into pounds:\n\n2.2 * weight_kg\n\n[1] 121\n\n\nWe can also change a variable’s value by assigning it a new one:\n\nweight_kg &lt;- 57.5\n2.2 * weight_kg\n\n[1] 126.5\n\n\nImportantly, assigning a value to one variable does not change the values of other variables. For example, let’s store the animal’s weight in pounds in a new variable, weight_lb:\n\nweight_lb &lt;- 2.2 * weight_kg\n\nand then change weight_kg to 100.\n\nweight_kg &lt;- 100\nweight_lb\n\n[1] 126.5\n\n\nNotice that weight_lb is unchanged.\n\n3.4.1.1 Challenge\nWhat are the values of these variables after each statement in the following?\n\nmass &lt;- 47.5\nage  &lt;- 122\nmass &lt;- mass * 2.0      \nage  &lt;- age - 20  \nmass_index &lt;- mass/age"
  },
  {
    "objectID": "lec02-base-r.html#functions-and-their-arguments",
    "href": "lec02-base-r.html#functions-and-their-arguments",
    "title": "3  Base R: assignment, vectors, functions, and loops",
    "section": "3.5 Functions and their arguments!",
    "text": "3.5 Functions and their arguments!\nFunctions are sets of statements that are organized to preform certain tasks. They can be understood through analogy with cooking. Ingredients (called inputs or arguments) combine according to some set of reactions (the statements and commands of the function) to yield a product or output. A function does not have to return a number: a list of values could be returned, another function, or a list of functions.\nMany functions are built into R, including sqrt(). For sqrt(), the input must be a number larger than zero, and the value that is returned by the function is the square root of that number. Executing a function is called running or calling the function. An example of a function call is:\n\nsqrt(9)\n\n[1] 3\n\n# the input must be in the domain of the function:\nsqrt(\"hello\")\n\nError in sqrt(\"hello\"): non-numeric argument to mathematical function\n\nsqrt(-1) # note: sqrt() can take in *complex* numbers, including -1+0i\n\nWarning in sqrt(-1): NaNs produced\n\n\n[1] NaN\n\n\nThis is the same as assigning the value to a variable and then passing that variable to the function:\n\na &lt;- 9\nb &lt;- sqrt(a)\nb\n\n[1] 3\n\n\nHere, the value of a is given to the sqrt() function, the sqrt() function calculates the square root, and returns the value which is then assigned to variable b. This set up is important when you write more complex functions where multiple variables are passed to different arguments in different parts of a function.\nsqrt() is very simple because it takes just one argument. Arguments can be anything, not only numbers or files. Some functions take arguments which may either be specified by the user, or, if left out, take on a default value: these are called options. Options are typically used to alter the way the function operates, such as whether it ignores ‘bad values’, or what symbol to use in a plot. However, if you want something specific, you can specify a value of your choice which will be used instead of the default.\n\n3.5.1 Tab-completion\nTo access help about sqrt, tab-completion can be a useful tool. Type s and press Tab. You can see that R gives you suggestions of what functions and variables are available that start with the letter s, and thanks to RStudio they are formatted in this nice list. There are many suggestions here, so let’s be a bit more specific and append a q, to find what we want. If we press tab again, R will helpfully display all the available parameters for this function that we can pass an argument to.\n\n#s&lt;tab&gt;q\n#sqrt(&lt;tab&gt;)\n\nTo read the full help about sqrt, we can use the question mark, or type it directly into the help document browser.\n\n?sqrt\n\nAs you can see, sqrt() takes only one argument, x, which needs to be a numerical vector. Don’t worry too much about the fact that it says vector here; we will talk more about that later. Briefly, a numerical vector is one or more numbers. In R, every number is a vector, so you don’t have to do anything special to create a vector. More on vectors later!\nLet’s try a function that can take multiple arguments: round().\n\n#round(&lt;tab&gt;)\n?round\n\nIf we try round with a value:\n\nround(3.14159)\n\n[1] 3\n\n\nHere, we’ve called round() with just one argument, 3.14159, and it has returned the value 3. That’s because the default is to round to the nearest whole number, or integer. If we want more digits we can pass an argument to the digits parameter, to specify how many decimals we want to round to.\n\nround(3.14159, digits = 2)\n\n[1] 3.14\n\n\nAbove we have passed the argument 2 to the parameter digits. We can leave out the word digits since we know it comes as the second parameter, after x.\n\nround(3.14159, 2)\n\n[1] 3.14\n\n\nAs you notice, we have been leaving out x from the beginning. If you provide the names for both the arguments, we can switch their order:\n\nround(digits = 2, x = 3.14159)\n\n[1] 3.14\n\n\nIt’s good practice to put non-optional arguments before optional arguments, and to specify the names of all optional arguments. If you don’t, someone reading your code might have to look up the definition of a function with unfamiliar arguments to understand what you’re doing."
  },
  {
    "objectID": "lec02-base-r.html#if-else-statements",
    "href": "lec02-base-r.html#if-else-statements",
    "title": "3  Base R: assignment, vectors, functions, and loops",
    "section": "3.6 If else statements",
    "text": "3.6 If else statements\nIt is often useful to preform calculations only when certain conditions are met. One way to do this is using an “if else” statement. The syntax of such a statement is below:\n\n# if (condition){\n#   computation\n# } else{ \n#   another computation\n# }\n\nWithout the else bit, the computation will be preformed if the condition is satisfied and nothing will be done (and variables in the environment will be unchanged) otherwise.\n\nt &lt;- 1\n\nt &lt; 10 # returns the truth value of this statement\n\n[1] TRUE\n\nt == 10\n\n[1] FALSE\n\nt &gt; 10\n\n[1] FALSE\n\nt &gt; 10 | t == 10\n\n[1] FALSE\n\n### &lt; (less than), &gt; (greater than), == (equals)\n### & (and), | (or), ! (not) are common logical operators\n\nif (t &lt; 10){\n  print(t)\n} else{\n  print(t-1)\n}\n\n[1] 1\n\n### setting t &lt;- 10 and executing the above returns 9\n\nIn fact, if else statements lend themselves naturally to deciding which of &gt;2 alternative computations should be preformed, based on a set of appropriate conditions. For example,\n\nt &lt;- 10\nt2 &lt;- 20\n\nif (t &lt; 10 & t2 &gt; 19){\n  print(\"1\")\n} else if (t &lt; 10 & t2 &gt; 19){\n  print(\"2\")\n} else if (t &lt;= 10 & t2 &gt; 19){\n  print(\"3\")\n}\n\n[1] \"3\"\n\n### notice how the third condition is met, but the others are not\n### when the first condition is met (even if the others are too), \"1\" is printed:\n\nif (t &lt;= 10 & t2 &gt; 19){\n  print(\"1\")\n} else if (t &lt;= 10 & t2 &gt; 19){\n  print(\"2\")\n} else if (t &lt;= 10 & t2 &gt; 19){\n  print(\"3\")\n}\n\n[1] \"1\""
  },
  {
    "objectID": "lec02-base-r.html#writing-functions",
    "href": "lec02-base-r.html#writing-functions",
    "title": "3  Base R: assignment, vectors, functions, and loops",
    "section": "3.7 Writing functions",
    "text": "3.7 Writing functions\nWe have seen there are many built-in functions in R, which we will use throughout the semester: sum, c(), mean(), all(), plot(), ifelse(), print(). We can also write our own functions for custom use. For example, the below chuck defines two functions which check if two scalar inputs are positive.\n\ncheck_if_numbers_are_postive_function1 &lt;- function(num1, num2) {\n  if (num1 &gt; 0 & num2 &gt; 0){\n    return(\"both numbers are postive!\")\n  } else{\n    return(\"one or both numbers are not postive.\")\n  }\n}\n\ncheck_if_numbers_are_postive_function1(4, 5)\n\n[1] \"both numbers are postive!\"\n\ncheck_if_numbers_are_postive_function1(-4, 5)\n\n[1] \"one or both numbers are not postive.\"\n\ncheck_if_numbers_are_postive_function2 &lt;- function(num1, num2) {\n  if (num1 &gt; 0){\n    if (num2 &gt; 0){\n      return(\"both numbers are postive!\")\n    }\n  }\n}\n\ncheck_if_numbers_are_postive_function2(4, 5)\n\n[1] \"both numbers are postive!\"\n\ncheck_if_numbers_are_postive_function2(-4, 5)\n\nAlthough these functions agree when both inputs are positive (i.e., they return the same output), the second function does not return a statement indicating one or both of the inputs are non-positive when this is the case. This is because we have not indicated what should be returned when the condition in one or the other if the statement in check_if_numbers_are_postive_function2 is not met.\nWe can do this as follows:\n\ncheck_if_numbers_are_postive_function2 &lt;- function(num1, num2) {\n  \n  if (! num1 &gt; 0){\n    return(\"one or both numbers are not postive.\")\n  }\n  \n  if (num1 &gt; 0){\n    if (num2 &gt; 0){\n      return(\"both numbers are postive!\")\n    }\n     if (! num2 &gt; 0){\n      return(\"one or both numbers are not postive.\")\n    }\n  }\n  \n}\n\ncheck_if_numbers_are_postive_function2(4, 5)\n\n[1] \"both numbers are postive!\"\n\ncheck_if_numbers_are_postive_function2(-4, 5)\n\n[1] \"one or both numbers are not postive.\"\n\ncheck_if_numbers_are_postive_function2(4, -5)\n\n[1] \"one or both numbers are not postive.\"\n\n\nImportantly, these functions are not written with elegance in mind. There are better ways to check if two numbers are both positive. We encourage you to think more about how to write functions (like the above) with elegance and efficiency in mind, and how trade-offs between the two might come up.\n\n3.7.0.1 Challenge\nCan you write a function that calculates the mean of 3 numbers?\n\nmean_of_three_numbers &lt;- function(num1, num2, num3) {\n   my_sum &lt;- num1 + num2 + num3\n   my_mean &lt;- my_sum / 3\n   return(my_mean)\n}\nmean_of_three_numbers(2, 4, 6)\n\n[1] 4"
  },
  {
    "objectID": "lec02-base-r.html#vectors-and-data-types",
    "href": "lec02-base-r.html#vectors-and-data-types",
    "title": "3  Base R: assignment, vectors, functions, and loops",
    "section": "3.8 Vectors and data types",
    "text": "3.8 Vectors and data types\nA vector is the most common data type in R, and is the workhorse of the language. A vector is composed of a series of values, which can be numbers (0, \\(\\pi\\), 72) or characters (“hello”, “I’m a ChaRaCTER”). We can assign a series of values to a vector using the c() function, which stands for concatenate. For example we can create a vector of animal weights and assign it to a new object weight_g:\n\nweight_g &lt;- c(50, 60, 65, 82) # concatenate values into a vector\nweight_g\n\n[1] 50 60 65 82\n\n\nYou can also use the command seq to create a sequence of numbers.\n\nseq(from = 0, to = 30) # default spacing is =1\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n[26] 25 26 27 28 29 30\n\nseq(from = 0, to = 30, by = 3) # returns every third number in c(0,1,2,...,30)\n\n [1]  0  3  6  9 12 15 18 21 24 27 30\n\n\nA vector can also contain characters (in addition to numbers):\n\nanimals &lt;- c('mouse', 'rat', 'dog')\nanimals\n\n[1] \"mouse\" \"rat\"   \"dog\"  \n\n\nThe quotes around “mouse”, “rat”, etc. are essential here and can be either single or double quotes. Without the quotes R will assume there are objects called mouse, rat and dog. As these objects don’t exist in R’s memory, there will be an error message.\nThere are many functions that allow you to inspect the content of a vector. length() tells you how many elements are in a particular vector:\n\nlength(weight_g)\n\n[1] 4\n\nlength(animals)\n\n[1] 3\n\n\nAn important feature of a vector is that all of the elements are the same type of data. The function class() indicates the class (the type of element) of an object:\n\nclass(weight_g)\n\n[1] \"numeric\"\n\nclass(animals)\n\n[1] \"character\"\n\n\nThe function str() provides an overview of the structure of an object and its elements. It is a useful function when working with large and complex objects:\n\nstr(weight_g)\n\n num [1:4] 50 60 65 82\n\nstr(animals)\n\n chr [1:3] \"mouse\" \"rat\" \"dog\"\n\n\nYou can use the c() function to add other elements to your vector:\n\nweight_g &lt;- c(weight_g, 90) # add to the end of the vector\nweight_g &lt;- c(30, weight_g) # add to the beginning of the vector\nweight_g\n\n[1] 30 50 60 65 82 90\n\n\nIn the first line, we take the original vector weight_g, add the value 90 to the end of it, and save the result back into weight_g. Then we add the value 30 to the beginning, again saving the result back into weight_g.\nWe can do this over and over again to grow a vector, or assemble a dataset. As we program, this may be useful to add results that we are collecting or calculating.\nAn atomic vector is the simplest R data type and it is a linear vector of a single type, e.g., all numbers. Above, we saw two of the six atomic vector types that R uses: \"character\" and \"numeric\" (or \"double\"). These are the basic building blocks that all R objects are built from.\nThe other four atomic vector types are:\n\n\"logical\" for TRUE and FALSE (the boolean data type)\n\"integer\" for integer numbers (e.g., 2L, the L indicates to R that it’s an integer)\n\"complex\" to represent complex numbers with real and imaginary parts (e.g., 1 + 4i).\n\"raw\" for bitstreams. We will not discuss this type further.\n\nVectors are one of the many data structures that R uses. Other important ones are lists (list), matrices (matrix), data frames (data.frame), factors (factor) and arrays (array). In this class, we will focus on data frames, which is most commonly used one for data analyses.\n\n3.8.0.1 Challenge\nWe’ve seen that atomic vectors can be of type character, numeric (or double), integer, and logical. What happens if we try to mix these types? Find out by using class to test these examples.\n\nnum_char &lt;- c(1, 2, 3, 'a')\nnum_logical &lt;- c(1, 2, 3, TRUE)\nchar_logical &lt;- c('a', 'b', 'c', TRUE)\ntricky &lt;- c(1, 2, 3, '4')\n\n\n# Answer\nclass(num_char)\n\n[1] \"character\"\n\nclass(num_logical)\n\n[1] \"numeric\"\n\nclass(char_logical)\n\n[1] \"character\"\n\nclass(tricky)\n\n[1] \"character\"\n\n\nThis happens because vectors can be of only one data type. Instead of throwing an error and saying that you are trying to mix different types in the same vector, R tries to convert (coerce) the content of this vector to find a “common denominator”. A logical can be turn into 1 or 0, and a number can be turned into a string/character representation. It would be difficult to do it the other way around: would 5 be TRUE or FALSE? What number would ‘t’ be? This establishes a hierarchy for conversions/coercions, whereby some types get preferentially coerced into other types. From the above example, we can see that the hierarchy goes logical -&gt; numeric -&gt; character, and logical can also be directly coerced into character."
  },
  {
    "objectID": "lec02-base-r.html#subsetting-vectors",
    "href": "lec02-base-r.html#subsetting-vectors",
    "title": "3  Base R: assignment, vectors, functions, and loops",
    "section": "3.9 Subsetting vectors",
    "text": "3.9 Subsetting vectors\nIf we want to extract one or several values from a vector, we provide one or several indices in square brackets:\n\nanimals &lt;- c(\"mouse\", \"rat\", \"dog\", \"cat\")\nanimals[2]\n\n[1] \"rat\"\n\nanimals[c(3, 2)] # Provide multiple indices simultaneously\n\n[1] \"dog\" \"rat\"\n\n\nWe can also repeat the indices to create an object with more elements than the original one:\n\nmore_animals &lt;- animals[c(1, 2, 3, 2, 1, 4)]\nmore_animals\n\n[1] \"mouse\" \"rat\"   \"dog\"   \"rat\"   \"mouse\" \"cat\"  \n\n\nR indices start at 1. Programming languages like Fortran, MATLAB, Julia, and R start counting at 1, because that’s what human beings typically do. Languages in the C family (including C++, Java, Perl, and Python) start counting at 0.\n\n3.9.1 Conditional subsetting\nAnother common way of subsetting is by using a logical vector. TRUE will select the element with the same index, while FALSE will not:\n\nweight_g &lt;- c(21, 34, 39, 54, 55)\nweight_g[c(TRUE, FALSE, TRUE, TRUE, FALSE)]\n\n[1] 21 39 54\n\n\nTypically, these logical vectors are not typed by hand, but are the output of other functions or logical tests."
  },
  {
    "objectID": "lec02-base-r.html#na-na-na-na-na-na-missing-data",
    "href": "lec02-base-r.html#na-na-na-na-na-na-missing-data",
    "title": "3  Base R: assignment, vectors, functions, and loops",
    "section": "3.10 NA NA NA NA NA NA… Missing data??",
    "text": "3.10 NA NA NA NA NA NA… Missing data??\nDue to its origins as a statistical computing language, R includes tools to deal with missing data easily. Missing data are represented in vectors as NA.\nImportantly, many built-in R functions will return NA if the data you are working with include missing values. This feature makes it harder to overlook the cases where you are dealing with missing data.\n\nheights &lt;- c(2, 4, 4, NA, 6)\nmean(heights)\n\n[1] NA\n\nmax(heights)\n\n[1] NA\n\n\nFor functions such as mean(), you can add the argument na.rm = TRUE to preform calculations ignoring the missing values:\n\nmean(heights, na.rm = TRUE)\n\n[1] 4\n\nmax(heights, na.rm = TRUE)\n\n[1] 6\n\n\nIt is also possible to use conditional subsetting to remove NAs. The function is.na() is helpful in this case. This function examines each element in a vector to see whether it is NA, and returns a logical vector.\n\nis.na(heights)\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\nCombining this function and ! (the logical operator not), we can extract elements that are not NAs:\n\n## Extract those elements which are not missing values.\nheights[!is.na(heights)]\n\n[1] 2 4 4 6\n\n\nAlternatively, we can use the these functions to achieve the same outcome.\n\n# Returns the object with incomplete cases removed. \nna.omit(heights)\n\n[1] 2 4 4 6\nattr(,\"na.action\")\n[1] 4\nattr(,\"class\")\n[1] \"omit\"\n\n# Extract those elements which are complete cases. \nheights[complete.cases(heights)]\n\n[1] 2 4 4 6\n\n\nImportant note: missing data are ubiquitous. Make sure you know why NAs exist in your data before removing them. If NAs are removed, document why and be sure to store the data pre- and post-processing."
  },
  {
    "objectID": "lec02-base-r.html#loops-and-vectorization",
    "href": "lec02-base-r.html#loops-and-vectorization",
    "title": "3  Base R: assignment, vectors, functions, and loops",
    "section": "3.11 Loops and vectorization",
    "text": "3.11 Loops and vectorization\nLoops are essential in programming. They come in two types: for and while.\nThe syntax for a for loop is as follows:\n\n# for (iterator in values_iterator_can_assume){\n#   computation\n# }\n\nThe syntax for a while loop is as follows:\n\n# while (condition){\n#   computation\n# }\n\nThe key difference between these types of loop is that a while loop breaks when the condition fails to be met; the loop preforms calculations while the condition is met. A for loop preforms the computation for all values of the iterator in the list/vector/etc. of values specified in the “for” statement.\nThe below for loop prints the values in the vector the iterator num can assume (one by one):\n\nv &lt;- c(2, 4, 6, 8, 10)\nfor (num in v) {\n    print(num)\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n[1] 10\n\n\nEquivalently, we could write\n\nfor (i in 1:5) {\n  print(v[i])\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n[1] 10\n\n\nThis set up is quite powerful. We can now perform tasks iteratively:\n\n# creates vector where each number is 3 more than the previous number:\n\nx &lt;- c(0.4)\n\nfor (i in 1:5) {\n  x[i+1] &lt;- x[i] + 3 \n}\n\nx\n\n[1]  0.4  3.4  6.4  9.4 12.4 15.4\n\n# calls sqrt() function from inside loop\n\nx &lt;- c(0.4)\n\nfor (i in 1:5) {\n  x[i+1] &lt;- sqrt(x[i])\n}\n\nx\n\n[1] 0.4000000 0.6324555 0.7952707 0.8917795 0.9443408 0.9717720\n\n\nTo constrast for and while loops, consider the following:\n\nx &lt;- 0.4\ni &lt;- 1\ny &lt;- c() ### need to declare y so that values can be added in below loop\n\nwhile (x &lt;= 0.9999) {\n  y[i] &lt;- x\n  x &lt;- sqrt(x)\n  i &lt;- i + 1 # updating i so that y can be updated in next step\n}\n\n### note we could just keep track of x if we:\n### 1) use the condition x[i] &lt;= 0.9999\n### 2) calculate the next term in the sequence of sqrts using x[i+1] &lt;- sqrt(x[i])\n\ny\n\n [1] 0.4000000 0.6324555 0.7952707 0.8917795 0.9443408 0.9717720 0.9857850\n [8] 0.9928670 0.9964271 0.9982120 0.9991056 0.9995527 0.9997763 0.9998882\n\n\nThe above loop returns the sequence of square roots \\(0.4, \\sqrt{0.4}, \\sqrt{\\sqrt{0.4}}, \\dots\\). Importantly, the loop terminates when an element of this sequence is greater than 0.9999. The number of iterations until this happens is not specified. This means while loops can run for infinite time if their conditions are never violated. It is best to have checks in place to make sure this doesn’t happen!"
  },
  {
    "objectID": "lec02-base-r.html#footnotes",
    "href": "lec02-base-r.html#footnotes",
    "title": "3  Base R: assignment, vectors, functions, and loops",
    "section": "",
    "text": "Refer to the tidy style guide for which style to adhere to.↩︎"
  },
  {
    "objectID": "lec03-dataframes-dplyr.html#lesson-preamble",
    "href": "lec03-dataframes-dplyr.html#lesson-preamble",
    "title": "4  Data frames, intro to dplyr",
    "section": "4.1 Lesson Preamble",
    "text": "4.1 Lesson Preamble\n\n4.1.1 Learning Objectives\n\nDescribe what a data frame is.\nLoad external data from a .csv file into a data frame in R.\nSummarize the contents of a data frame in R.\nUnderstand the purpose of the dplyr package.\n\n4.1.2 Lecture outline\n\nR packages for data analyses (20 mins)\nData set background (10 mins)\nWhat are data frames (10 mins)\nIntroduction to data and wrangling (40 mins)"
  },
  {
    "objectID": "lec03-dataframes-dplyr.html#setting-up-the-workspace",
    "href": "lec03-dataframes-dplyr.html#setting-up-the-workspace",
    "title": "4  Data frames, intro to dplyr",
    "section": "4.2 Setting up the workspace",
    "text": "4.2 Setting up the workspace\nOne of the keys to a successful R working session is to make sure you have everything you need ready to go, and neatly organized. Just like when you are cooking or baking, having everything pre-measured and organized in neat way can be so helpful to ensure efficient, no fuss, and nice dishes in the end.\nNow we’ll take another couple of minutes to get our R workspace ready.\n\n4.2.1 Set up your Notebook\nFirst, we are going to set up our Notebook.\n---\ntitle: Introduction to R\n---\nRecall that this header block is called the YAML header. This is optional, without this your document will still knit, but this is useful for fine-tuning the output of your document, such as font size, figure sizes, and generating a table of content. We will mostly be using it to give our documents nice headers. If you are interested in playing with other customizations options the YAML provides, check out this guide.\n\n\n4.2.2 Check your directory\nOk, now the Notebook is ready. Next, we want to make sure we are working out of the correct directory, which is a fancy way to say the folder that we will be reading files from, and saving files to.\nBy default, R works out of your Documents folder. We can check this with the following function getwd(), which stands for get working directory.\n\ngetwd()\n\nIf this is the folder you wish to work out of, then we are good to go. If not, then we need to redirect R to some place else. Normally, in an R Script, you would do so by using the function setwd() (set working directory). In R Notebook, this is a slightly more involved process, because you cannot just quickly switch directories with setwd(). The reason for this is because R is trying to be helpful making sure you don’t lose your files half way through a script when you switch folders. In order to change directories in R Notebook, we have to reach all the way in and change the root directory in a special “setup code chunk”. To make this setup code chunk, add the words setup after r in your code chunk header.\n\n\nThe setup code chunk must NOT contain any commands other than the setup options you wish to change.\nWe then need to find the path to the folder we want R to work out of. In PC, right-click on the desired folder, and you can find its location under the General tab. In Mac, right-click on the desired folder and choose Get Info to find it’s location.\n\n# For PC (remember to change forward-slash to back-slash):\n# knitr::opts_knit$set(root.dir = \"C:/Users/Documents/UofT/eeb313\")\n\n# For Mac (can get away with using ~ as short hand):\n# knitr::opts_knit$set(root.dir = \"~/Documents/UofT/eeb313\")\n\ngetwd()\n\nHowever, remember last week when we talked about relative file paths? The functions to set your working directory is slowly becoming outdated as project-oriented workflows become the norm.\nEach R script or .Rmd report should contain a call to here::i_am('path/to/this/file') at the top to establish the root directory.\nSubsequent file paths can be made using the here function. You should also be saving your files and plots by using the here() function. More on that later.\n\nlibrary(here)\nhere()\n\n# use to set \"lectures\" as working directory\n# here::i_am(\"lectures\", \"lec03-dataframes-dplyr.Rmd\")\n\nOne important distinction from the working directory is that this remains stable even if the working directory is changed. I recommend that you steer clear from ever changing the working directory.\n\n\n4.2.3 Load your packages\nNext, we want to check to see that all of our required packages are properly loaded and ready to go.\nIn a nutshell, packages are bundles of functions that perform related tasks (like a book of recipes). Our installation of R comes with some packages already, since they are considered critical for using R, e.g., c(), mean(), +, -, etc. These pre-installed packages and function are commonly collectively referred to as base R.\nIn addition to these foundational packages and functions, CRAN (Comprehensive R Archive Network), the official repository for R-packages, houses thousands more packages. There is literally a package for every occasion you can and cannot imagine. Did you know that you can call an Uber from R (ubeR)? How about, instead of boring dots as data points in figures, you can plot emojis (emojifont)? Frustrated with code and need a pick-me-up? The packages praise1 and kittyR2 got you covered! All of this is possible due to R being an open language, and many people generously contribute their time to its development.\nThese additionally packages are not installed by default, because then base R installation would be huge and most people would only be using a fraction of everything installed on their machine. It would be like if you downloaded the Firefox or Chrome browser and you would get all extensions and add-ons installed by default, or as if your phone came with every app ever made for it already installed when you bought it: quite impractical.\nTo install a package in R, we use the function install.packages(). Think of this function as downloading the package to your machine - you only need to do this once.\n\ninstall.packages('tidyverse') # This is a chonky package, takes a while to download\n\ntidyverse3 is a conglomerate package that is a collection of packages that has similar functions, just like Microsoft Word is part of Microsoft Office. tidyverse, as its name may suggest, contains many packages that makes data cleaning and exploring more intuitive and effective. It is basically an entire philosophy on how to handle data and has a massive following.\nThe two tidyverse packages we will be using the most frequently in this course is dplyr and ggplot2. dplyr is great for data wrangling (Lecture 2) and ggplot2 makes killer plots (Lecture 3).\nTo use functions in the dplyr package, type dplyr:: and then the function name.\n\ndplyr::glimpse(cars) \n\nRows: 50\nColumns: 2\n$ speed &lt;dbl&gt; 4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13…\n$ dist  &lt;dbl&gt; 2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34…\n\n# `glimpse` gives us a glimpse... of an object\n# `cars` is a base R dataset\n\nSince we will be using this package a lot, it would be a little annoying to have to type dplyr:: every time. We can bypass this step by loading the package into our current environment. Think of this is “opening” the package for your work session.\n\nlibrary(tidyverse) # More convenient to load all tidy packages at once\n\nWarning: package 'tidyverse' was built under R version 4.1.2\n\n\nWarning: package 'tibble' was built under R version 4.1.2\n\n\nWarning: package 'tidyr' was built under R version 4.1.2\n\n\nWarning: package 'readr' was built under R version 4.1.2\n\n\nWarning: package 'stringr' was built under R version 4.1.2\n\n\nWarning: package 'forcats' was built under R version 4.1.2\n\n\nWarning: package 'lubridate' was built under R version 4.1.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nglimpse(cars)\n\nRows: 50\nColumns: 2\n$ speed &lt;dbl&gt; 4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13…\n$ dist  &lt;dbl&gt; 2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34…\n\n\nThis needs to be done once for every new R session, and so it is common practice to keep a list of all the packages used at the top of your script or notebook for convenience and load all of it at start up.\nThat’s a lot of red though! What are these warning signs and checks?\nAll the warning signs indicate are the version of R that they were built under. They can frequently be ignored unless your version of R is so old that the packages can no longer be run on R! Note that packages are frequently updated, and functions may become deprecated.\nNext, the warning shows you all the packages that were successfully installed.\nFinally, there are some conflicts! All this means is that there are multiple functions with the same name that may do different things. R prioritizes functions from certain packages over others. So, in this case, the filter() function from dplyr will take precedent over the filter() function from the stats package. If you want to use the latter, use double colons :: to indicate that you are calling a function from a certain package:\n\nstats::filter()\n\n\n4.2.3.1 An Aside on Reproducible Environments\nAt this point, you are already living a project-centric lifestyle. With renv, you’ll start using project libraries, giving each project its own independent collection of packages. To increase reproducibility of a project, we must keep track of what packages are used. This also helps us avoid error messages if we update R or update a package, and things no longer work.\nTo convert a project to use renv, call renv::init(). It adds three new files and directories to your project. Make sure you indicate where your project lives by using here()\n\n# install.packages(\"renv\")\nlibrary(renv)\nrenv::init(here())\n\nThat’s a lot of output! What does it mean?\nYou should now see lockfile, project_name/renv.lock. This lockfile records all of the information about packages needed by your project, such as the version of package and where was it installed from. It knows all this because renv scans all files in your project directory, and looks for functions like library, require, or package::function.\nWe also see project_name/.Rprofile, containing one line. When R starts, it searches for .Rprofile and runs what it finds. We can use this file to change various options.\nYou will also see a folder project_name/renv/, which contains your project library. If you already have a package installed elsewhere, renv will link to it. Otherwise, it’ll be installed in renv/library.\nEssentially, whenever we open our project, we activate our R environment using these three files. renv will automatically be active in any R session that is run from the project directory. To activate\n\nactivate(here())\n\n# or use\nknitr::opts_knit$set(root.dir = here())\n\nNow, every time you open your project, run restore() in order to start from where you left off!\n\nrestore()\n\nIf you are adding new packages that you require to your code, uses snapshot() to record changes to the renv.lock file. This function captures the state of your project at that point in time.\n\nsnapshot()\n\nTo summarize, the general workflow[^1] when working with renv is:\n\nCall renv::init() to initialize a new project-local environment with a private R library. Work in the project as normal, installing and removing new R packages as they are needed in the project.\nCall renv::snapshot() to save the state of the project library to the lockfile (called renv.lock). Continue working on your project, installing and updating R packages as needed.\nCall renv::snapshot() again to save the state of your project library if your attempts to update R packages were successful, or call renv::restore() to revert to the previous state as encoded in the lockfile if your attempts to update packages introduced some new problems.\n\n[^1] Material taken from Posit.\nThe last thing required for reproducibility is a version control system, like Git and GitHub. That lecture will be coming later on in the semester!\n\n\n\n4.2.4 Load your data\nJust a moment ago we took a glimpse of the dataset cars, which is one of the datasets that came with R. Now we are going to try loading our own data.\nWe will be working with real data from a longitudinal study of the species abundance in the Chihuahuan desert ecosystem near Portal, Arizona, USA. This study includes observations of plants, ants, and rodents from 1977 - 2002, and has been used in over 100 publications. More information is available in the abstract of this paper from 2009. There are several datasets available related to this study, and we will be working with datasets that have been preprocessed by Data Carpentry to facilitate teaching. These are made available online as The Portal Project Teaching Database, both at the Data Carpentry website, and on Figshare. Figshare is a great place to publish data, code, figures, and more openly to make them available for other researchers and to communicate findings that are not part of a longer paper.\nWe are studying the species and weight of animals caught in plots in our study area. The dataset is stored as a comma separated value (CSV) file. Each row holds information for a single animal, and the columns represent:\n\n\n\nColumn\nDescription\n\n\n\n\nrecord_id\nunique id for the observation\n\n\nmonth\nmonth of observation\n\n\nday\nday of observation\n\n\nyear\nyear of observation\n\n\nplot_id\nID of a particular plot\n\n\nspecies_id\n2-letter code\n\n\nsex\nsex of animal (“M”, “F”)\n\n\nhindfoot_length\nlength of the hindfoot in mm\n\n\nweight\nweight of the animal in grams\n\n\ngenus\ngenus of animal\n\n\nspecies\nspecies of animal\n\n\ntaxa\ne.g. rodent, reptile, bird, rabbit\n\n\nplot_type\ntype of plot\n\n\n\nTo read data into R, we are going to use a function called read_csv. One useful option that read_csv includes is the ability to read a CSV file directly from a URL, without downloading it in a separate step:\n\nlibrary(readr)\nsurveys &lt;- read_csv('https://ndownloader.figshare.com/files/2292169')\n\nRows: 34786 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): species_id, sex, genus, species, taxa, plot_type\ndbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHowever, it is often a good idea to download the data first, so you have a copy stored locally on your computer in case you want to do some offline analyses, or the online version of the file changes or the file is taken down. You can either download the data manually or from within R:\n\ndownload.file(\"https://ndownloader.figshare.com/files/2292169\",\n              \"portal_data.csv\")\n# Saves file to current or root directory with this name\n# You can use `here()` to indicate sublevels in your directory\n\nThe data is read in by specifying its file name. R knows to look in your current directory for something with this name (don’t forget the quotation marks).\n\nsurveys &lt;- read_csv(\"portal_data.csv\")\n\nRows: 34786 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): species_id, sex, genus, species, taxa, plot_type\ndbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s have a look at this dataset.\n\nsurveys\n\n# A tibble: 34,786 × 13\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2        72     8    19  1977       2 NL         M                  31     NA\n 3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n 4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n 5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n 6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n 7       435    12    10  1977       2 NL         &lt;NA&gt;               NA     NA\n 8       506     1     8  1978       2 NL         &lt;NA&gt;               NA     NA\n 9       588     2    18  1978       2 NL         M                  NA    218\n10       661     3    11  1978       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 34,776 more rows\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\nThis displays a nice tabular view of the data, which also includes pagination; there are many rows and we can click the arrow to view all the columns. Technically, this object is actually a tibble rather than a data frame, as indicated in the output. The reason for this is that read_csv automatically converts the data into to a tibble when loading it. Since a tibble is just a data frame with some convenient extra functionality, we will use these words interchangeably from now on.\nIf we just want to glance at how the data frame looks, it is sufficient to display only the top (the first 6 lines) using the function head():\n\nhead(surveys)\n\n# A tibble: 6 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1         1     7    16  1977       2 NL         M                  32     NA\n2        72     8    19  1977       2 NL         M                  31     NA\n3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\nAwesome! Everything is set, and now we are finally ready to start working with real data in R!"
  },
  {
    "objectID": "lec03-dataframes-dplyr.html#what-are-data-frames",
    "href": "lec03-dataframes-dplyr.html#what-are-data-frames",
    "title": "4  Data frames, intro to dplyr",
    "section": "4.3 What are data frames?",
    "text": "4.3 What are data frames?\nData frames are the de facto data structure for most tabular data, and what we use for statistics and plotting. A data frame can be created by hand, but most commonly they are generated by the function read_csv(); in other words, when importing spreadsheets from your hard drive (or the web).\nA data frame is a representation of data in the format of a table where the columns are vectors that all have the same length. Because the columns are vectors, each column contain the same type of data (e.g., characters, integers, factors). We can see this when inspecting the structure of a data frame with the function str() (structure):\n\nstr(surveys)\n\nspc_tbl_ [34,786 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ record_id      : num [1:34786] 1 72 224 266 349 363 435 506 588 661 ...\n $ month          : num [1:34786] 7 8 9 10 11 11 12 1 2 3 ...\n $ day            : num [1:34786] 16 19 13 16 12 12 10 8 18 11 ...\n $ year           : num [1:34786] 1977 1977 1977 1977 1977 ...\n $ plot_id        : num [1:34786] 2 2 2 2 2 2 2 2 2 2 ...\n $ species_id     : chr [1:34786] \"NL\" \"NL\" \"NL\" \"NL\" ...\n $ sex            : chr [1:34786] \"M\" \"M\" NA NA ...\n $ hindfoot_length: num [1:34786] 32 31 NA NA NA NA NA NA NA NA ...\n $ weight         : num [1:34786] NA NA NA NA NA NA NA NA 218 NA ...\n $ genus          : chr [1:34786] \"Neotoma\" \"Neotoma\" \"Neotoma\" \"Neotoma\" ...\n $ species        : chr [1:34786] \"albigula\" \"albigula\" \"albigula\" \"albigula\" ...\n $ taxa           : chr [1:34786] \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ...\n $ plot_type      : chr [1:34786] \"Control\" \"Control\" \"Control\" \"Control\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   record_id = col_double(),\n  ..   month = col_double(),\n  ..   day = col_double(),\n  ..   year = col_double(),\n  ..   plot_id = col_double(),\n  ..   species_id = col_character(),\n  ..   sex = col_character(),\n  ..   hindfoot_length = col_double(),\n  ..   weight = col_double(),\n  ..   genus = col_character(),\n  ..   species = col_character(),\n  ..   taxa = col_character(),\n  ..   plot_type = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nInteger refers to a whole number, such as 1, 2, 3, 4, etc. Numbers with decimals, 1.0, 2.4, 3.333, are referred to as floats. Factors are used to represent categorical data. Factors can be ordered or unordered, and understanding them is necessary for statistical analysis and for plotting. Factors are stored as integers, and have labels (text) associated with these unique integers. While factors look (and often behave) like character vectors, they are actually integers under the hood, and you need to be careful when treating them like strings.\n\n4.3.1 Inspecting data.frame objects\nWe already saw how the functions head() and str() can be useful to check the content and the structure of a data frame. Here is a non-exhaustive list of functions to get a sense of the content/structure of the data. Let’s try them out!\n\nSize:\n\ndim(surveys) - returns a vector with the number of rows in the first element and the number of columns as the second element (the dimensions of the object)\nnrow(surveys) - returns the number of rows\nncol(surveys) - returns the number of columns\n\nContent:\n\nhead(surveys) - shows the first 6 rows\ntail(surveys) - shows the last 6 rows\n\nNames:\n\nnames(surveys) - returns the column names (synonym of colnames() for data.frame objects)\nrownames(surveys) - returns the row names\n\nSummary:\n\nstr(surveys) - structure of the object and information about the class, length, and content of each column\nsummary(surveys) - summary statistics for each column\n\n\nNote: most of these functions are “generic”, they can be used on other types of objects besides data.frame.\n\n4.3.1.1 Challenge\nBased on the output of str(surveys), can you answer the following questions?\n\nWhat is the class of the object surveys?\nHow many rows and how many columns are in this object?\nHow many species have been recorded during these surveys?\n\n\n\n\n4.3.2 Indexing and subsetting data frames\nOur survey data frame has rows and columns (that is, it has 2 dimensions). If we want to extract some specific data from it, we need to specify the “coordinates” we want from it in a manner similar to when we indexed vectors. Row numbers come first, followed by column numbers. When indexing, base R data frames return a different format depending on how we index the data (i.e., either a vector or a data frame), but with enhanced data frames, tibbles, the returned object is almost always a data frame.\n\nsurveys[1, 1]   # First element in the first column of the data frame\n\n# A tibble: 1 × 1\n  record_id\n      &lt;dbl&gt;\n1         1\n\nsurveys[1, 6]   # First element in the 6th column\n\n# A tibble: 1 × 1\n  species_id\n  &lt;chr&gt;     \n1 NL        \n\nsurveys[, 1]    # First column in the data frame\n\n# A tibble: 34,786 × 1\n   record_id\n       &lt;dbl&gt;\n 1         1\n 2        72\n 3       224\n 4       266\n 5       349\n 6       363\n 7       435\n 8       506\n 9       588\n10       661\n# ℹ 34,776 more rows\n\nsurveys[1]      # First column in the data frame\n\n# A tibble: 34,786 × 1\n   record_id\n       &lt;dbl&gt;\n 1         1\n 2        72\n 3       224\n 4       266\n 5       349\n 6       363\n 7       435\n 8       506\n 9       588\n10       661\n# ℹ 34,776 more rows\n\nsurveys[1:3, 7] # First three elements in the 7th column\n\n# A tibble: 3 × 1\n  sex  \n  &lt;chr&gt;\n1 M    \n2 M    \n3 &lt;NA&gt; \n\nsurveys[3, ]    # The 3rd element for all columns\n\n# A tibble: 1 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\nsurveys[1:6, ]  # Equivalent to head(surveys)\n\n# A tibble: 6 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1         1     7    16  1977       2 NL         M                  32     NA\n2        72     8    19  1977       2 NL         M                  31     NA\n3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\nYou can also exclude certain parts of a data frame using the “-” sign:\n\nsurveys[,-1]    # All columns, except the first\n\n# A tibble: 34,786 × 12\n   month   day  year plot_id species_id sex   hindfoot_length weight genus  \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  \n 1     7    16  1977       2 NL         M                  32     NA Neotoma\n 2     8    19  1977       2 NL         M                  31     NA Neotoma\n 3     9    13  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 4    10    16  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 5    11    12  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 6    11    12  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 7    12    10  1977       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 8     1     8  1978       2 NL         &lt;NA&gt;               NA     NA Neotoma\n 9     2    18  1978       2 NL         M                  NA    218 Neotoma\n10     3    11  1978       2 NL         &lt;NA&gt;               NA     NA Neotoma\n# ℹ 34,776 more rows\n# ℹ 3 more variables: species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\nsurveys[-c(7:34786),] # Equivalent to head(surveys)\n\n# A tibble: 6 × 13\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1         1     7    16  1977       2 NL         M                  32     NA\n2        72     8    19  1977       2 NL         M                  31     NA\n3       224     9    13  1977       2 NL         &lt;NA&gt;               NA     NA\n4       266    10    16  1977       2 NL         &lt;NA&gt;               NA     NA\n5       349    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n6       363    11    12  1977       2 NL         &lt;NA&gt;               NA     NA\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\nIn addition to using numeric values to subset a data.frame (or matrix), columns can be called by name, using the following notations:\n\n# Only printing the first six rows as a demonstration\n\nsurveys[1:6, \"species_id\"] # Result is a data.frame\n\n# A tibble: 6 × 1\n  species_id\n  &lt;chr&gt;     \n1 NL        \n2 NL        \n3 NL        \n4 NL        \n5 NL        \n6 NL        \n\nhead(surveys$species_id) # Result is a vector\n\n[1] \"NL\" \"NL\" \"NL\" \"NL\" \"NL\" \"NL\"\n\n\n\n4.3.2.1 Challenge\n\nCreate a data.frame (surveys_200) containing only the observations from row 200 of the surveys dataset.\nNotice how nrow() gave you the number of rows in a data.frame?\n\nUse that number to pull out just that last row in the data frame.\nCompare that with what you see as the last row using tail() to make sure it’s meeting expectations.\nPull out that last row using nrow() instead of the row number.\nCreate a new data frame object (surveys_last) from that last row.\n\nUse nrow() to extract the row that is in the middle of the data frame. Store the content of this row in an object named surveys_middle.\nCombine nrow() with the - notation above to reproduce the behavior of head(surveys) keeping just the first through 6th rows of the surveys dataset."
  },
  {
    "objectID": "lec03-dataframes-dplyr.html#working-with-a-subset-of-the-data",
    "href": "lec03-dataframes-dplyr.html#working-with-a-subset-of-the-data",
    "title": "4  Data frames, intro to dplyr",
    "section": "4.4 Working with a subset of the data",
    "text": "4.4 Working with a subset of the data\nSometimes when we are working with a big dataset, we might want to test our code on a smaller portion of the data first to make sure it works before running the code on the entire thing, which could take a long time. There are many ways to subset your data. Common practices include pulling a random number of samples from the full dataset or take the first n rows of your dataset. Here, we take every 8th row from the dataset to work with for the rest of the lecture. This ensures that everybody is working with the same dataset (we don’t want to randomize) and we also have a good representation of observations from different plot types (the first 5000 rows only captured two out of a total of five plot types). Think about these issues when you create your own test data set.\nWhen you work on your own projects, don’t forget to go back to the full data set for your final, official analysis!!!\n\nstr(surveys)\n\nspc_tbl_ [34,786 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ record_id      : num [1:34786] 1 72 224 266 349 363 435 506 588 661 ...\n $ month          : num [1:34786] 7 8 9 10 11 11 12 1 2 3 ...\n $ day            : num [1:34786] 16 19 13 16 12 12 10 8 18 11 ...\n $ year           : num [1:34786] 1977 1977 1977 1977 1977 ...\n $ plot_id        : num [1:34786] 2 2 2 2 2 2 2 2 2 2 ...\n $ species_id     : chr [1:34786] \"NL\" \"NL\" \"NL\" \"NL\" ...\n $ sex            : chr [1:34786] \"M\" \"M\" NA NA ...\n $ hindfoot_length: num [1:34786] 32 31 NA NA NA NA NA NA NA NA ...\n $ weight         : num [1:34786] NA NA NA NA NA NA NA NA 218 NA ...\n $ genus          : chr [1:34786] \"Neotoma\" \"Neotoma\" \"Neotoma\" \"Neotoma\" ...\n $ species        : chr [1:34786] \"albigula\" \"albigula\" \"albigula\" \"albigula\" ...\n $ taxa           : chr [1:34786] \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ...\n $ plot_type      : chr [1:34786] \"Control\" \"Control\" \"Control\" \"Control\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   record_id = col_double(),\n  ..   month = col_double(),\n  ..   day = col_double(),\n  ..   year = col_double(),\n  ..   plot_id = col_double(),\n  ..   species_id = col_character(),\n  ..   sex = col_character(),\n  ..   hindfoot_length = col_double(),\n  ..   weight = col_double(),\n  ..   genus = col_character(),\n  ..   species = col_character(),\n  ..   taxa = col_character(),\n  ..   plot_type = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsurveys_subset&lt;- surveys[seq(1, 34786, 8), ]\nstr(surveys_subset)\n\ntibble [4,349 × 13] (S3: tbl_df/tbl/data.frame)\n $ record_id      : num [1:4349] 1 588 1453 3000 5558 ...\n $ month          : num [1:4349] 7 2 11 5 3 8 11 9 12 9 ...\n $ day            : num [1:4349] 16 18 5 18 29 16 13 30 8 7 ...\n $ year           : num [1:4349] 1977 1978 1978 1980 1982 ...\n $ plot_id        : num [1:4349] 2 2 2 2 2 2 2 2 2 2 ...\n $ species_id     : chr [1:4349] \"NL\" \"NL\" \"NL\" \"NL\" ...\n $ sex            : chr [1:4349] \"M\" \"M\" \"M\" \"F\" ...\n $ hindfoot_length: num [1:4349] 32 NA NA 31 33 33 32 32 32 32 ...\n $ weight         : num [1:4349] NA 218 218 87 211 152 158 173 160 135 ...\n $ genus          : chr [1:4349] \"Neotoma\" \"Neotoma\" \"Neotoma\" \"Neotoma\" ...\n $ species        : chr [1:4349] \"albigula\" \"albigula\" \"albigula\" \"albigula\" ...\n $ taxa           : chr [1:4349] \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ...\n $ plot_type      : chr [1:4349] \"Control\" \"Control\" \"Control\" \"Control\" ..."
  },
  {
    "objectID": "lec03-dataframes-dplyr.html#exporting-data",
    "href": "lec03-dataframes-dplyr.html#exporting-data",
    "title": "4  Data frames, intro to dplyr",
    "section": "4.5 Exporting data",
    "text": "4.5 Exporting data\nAs you begin to play with your raw data, you may want to export these new, processed, datasets to share them with your collaborators or for archival.\nSimilar to the read_csv() function used for reading CSV files into R, there is a write_csv() function that generates CSV files from data frames.\nBefore using write_csv(), we are going to create a new folder, data-processed, in our working directory that will store this generated dataset. We don’t want to store manipulated datasets in the same directory as our raw data. It’s good practice to keep them separate. The raw data would ideally be put in a data-raw folder, which should only contain the raw, unaltered data, and should be left alone to make sure we don’t delete or modify it from how it was when we downloaded or recorded it ourself. Keep good notes on this original data, e.g., when did you download it and which links were used.\nManually create a new folder called “data-processed” in your directory. Alternatively, get R to help you with it. Remember to make sure that we are in the correct relative file directory!\n\nhere()\n\ndir.create(here(\"1_lectures\", \"processed_data\"))\n\nSave the new dataset to a directory where you will be able to access it for next class. There are a couple ways you could do this depending on which function you use. The function write_csv exists in the readr package, while write.csv is from base R. These two functions have slightly different arguments. Or, you can use save from base, which writes the R object into a specific file format.\n\nwrite_csv(surveys_subset,\n          file = here(\"survey_subset.csv\"))\n\n# If that fails you can always go back to base R! \nwrite.csv(surveys_subset,\n          file = here(\"survey_subset.csv\"))\n\nsave(surveys_subset, file = here(\"survey_subset.csv\"))\n\nWe are going to prepare a cleaned up version of the data without NAs.\n\n# Note that this omits observations with NA in *any* column.\n# There is no way to control which columns to use.\nsurveys_complete_naomit &lt;- na.omit(surveys)\n\n# Compare the dimensions of the original and the cleaned data frame\ndim(surveys)\n\n[1] 34786    13\n\ndim(surveys_complete_naomit)\n\n[1] 30676    13\n\n\nNow that our dataset is ready, we can save it as a CSV file in our Processed data folder.\n\n# To save to newly created directory\nwrite_csv(surveys_complete_naomit, here(\"surveys_complete_naomit.csv\"))"
  },
  {
    "objectID": "lec03-dataframes-dplyr.html#data-wrangling-with-dplyr",
    "href": "lec03-dataframes-dplyr.html#data-wrangling-with-dplyr",
    "title": "4  Data frames, intro to dplyr",
    "section": "4.6 Data wrangling with dplyr",
    "text": "4.6 Data wrangling with dplyr\nWrangling here is used in the sense of maneuvering, managing, controlling, cleaning, and turning your data upside down and inside out to look at it from different angles in order to understand it and prepare it for analyses. The package dplyr provides easy tools for the most common data manipulation tasks. It is built to work directly with data frames, with many common tasks optimized by being written in a compiled language (C++), which means that many operations run much faster than similar tools in R. An additional feature is the ability to work directly with data stored in an external database, such as SQL-databases. The ability to work with databases is great because you are able to work with much bigger datasets (100s of GB) than your computer could normally handle. We will not talk in detail about this in class, but there are great resources online to learn more (e.g., this lecture from Data Carpentry).\n\n4.6.1 Coercing as factors\nNotice for the sex of the observations, the summary is returning that there are characters in this column but not much else. Let’s take a look at the data in this column closer. Again, previous function parameters have words read in as characters but recent defaults have changed to read words in as factors.\nAs a column of character values, the relationship between the observations being recorded as “M”, or “F” are not being recognized. We will need convert this column to factor.\nLet’s overwrite the column in the original dataset. Remember, there is no undo button in programming. Double check your work before you overwrite objects\n\nsurveys_subset$sex &lt;- as.factor(surveys_subset$sex)\n\n\n\n4.6.2 Renaming columns in a dataframe\nFirst, let’s check out our surveys_subset dataframe.\n\nsurveys_subset\n\n# A tibble: 4,349 × 13\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;fct&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2       588     2    18  1978       2 NL         M                  NA    218\n 3      1453    11     5  1978       2 NL         M                  NA    218\n 4      3000     5    18  1980       2 NL         F                  31     87\n 5      5558     3    29  1982       2 NL         M                  33    211\n 6      6500     8    16  1982       2 NL         F                  33    152\n 7      8657    11    13  1983       2 NL         F                  32    158\n 8      9605     9    30  1984       2 NL         F                  32    173\n 9     11215    12     8  1985       2 NL         F                  32    160\n10     11879     9     7  1986       2 NL         F                  32    135\n# ℹ 4,339 more rows\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\nThis dataframe contains hindfoot length and weight data of several species in different taxanomic groups. While we were collecting data, we notice that we didn’t write down the units in our lab notebook of hindfoot_length and weight! Let’s add it into the column manually here so that we don’t forget.\nWe can do that by renaming our column, using the rename() function.\n\nrename(surveys_subset,\n       hindfoot_length_cm = hindfoot_length,\n       weight_g = weight)\n\n# A tibble: 4,349 × 13\n   record_id month   day  year plot_id species_id sex   hindfoot_length_cm\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;fct&gt;              &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                     32\n 2       588     2    18  1978       2 NL         M                     NA\n 3      1453    11     5  1978       2 NL         M                     NA\n 4      3000     5    18  1980       2 NL         F                     31\n 5      5558     3    29  1982       2 NL         M                     33\n 6      6500     8    16  1982       2 NL         F                     33\n 7      8657    11    13  1983       2 NL         F                     32\n 8      9605     9    30  1984       2 NL         F                     32\n 9     11215    12     8  1985       2 NL         F                     32\n10     11879     9     7  1986       2 NL         F                     32\n# ℹ 4,339 more rows\n# ℹ 5 more variables: weight_g &lt;dbl&gt;, genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;,\n#   plot_type &lt;chr&gt;\n\n\nLet’s assign this dataframe to an object, by using the put &lt;- function.\n\nsurveys_subset_units &lt;- rename(surveys_subset,\n       hindfoot_length_cm = hindfoot_length,\n       weight_g = weight)\n\n\n4.6.2.1 An aside on naming conventions\nThere are several different styles of naming objects in R. We talked about this briefly last class when discussing how to name our functions. Naming things is one of the most difficult problems in data science since it can be instrumental in helping with maintenance of code and code-sharing These styles include:\n\ncamelCase\n\nThese names start with small letter and every subsequent word will start with upperCase letter\n\nPascalCase\n\nPascalCase is just like camel case but the only difference is the first letter is also UpperCase.\n\nsnake_case\n\nThese names are all lower case with underscore between the name.\n\nkebab-case\n\nthis is the style that is recommended to be avoided, because the - sign can be mistaken for the sign for subtraction! If you have a value saved in kebab, and another value saved in case, then R will try to find the difference between these two objects, as opposed to saving it as an object.\n\n\nWhere possible, avoid re-using names of common functions and variables. This will cause confusion for the readers of your code. This comes with more practice in common R packages - once you get a sense of the names of functions and data frames that already exist, you will know to avoid them!\nOther object-naming tips:\n\nchoose a convention and stick with it!\ndon’t use dots .\nuse names that are concise, meaningful, and consistent throughout your document. This is not easy!\ngenerally, variable names should be nouns, and function names should be verbs."
  },
  {
    "objectID": "lec03-dataframes-dplyr.html#footnotes",
    "href": "lec03-dataframes-dplyr.html#footnotes",
    "title": "4  Data frames, intro to dplyr",
    "section": "",
    "text": "Every time you call the function praise() you get praised once. Can you think of creative ways to get praised faster? (Hint 1: What about those loops we talked about earlier?) (Hint 2: Can we vectorize this operation and make the praising even more efficient?)↩︎\nTo install the kittyR package, use this code: devtools::install_github(\"IndrajeetPatil/kittyR\")↩︎\nThis course is focused on tidyverse functions, because that seems to be the trend these days. Although all of our teaching material is written in tidy lingo, it is mostly for the sake of consistency. In all honesty, tidy is pretty great, but some functions are more intuitive in base, so most people code in a mix of the two. If you learned base R elsewhere and perfer to use those functions instead, by all means, go ahead. The correct code is code that does what you want it to do.↩︎"
  },
  {
    "objectID": "lec04-data-wrangling.html#lesson-preamble",
    "href": "lec04-data-wrangling.html#lesson-preamble",
    "title": "5  Data wrangling in dplyr",
    "section": "5.1 Lesson preamble",
    "text": "5.1 Lesson preamble\n\n5.1.1 Learning Objectives\n\nLearn to use data wrangling commands select, filter, %&gt;%, and mutate from the dplyr package.\nUnderstand the split-apply-combine concept for data analysis.\nUse summarize, group_by, and tally to split a data frame into groups of observations, apply a summary statistics for each group, and then combine the results.\nLearn to switch between long and wide format\n\n5.1.2 Lesson outline\n\nContinue data wrangling in dplyr (30 mins)\nSplit-apply-combine techniques in dplyr (20 mins)\nUsing group_by and tally to summarize categorical data (25 mins)\nReshaping data (15 mins)"
  },
  {
    "objectID": "lec04-data-wrangling.html#getting-ready-to-code",
    "href": "lec04-data-wrangling.html#getting-ready-to-code",
    "title": "5  Data wrangling in dplyr",
    "section": "5.2 Getting ready to code",
    "text": "5.2 Getting ready to code\nWe are going to pick up where we left off last lecture, and continue to work with the same desert animal survey data.\nWe’ll start by doing our checks. You can set up your yaml block and check your working directory with getwd() on your own. Make sure you are in the directory where you saved our dataset last time and clear your workspace for our new session. We’ll also load the required packages (mostly tidyverse for this class) and data. Make sure what you need to be comfortable is within reach such as tea, coffee, cookies, blankets.\n\n\n# setwd(\"~/Documents/UofT/PhD/Teaching/2022-2023/EEB313/2022/Lectures\")\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.2\n\n\nWarning: package 'tibble' was built under R version 4.1.2\n\n\nWarning: package 'tidyr' was built under R version 4.1.2\n\n\nWarning: package 'readr' was built under R version 4.1.2\n\n\nWarning: package 'stringr' was built under R version 4.1.2\n\n\nWarning: package 'forcats' was built under R version 4.1.2\n\n\nWarning: package 'lubridate' was built under R version 4.1.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nsurveys &lt;- read.csv(\"portal_data.csv\", na.strings = c(\"\",\".\",\"NA\"))\nsurveys %&gt;% head()\n\n  record_id month day year plot_id species_id  sex hindfoot_length weight\n1         1     7  16 1977       2         NL    M              32     NA\n2        72     8  19 1977       2         NL    M              31     NA\n3       224     9  13 1977       2         NL &lt;NA&gt;              NA     NA\n4       266    10  16 1977       2         NL &lt;NA&gt;              NA     NA\n5       349    11  12 1977       2         NL &lt;NA&gt;              NA     NA\n6       363    11  12 1977       2         NL &lt;NA&gt;              NA     NA\n    genus  species   taxa plot_type\n1 Neotoma albigula Rodent   Control\n2 Neotoma albigula Rodent   Control\n3 Neotoma albigula Rodent   Control\n4 Neotoma albigula Rodent   Control\n5 Neotoma albigula Rodent   Control\n6 Neotoma albigula Rodent   Control\n\nsurveys_subset &lt;- surveys[seq(1, 34786, 8), ]"
  },
  {
    "objectID": "lec04-data-wrangling.html#data-wrangling-with-dplyr",
    "href": "lec04-data-wrangling.html#data-wrangling-with-dplyr",
    "title": "5  Data wrangling in dplyr",
    "section": "5.3 Data wrangling with dplyr",
    "text": "5.3 Data wrangling with dplyr\nWrangling here is used in the sense of maneuvering, managing, controlling, and turning your data upside down and inside out to look at it from different angles in order to understand it. The package dplyr provides easy tools for the most common data manipulation tasks. It is built to work directly with data frames, with many common tasks optimized by being written in a compiled language (C++), which means that many operations run much faster than similar tools in R. An additional feature is the ability to work directly with data stored in an external database, such as SQL-databases. The ability to work with databases is great because you are able to work with much bigger datasets (100s of GB) than your computer could normally handle. We will not talk in detail about this in class, but there are great resources online to learn more (e.g. this lecture from Data Carpentry).\n\n5.3.1 Selecting columns and filtering rows\nWe’re going to learn some of the most common dplyr functions: select(), filter(), mutate(), group_by(), arrange, and summarise(). To select columns of a data frame, use select(). The first argument to this function is the data frame (surveys_subset), and the subsequent arguments are the columns to keep.\n\nselect(surveys_subset, plot_id, species_id, weight, year) %&gt;% head()\n\n   plot_id species_id weight year\n1        2         NL     NA 1977\n9        2         NL    218 1978\n17       2         NL    218 1978\n25       2         NL     87 1980\n33       2         NL    211 1982\n41       2         NL    152 1982\n\n\nNote: Unlike in base R, here we don’t need to use quotations around column names.\nTo choose rows based on a specific criteria, use filter():\n\nfilter(surveys_subset, year == 1995) %&gt;% head()\n\n  record_id month day year plot_id species_id sex hindfoot_length weight\n1     22044     2   4 1995       2         DM   M              37     46\n2     22550     8  26 1995       2         DM   F              37     26\n3     22997    12   2 1995       2         DM   M              36     50\n4     23136    12  21 1995       2         DM   M              33     27\n5     22441     7  20 1995       2         PP   F              19     17\n6     22669     9  23 1995       2         PP   F              20     17\n        genus      species   taxa plot_type\n1   Dipodomys     merriami Rodent   Control\n2   Dipodomys     merriami Rodent   Control\n3   Dipodomys     merriami Rodent   Control\n4   Dipodomys     merriami Rodent   Control\n5 Chaetodipus penicillatus Rodent   Control\n6 Chaetodipus penicillatus Rodent   Control\n\n\nNote2: To check for equality, R requires two equal signs (==). This is different than object assignment where we use (‘&lt;-’) or (‘=’) to assign values to an object. With filter we want to pull out all rows where year is equal to 1995 not assign the value 1995 to an object named year, so we use the == symbol.\nNote3: In general, when you want to find all rows that equal a numeric number you don’t have to use quotes. However, when you want to find all rows that equal a character you do need to put quotes around the value (for instance taxa == \"Rodent\").\n\n5.3.1.1 An aside on conditionals\nWithin filter you might want to filter rows using conditionals. Basic conditionals in R are broadly similar to how they’re expressed mathematically:\n\n2 &lt; 3\n\n[1] TRUE\n\n5 &gt; 9\n\n[1] FALSE\n\n5 == 5\n\n[1] TRUE\n\n\nHowever, there are a few idiosyncrasies to be mindful of for other conditionals:\n\n2 != 3 # Not equal\n\n[1] TRUE\n\n2 &lt;= 3 # Less than or equal to\n\n[1] TRUE\n\n5 &gt;= 9 # Greater than or equal to\n\n[1] FALSE\n\n\nFinally, the %in% operator is used to check for membership:\n\n2 %in% c(2, 3, 4) # Checks whether 2 is in c(2, 3, 4), returns logical vector\n\n[1] TRUE\n\n\nAll of the above conditionals are compatible with filter, with the key difference being that filter expects column names as part of conditional statements instead of individual numbers.\n\n\n\n5.3.2 Chaining functions together using pipes\nBut what if you wanted to select and filter at the same time? There are three ways to do this: use intermediate steps, nesting functions, or pipes.\nWith intermediate steps, you essentially create a temporary data frame and use that as input to the next function:\n\ntemp_df &lt;- select(surveys_subset, plot_id, species_id, weight, year)\nfilter(temp_df, year == 1995) %&gt;% head()\n\n  plot_id species_id weight year\n1       2         DM     46 1995\n2       2         DM     26 1995\n3       2         DM     50 1995\n4       2         DM     27 1995\n5       2         PP     17 1995\n6       2         PP     17 1995\n\n\nThis can quickly clutter up your workspace with lots of objects.\nYou can also nest functions (i.e., one function inside of another).\n\nfilter(select(surveys_subset, plot_id, species_id, weight, year), year == 1995) %&gt;% head()\n\n  plot_id species_id weight year\n1       2         DM     46 1995\n2       2         DM     26 1995\n3       2         DM     50 1995\n4       2         DM     27 1995\n5       2         PP     17 1995\n6       2         PP     17 1995\n\n\nThis is handy, but can be difficult to read if too many functions are nested as they are evaluated from the inside out.\nThe last option, forward pipes, are a fairly recent addition to R. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset. Pipes in R look like %&gt;% and are made available via the magrittr package that is a part of the tidyverse. If you use RStudio, you can type the pipe with Ctrl/Cmd + Shift + M.\nFun fact: The name magrittr comes from the Belgian artist Rene Magritte, who has a painting called “The Treachery of Images” that says in French “This is not a pipe”.\n\nsurveys_subset %&gt;% \n    select(., plot_id, species_id, weight, year) %&gt;% \n    filter(., year == 1995) %&gt;% head()\n\n  plot_id species_id weight year\n1       2         DM     46 1995\n2       2         DM     26 1995\n3       2         DM     50 1995\n4       2         DM     27 1995\n5       2         PP     17 1995\n6       2         PP     17 1995\n\n\nThe . refers to the object that is passed from the previous line. In this example, the data frame surveys_subset is passed to the . in the select() statement. Then, the modified data frame (which is the result of the select() operation) is passed to the . in the filter() statement. Put more simply: whatever was the result from the line above will be used in the current line.\nSince it gets a bit tedious to write out all the dots, dplyr allows for them to be omitted. In the dplyr family of functions, the first argument is always a data frame, and by default the pipe will pass the output from the line above to this argument. The chunk below, with the . omitted, gives the same output as the one above:\n\nsurveys_subset %&gt;% \n    select(plot_id, species_id, weight, year) %&gt;% \n    filter(year == 1995) %&gt;% head()\n\n  plot_id species_id weight year\n1       2         DM     46 1995\n2       2         DM     26 1995\n3       2         DM     50 1995\n4       2         DM     27 1995\n5       2         PP     17 1995\n6       2         PP     17 1995\n\n\nIf this runs off your screen and you just want to see the first few rows, you can use a pipe to view the head() of the data. Pipes work with non-dplyr functions, too, as long as either the dplyr or magrittr package is loaded.\n\nsurveys_subset %&gt;% \n    select(plot_id, species_id, weight, year) %&gt;% \n    filter(year == 1995) %&gt;%\n    head()\n\n  plot_id species_id weight year\n1       2         DM     46 1995\n2       2         DM     26 1995\n3       2         DM     50 1995\n4       2         DM     27 1995\n5       2         PP     17 1995\n6       2         PP     17 1995\n\n\nIf we wanted to create a new object with this smaller version of the data, we could do so by assigning it a new name:\n\nsurveys_1995 &lt;- surveys_subset %&gt;% \n    select(plot_id, species_id, weight, year) %&gt;% \n    filter(year == 1995)\n\nsurveys_1995 %&gt;% head()\n\n  plot_id species_id weight year\n1       2         DM     46 1995\n2       2         DM     26 1995\n3       2         DM     50 1995\n4       2         DM     27 1995\n5       2         PP     17 1995\n6       2         PP     17 1995\n\n\n\n5.3.2.1 Challenge\nUse the pipe to subset the data frame, keeping only rows where weight is less than 10, and only the columns species_id, sex, and weight.\n\nsurveys_subset %&gt;%\n  filter(weight &lt; 10) %&gt;%\n  select(species_id, sex, weight) %&gt;% head()\n\n  species_id sex weight\n1         PF   M      8\n2         PF   M      6\n3         RM   F      8\n4         RM   F      8\n5         PF   F      6\n6         PF   M      8\n\n\nWe could write a single expression to filter for several criteria, either matching all criteria (&) or any criteria (|):\n\nsurveys_subset %&gt;% \n    filter(taxa == 'Rodent' & sex == 'F') %&gt;% \n    select(sex, taxa) %&gt;% head()\n\n  sex   taxa\n1   F Rodent\n2   F Rodent\n3   F Rodent\n4   F Rodent\n5   F Rodent\n6   F Rodent\n\n\n\nsurveys_subset %&gt;% \n    filter(species == 'clarki' | species == 'audubonii') %&gt;% \n    select(species, taxa) %&gt;% head()\n\n    species    taxa\n1 audubonii  Rabbit\n2 audubonii  Rabbit\n3 audubonii  Rabbit\n4 audubonii  Rabbit\n5    clarki Reptile\n6 audubonii  Rabbit\n\n\n\n\n\n5.3.3 Creating new columns with mutate\nFrequently, you’ll want to create new columns based on the values in existing columns. For instance, you might want to do unit conversions, or find the ratio of values in two columns. For this we’ll use mutate().\nTo create a new column of weight in kg:\n\nsurveys_subset %&gt;%\n    mutate(weight_kg = weight / 1000) %&gt;% head()\n\n   record_id month day year plot_id species_id sex hindfoot_length weight\n1          1     7  16 1977       2         NL   M              32     NA\n9        588     2  18 1978       2         NL   M              NA    218\n17      1453    11   5 1978       2         NL   M              NA    218\n25      3000     5  18 1980       2         NL   F              31     87\n33      5558     3  29 1982       2         NL   M              33    211\n41      6500     8  16 1982       2         NL   F              33    152\n     genus  species   taxa plot_type weight_kg\n1  Neotoma albigula Rodent   Control        NA\n9  Neotoma albigula Rodent   Control     0.218\n17 Neotoma albigula Rodent   Control     0.218\n25 Neotoma albigula Rodent   Control     0.087\n33 Neotoma albigula Rodent   Control     0.211\n41 Neotoma albigula Rodent   Control     0.152\n\n\nYou can also create a second new column based on the first new column within the same call of mutate():\n\nsurveys_subset %&gt;%\n    mutate(weight_kg = weight / 1000,\n           weight_kg2 = weight_kg * 2) %&gt;% head()\n\n   record_id month day year plot_id species_id sex hindfoot_length weight\n1          1     7  16 1977       2         NL   M              32     NA\n9        588     2  18 1978       2         NL   M              NA    218\n17      1453    11   5 1978       2         NL   M              NA    218\n25      3000     5  18 1980       2         NL   F              31     87\n33      5558     3  29 1982       2         NL   M              33    211\n41      6500     8  16 1982       2         NL   F              33    152\n     genus  species   taxa plot_type weight_kg weight_kg2\n1  Neotoma albigula Rodent   Control        NA         NA\n9  Neotoma albigula Rodent   Control     0.218      0.436\n17 Neotoma albigula Rodent   Control     0.218      0.436\n25 Neotoma albigula Rodent   Control     0.087      0.174\n33 Neotoma albigula Rodent   Control     0.211      0.422\n41 Neotoma albigula Rodent   Control     0.152      0.304\n\n\nWe can see that there is some NAs in our new column. If we wanted to remove those we could insert a filter() in the chain, paired with the !is.na notation we learned in the last lecture:\n\nsurveys_subset %&gt;%\n    filter(!is.na(weight)) %&gt;%\n    mutate(weight_kg = weight / 1000) %&gt;% head()\n\n  record_id month day year plot_id species_id sex hindfoot_length weight\n1       588     2  18 1978       2         NL   M              NA    218\n2      1453    11   5 1978       2         NL   M              NA    218\n3      3000     5  18 1980       2         NL   F              31     87\n4      5558     3  29 1982       2         NL   M              33    211\n5      6500     8  16 1982       2         NL   F              33    152\n6      8657    11  13 1983       2         NL   F              32    158\n    genus  species   taxa plot_type weight_kg\n1 Neotoma albigula Rodent   Control     0.218\n2 Neotoma albigula Rodent   Control     0.218\n3 Neotoma albigula Rodent   Control     0.087\n4 Neotoma albigula Rodent   Control     0.211\n5 Neotoma albigula Rodent   Control     0.152\n6 Neotoma albigula Rodent   Control     0.158\n\n\n\n5.3.3.1 Challenge\nCreate a new data frame from the surveys_subset data that meets the following criteria: contains only the species_id column and a new column called hindfoot_half containing values that are half the hindfoot_length values. In this hindfoot_half column, there should be no NAs and all values should be less than 30. (Hint: think about how the commands should be ordered to produce this data frame.)\n\n## Answer\nsurveys_hindfoot_half &lt;- surveys_subset %&gt;%\n    filter(!is.na(hindfoot_length)) %&gt;%\n    mutate(hindfoot_half = hindfoot_length / 2) %&gt;%\n    filter(hindfoot_half &lt; 30) %&gt;%\n    select(species_id, hindfoot_half)\n\nsurveys_hindfoot_half %&gt;% head()\n\n  species_id hindfoot_half\n1         NL          16.0\n2         NL          15.5\n3         NL          16.5\n4         NL          16.5\n5         NL          16.0\n6         NL          16.0"
  },
  {
    "objectID": "lec04-data-wrangling.html#split-apply-combine-techniques-in-dplyr",
    "href": "lec04-data-wrangling.html#split-apply-combine-techniques-in-dplyr",
    "title": "5  Data wrangling in dplyr",
    "section": "5.4 Split-apply-combine techniques in dplyr",
    "text": "5.4 Split-apply-combine techniques in dplyr\nMany data analysis tasks can be approached using the split-apply-combine paradigm: split the data into groups, apply some analysis to each group, and then combine the results.\ndplyr facilitates this workflow through the use of group_by() and summarize(), which collapses each group into a single-row summary of that group. The arguments to group_by() are the column names that contain the categorical variables for which you want to calculate the summary statistics. Let’s view the mean weight by sex.\n\nsurveys_subset %&gt;%\n    group_by(sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n# A tibble: 3 × 2\n  sex   mean_weight\n  &lt;chr&gt;       &lt;dbl&gt;\n1 F              NA\n2 M              NA\n3 &lt;NA&gt;           NA\n\n\nThe mean weights become NA since there are individual observations that are NA. Let’s remove those observations.\n\nsurveys_subset %&gt;%\n    filter(!is.na(weight)) %&gt;%\n    group_by(sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n# A tibble: 3 × 2\n  sex   mean_weight\n  &lt;chr&gt;       &lt;dbl&gt;\n1 F            43.0\n2 M            42.1\n3 &lt;NA&gt;         60.2\n\n\nThere is one row here that is neither male nor female. These are observations where the animal escaped before the sex could be determined. Let’s remove those as well.\n\nsurveys_subset %&gt;%\n    filter(!is.na(weight) & !is.na(sex)) %&gt;%\n    group_by(sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n# A tibble: 2 × 2\n  sex   mean_weight\n  &lt;chr&gt;       &lt;dbl&gt;\n1 F            43.0\n2 M            42.1\n\n\nYou can also group by multiple columns:\n\nsurveys_subset %&gt;%\n    filter(!is.na(weight) & !is.na(sex)) %&gt;%\n    group_by(genus, sex) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n`summarise()` has grouped output by 'genus'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 18 × 3\n# Groups:   genus [9]\n   genus           sex   mean_weight\n   &lt;chr&gt;           &lt;chr&gt;       &lt;dbl&gt;\n 1 Baiomys         F            7.5 \n 2 Baiomys         M            8   \n 3 Chaetodipus     F           23.8 \n 4 Chaetodipus     M           24.3 \n 5 Dipodomys       F           55.8 \n 6 Dipodomys       M           55.5 \n 7 Neotoma         F          157.  \n 8 Neotoma         M          180.  \n 9 Onychomys       F           26.2 \n10 Onychomys       M           25.9 \n11 Perognathus     F            9   \n12 Perognathus     M            8.12\n13 Peromyscus      F           22.7 \n14 Peromyscus      M           20.3 \n15 Reithrodontomys F           11.5 \n16 Reithrodontomys M            9.96\n17 Sigmodon        F           84   \n18 Sigmodon        M           57.1 \n\n\nSince we will use the same filtered and grouped data frame in multiple code chunks below, we could assign this subset of the data to a new name and use this data frame in the subsequent code chunks instead of typing out the functions each time.\n\nfiltered_surveys &lt;- surveys_subset %&gt;%\n    filter(!is.na(weight) & !is.na(sex)) %&gt;%\n    group_by(genus, sex)\n\nOnce the data are grouped, you can also summarize multiple variables at the same time. For instance, we could add a column indicating the minimum weight for each species for each sex:\n\nfiltered_surveys %&gt;%\n    summarize(mean_weight = mean(weight),\n              min_weight = min(weight))\n\n`summarise()` has grouped output by 'genus'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 18 × 4\n# Groups:   genus [9]\n   genus           sex   mean_weight min_weight\n   &lt;chr&gt;           &lt;chr&gt;       &lt;dbl&gt;      &lt;int&gt;\n 1 Baiomys         F            7.5           7\n 2 Baiomys         M            8             8\n 3 Chaetodipus     F           23.8           5\n 4 Chaetodipus     M           24.3           7\n 5 Dipodomys       F           55.8          13\n 6 Dipodomys       M           55.5          18\n 7 Neotoma         F          157.           68\n 8 Neotoma         M          180.           48\n 9 Onychomys       F           26.2          10\n10 Onychomys       M           25.9           9\n11 Perognathus     F            9             6\n12 Perognathus     M            8.12          5\n13 Peromyscus      F           22.7          10\n14 Peromyscus      M           20.3           9\n15 Reithrodontomys F           11.5           5\n16 Reithrodontomys M            9.96          4\n17 Sigmodon        F           84            50\n18 Sigmodon        M           57.1          35\n\n\n\n5.4.0.1 Challenge\n\nUsing the surveys_subset dataframe, use group_by() and summarize() to find the mean hindfoot length for the species Ammospermophilus harrisi.\nWhat was the heaviest animal measured in 1979?\n\n\n# Answer 1\nsurveys_subset %&gt;%\n    filter(!is.na(hindfoot_length)) %&gt;%\n    group_by(species) %&gt;%\n    summarize(mean_hindfoot_length = mean(hindfoot_length)) # 31\n\n# A tibble: 22 × 2\n   species     mean_hindfoot_length\n   &lt;chr&gt;                      &lt;dbl&gt;\n 1 albigula                    32.4\n 2 baileyi                     26.2\n 3 eremicus                    20.1\n 4 flavus                      15.8\n 5 fulvescens                  17.4\n 6 fulviventer                 25  \n 7 harrisi                     31  \n 8 hispidus                    28.3\n 9 intermedius                 22.5\n10 leucogaster                 20.7\n# ℹ 12 more rows\n\n# Answer 2\nsurveys_subset %&gt;%\n    filter(!is.na(weight)) %&gt;%\n    filter(year == 1979) %&gt;% \n    filter(weight == max(weight)) %&gt;% \n    select(genus, species) # Neotoma albigula (white-throated woodrat)\n\n    genus  species\n1 Neotoma albigula"
  },
  {
    "objectID": "lec04-data-wrangling.html#using-tally-to-summarize-categorical-data",
    "href": "lec04-data-wrangling.html#using-tally-to-summarize-categorical-data",
    "title": "5  Data wrangling in dplyr",
    "section": "5.5 Using tally to summarize categorical data",
    "text": "5.5 Using tally to summarize categorical data\nWhen working with data, it is also common to want to know the number of observations found for each factor or combination of factors. For this, dplyr provides tally(). For example, if we want to group by taxa and find the number of observations for each taxa, we would do:\n\nsurveys_subset %&gt;%\n    group_by(taxa) %&gt;%\n    tally()\n\n# A tibble: 4 × 2\n  taxa        n\n  &lt;chr&gt;   &lt;int&gt;\n1 Bird       57\n2 Rabbit      7\n3 Reptile     3\n4 Rodent   4282\n\n\nYou can also use count() to quickly count the unique values of one or more variables. count() combines group_by() and summarise(), so the following will give the same result as the code above:\n\nsurveys_subset %&gt;%\n    count(taxa)\n\n     taxa    n\n1    Bird   57\n2  Rabbit    7\n3 Reptile    3\n4  Rodent 4282\n\n\nWe can also use tally() or count() when grouping on multiple variables:\n\nsurveys_subset %&gt;%\n    group_by(taxa, sex) %&gt;%\n    tally()\n\n# A tibble: 6 × 3\n# Groups:   taxa [4]\n  taxa    sex       n\n  &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;\n1 Bird    &lt;NA&gt;     57\n2 Rabbit  &lt;NA&gt;      7\n3 Reptile &lt;NA&gt;      3\n4 Rodent  F      1979\n5 Rodent  M      2141\n6 Rodent  &lt;NA&gt;    162\n\nsurveys_subset %&gt;% \n  count(taxa, sex)\n\n     taxa  sex    n\n1    Bird &lt;NA&gt;   57\n2  Rabbit &lt;NA&gt;    7\n3 Reptile &lt;NA&gt;    3\n4  Rodent    F 1979\n5  Rodent    M 2141\n6  Rodent &lt;NA&gt;  162\n\n\nHere, tally() is the action applied to the groups created by group_by() and counts the total number of records for each category.\nIf there are many groups, tally() is not that useful on its own. For example, when we want to view the five most abundant species among the observations:\n\nsurveys_subset %&gt;%\n    group_by(species) %&gt;%\n    tally()\n\n# A tibble: 32 × 2\n   species             n\n   &lt;chr&gt;           &lt;int&gt;\n 1 albigula          158\n 2 audubonii           7\n 3 baileyi           356\n 4 bilineata          36\n 5 brunneicapillus     5\n 6 chlorurus           6\n 7 clarki              1\n 8 eremicus          164\n 9 flavus            199\n10 fulvescens          8\n# ℹ 22 more rows\n\n\nSince there are 32 rows in this output, we would like to order the table to display the most abundant species first. In dplyr, we say that we want to arrange() the data.\n\nsurveys_subset %&gt;%\n    group_by(species) %&gt;%\n    tally() %&gt;%\n    arrange(n) # `n` is the name of the column `tally` generated\n\n# A tibble: 32 × 2\n   species             n\n   &lt;chr&gt;           &lt;int&gt;\n 1 clarki              1\n 2 montanus            1\n 3 savannarum          1\n 4 squamata            1\n 5 intermedius         2\n 6 melanocorys         2\n 7 taylori             4\n 8 brunneicapillus     5\n 9 gramineus           5\n10 leucopus            5\n# ℹ 22 more rows\n\n\nStill not that useful. Since we are interested in the most abundant species, we want to display those with the highest count first, in other words, we want to arrange the column n in descending order:\n\nsurveys_subset %&gt;%\n    group_by(species) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n)) %&gt;%\n    head(5)\n\n# A tibble: 5 × 2\n  species          n\n  &lt;chr&gt;        &lt;int&gt;\n1 merriami      1324\n2 penicillatus   394\n3 ordii          378\n4 baileyi        356\n5 megalotis      328\n\n\nIf we want to include more attributes (like taxa and genus) about these species, we can include these in the call to group_by():\n\nsurveys_subset %&gt;%\n    group_by(species, taxa, genus) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n)) %&gt;%\n    head(5)\n\n# A tibble: 5 × 4\n# Groups:   species, taxa [5]\n  species      taxa   genus               n\n  &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;           &lt;int&gt;\n1 merriami     Rodent Dipodomys        1324\n2 penicillatus Rodent Chaetodipus       394\n3 ordii        Rodent Dipodomys         378\n4 baileyi      Rodent Chaetodipus       356\n5 megalotis    Rodent Reithrodontomys   328\n\n\nHere, we are using additional columns that are unique. Be careful not to include anything that would split the group into subgroups, such as sex, year etc.\n\n5.5.0.1 Challenge\nHow many individuals were caught in the rodent enclosure plot type?\n\nsurveys_subset %&gt;%\n    group_by(plot_type) %&gt;%\n    tally()\n\n# A tibble: 5 × 2\n  plot_type                     n\n  &lt;chr&gt;                     &lt;int&gt;\n1 Control                    1951\n2 Long-term Krat Exclosure    639\n3 Rodent Exclosure            530\n4 Short-term Krat Exclosure   739\n5 Spectab exclosure           490\n\n\n\n\n5.5.0.2 Challenge\nYou saw above how to count the number of individuals of each sex using a combination of group_by() and tally(). How could you get the same result using group_by() and summarize()? (Hint: see ?n.)\n\nsurveys_subset %&gt;%\n  group_by(sex) %&gt;%\n  summarize(n = n())\n\n# A tibble: 3 × 2\n  sex       n\n  &lt;chr&gt; &lt;int&gt;\n1 F      1979\n2 M      2141\n3 &lt;NA&gt;    229"
  },
  {
    "objectID": "lec04-data-wrangling.html#reshaping-with-gather-and-spread",
    "href": "lec04-data-wrangling.html#reshaping-with-gather-and-spread",
    "title": "5  Data wrangling in dplyr",
    "section": "5.6 Reshaping with gather and spread",
    "text": "5.6 Reshaping with gather and spread\nThe survey data presented here is almost in what we call a long format – every observation of every individual is its own row. This is an ideal format for data with a rich set of information per observation. It makes it difficult, however, to look at the relationships between measurements across plots. For example, what is the relationship between mean weights of different genera across all plots?\nTo answer that question, we want each plot to have a single row, with all of the measurements in a single plot having their own column. This is called a wide data format. For the surveys_subset data as we have it right now, this is going to be one heck of a wide data frame! However, if we were to summarize data within plots and species, we can reduce the dataset and begin to look for some relationships we’d want to examine. We need to create a new table where each row is the values for a particular variable associated with each plot. In practical terms, this means the values in genus would become the names of column variables and the cells would contain the values of the mean weight observed on each plot by genus.\nSo, in summary:\nLong format:\n\nevery column is a variable\n\nfirst column(s) repeat\n\nevery row is an observation\n\nWide format:\n\neach row is a measured thing\neach column is an independent observation\n\nfirst column does not repeat\n\n\nWe can use the functions called pivot_wider() and pivot_longer() (these are newer replacements for spread() and gather(), which were the older functions). Both functions are explained, but take some time to see what you prefer using!\nLet’s start by using dplyr to create a data frame with the mean body weight of each genus by plot.\n\nsurveys_gw &lt;- surveys_subset %&gt;%\n    filter(!is.na(weight)) %&gt;%\n    group_by(genus, plot_id) %&gt;%\n    summarize(mean_weight = mean(weight))\n\n`summarise()` has grouped output by 'genus'. You can override using the\n`.groups` argument.\n\nsurveys_gw %&gt;% head()\n\n# A tibble: 6 × 3\n# Groups:   genus [2]\n  genus       plot_id mean_weight\n  &lt;chr&gt;         &lt;int&gt;       &lt;dbl&gt;\n1 Baiomys           3         8  \n2 Baiomys           5         7  \n3 Baiomys          19         8  \n4 Chaetodipus       1        21.4\n5 Chaetodipus       2        24.5\n6 Chaetodipus       3        24.4\n\n\n\n5.6.1 Long to Wide with spread and pivot_wider\nNow, to make this long data wide, we use spread() from tidyr to spread out the different taxa into columns. spread() takes three arguments: the data, the key column (or column with identifying information), and the values column (the one with the numbers/values). We’ll use a pipe so we can ignore the data argument.\n\nsurveys_gw_wide1 &lt;- surveys_gw %&gt;%\n  spread(key = genus, value = mean_weight) \n\nhead(surveys_gw_wide1)\n\n# A tibble: 6 × 10\n  plot_id Baiomys Chaetodipus Dipodomys Neotoma Onychomys Perognathus Peromyscus\n    &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1       1      NA        21.4      60.8    155.      28.7       13          20.2\n2       2      NA        24.5      55.9    175.      26.6        7          22.4\n3       3       8        24.4      47.6    157.      23.5        7.5        22.4\n4       4      NA        23.1      57.6    200       29.9        7.67       20.5\n5       5       7        16.1      51.3    198.      26.3        7.6        20.5\n6       6      NA        24.2      59.1    184.      26.2        7.86       22  \n# ℹ 2 more variables: Reithrodontomys &lt;dbl&gt;, Sigmodon &lt;dbl&gt;\n\n\nNotice that some genera have NA values. That’s because some of those genera don’t have any record in that plot. Sometimes it is fine to leave those as NA. Sometimes we want to fill them as zeros, in which case we would add the argument fill=0. Remember, if arguments are presented in the correct order, you don’t have to specify them.\n\nsurveys_gw_wide1_fill0 &lt;- surveys_gw %&gt;%\n  spread(genus, mean_weight, fill = 0)\n\nhead(surveys_gw)\n\n# A tibble: 6 × 3\n# Groups:   genus [2]\n  genus       plot_id mean_weight\n  &lt;chr&gt;         &lt;int&gt;       &lt;dbl&gt;\n1 Baiomys           3         8  \n2 Baiomys           5         7  \n3 Baiomys          19         8  \n4 Chaetodipus       1        21.4\n5 Chaetodipus       2        24.5\n6 Chaetodipus       3        24.4\n\n\nAnother way to spread your data out is to use pivot_wider(), which takes 3 arguments as well: the data, the names_from column variable that will eventually become the column names, and the values_from column variable that will fill in the values.\n\nsurveys_gw_wide2 &lt;- surveys_gw %&gt;% \n  pivot_wider(names_from = genus, values_from = mean_weight)\n\nhead(surveys_gw_wide2)\n\n# A tibble: 6 × 10\n  plot_id Baiomys Chaetodipus Dipodomys Neotoma Onychomys Perognathus Peromyscus\n    &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1       3       8        24.4      47.6    157.      23.5        7.5        22.4\n2       5       7        16.1      51.3    198.      26.3        7.6        20.5\n3      19       8        25.3      41       NA       23.3        8.06       20.4\n4       1      NA        21.4      60.8    155.      28.7       13          20.2\n5       2      NA        24.5      55.9    175.      26.6        7          22.4\n6       4      NA        23.1      57.6    200       29.9        7.67       20.5\n# ℹ 2 more variables: Reithrodontomys &lt;dbl&gt;, Sigmodon &lt;dbl&gt;\n\n\nNow we can go back to our original question: what is the relationship between mean weights of different genera across all plots? We can easily see the weights for each genus in each plot!\n\n\n5.6.2 Wide to long with gather and pivot_longer\nWhat if we had the opposite problem, and wanted to go from a wide to long format? For that, we can use gather() to sweep up a set of columns into one key-value pair. We give it the arguments of a new key and value column name, and then we specify which columns we either want or do not want gathered up. So, to go backwards from surveys_gw_wide, and exclude plot_id from the gathering, we would do the following:\n\nsurveys_gw_long1 &lt;- surveys_gw_wide1 %&gt;%\n  gather(genus, mean_weight, -plot_id) \n\nhead(surveys_gw_long1)\n\n# A tibble: 6 × 3\n  plot_id genus   mean_weight\n    &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n1       1 Baiomys          NA\n2       2 Baiomys          NA\n3       3 Baiomys           8\n4       4 Baiomys          NA\n5       5 Baiomys           7\n6       6 Baiomys          NA\n\n\nNote that now the NA genera are included in the long format. Going from wide to long to wide can be a useful way to balance out a dataset so every replicate has the same composition.\nWe could also have used a specification for what columns to include. This can be useful if you have a large number of identifying columns, and it’s easier to specify what to gather than what to leave alone. And if the columns are sequential, we don’t even need to list them all out - just use the : operator! Say we only wanted the columns from Baiomys to Onychomys to be gathered together.\n\nsurveys_gw_wide1 %&gt;%\n  gather(genus, mean_weight_of_some_genera, Baiomys:Onychomys)\n\n# A tibble: 120 × 7\n   plot_id Perognathus Peromyscus Reithrodontomys Sigmodon genus  \n     &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  \n 1       1       13          20.2           13        NA   Baiomys\n 2       2        7          22.4           10.6      71.5 Baiomys\n 3       3        7.5        22.4            9.85     70.7 Baiomys\n 4       4        7.67       20.5            9.5      NA   Baiomys\n 5       5        7.6        20.5           11.3      NA   Baiomys\n 6       6        7.86       22             11.7      83.5 Baiomys\n 7       7       NA          21.3           11.2      NA   Baiomys\n 8       8        6.83       17.3           10        NA   Baiomys\n 9       9        7.75       18.8           11.8      36   Baiomys\n10      10       NA          20.5           10.2     110   Baiomys\n# ℹ 110 more rows\n# ℹ 1 more variable: mean_weight_of_some_genera &lt;dbl&gt;\n\n\nAnother method is to use pivot_longer(), which takes 4 arguments: the data, the names_to column variable that comes from the column names, the values_to column with the values, and cols which specifies which columns we want to keep or drop. Again, we will pipe from the dataset so we don’t have to specify the data argument:\n\nsurveys_gw_long2 &lt;- surveys_gw_wide2 %&gt;% \n  pivot_longer(names_to = \"genus\", values_to = \"mean_weight\", cols = -plot_id)\n\nsurveys_gw_long2\n\n# A tibble: 216 × 3\n   plot_id genus           mean_weight\n     &lt;int&gt; &lt;chr&gt;                 &lt;dbl&gt;\n 1       3 Baiomys                8   \n 2       3 Chaetodipus           24.4 \n 3       3 Dipodomys             47.6 \n 4       3 Neotoma              157.  \n 5       3 Onychomys             23.5 \n 6       3 Perognathus            7.5 \n 7       3 Peromyscus            22.4 \n 8       3 Reithrodontomys        9.85\n 9       3 Sigmodon              70.7 \n10       5 Baiomys                7   \n# ℹ 206 more rows\n\n\nIf the columns are directly adjacent as they are here, we don’t even need to list the all out: we can just use the : operator, as before.\n\nsurveys_gw_wide2 %&gt;% \n  pivot_longer(names_to = \"genus\", values_to = \"mean_weight\", cols = Baiomys:Sigmodon)\n\n# A tibble: 216 × 3\n   plot_id genus           mean_weight\n     &lt;int&gt; &lt;chr&gt;                 &lt;dbl&gt;\n 1       3 Baiomys                8   \n 2       3 Chaetodipus           24.4 \n 3       3 Dipodomys             47.6 \n 4       3 Neotoma              157.  \n 5       3 Onychomys             23.5 \n 6       3 Perognathus            7.5 \n 7       3 Peromyscus            22.4 \n 8       3 Reithrodontomys        9.85\n 9       3 Sigmodon              70.7 \n10       5 Baiomys                7   \n# ℹ 206 more rows\n\n\n\n5.6.2.1 Challenge\nStarting with the surveys_gw_wide2 dataset, how would you create a new dataset that gathers the mean weight of all the genera (excluding NAs) except for the genus Perognathus?\n\nsurveys_gw_wide2 %&gt;%\n  gather(genus, mean_weight, -plot_id, -Perognathus) %&gt;%\n  filter(!is.na(mean_weight)) %&gt;%\n  head()\n\n# A tibble: 6 × 4\n  plot_id Perognathus genus       mean_weight\n    &lt;int&gt;       &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1       3        7.5  Baiomys             8  \n2       5        7.6  Baiomys             7  \n3      19        8.06 Baiomys             8  \n4       3        7.5  Chaetodipus        24.4\n5       5        7.6  Chaetodipus        16.1\n6      19        8.06 Chaetodipus        25.3"
  },
  {
    "objectID": "lec05-data-visualization.html#lesson-preamble",
    "href": "lec05-data-visualization.html#lesson-preamble",
    "title": "6  Data visualization in ggplot",
    "section": "6.1 Lesson preamble",
    "text": "6.1 Lesson preamble\n\n6.1.1 Learning Objectives\n\nProduce scatter plots, line plots, and histograms using ggplot.\nSet universal plot settings.\nUnderstand how to combine dplyr and ggplot.\nUnderstand and apply faceting in ggplot.\n\n6.1.2 Lesson outline\n\nPlotting with ggplot2 (10 mins)\nBuilding plots iteratively (30 mins)\nSplit-apply-combine… plot! (20 mins)\nFaceting (10 mins)\nWhy plot? (15 mins)\nGeneralizable plots (15 mins)\nExporting (10 mins)\n\n\n\nNow we have seen how to get our dataset in our desired shape and form (aka “tidy”, where every column is a variable, and every row is an observation), we are of course itching to actually see what the data actually looks like. Luckily, our favourite package-of-packages tidyverse got us covered – it comes with a wonderful package for generating graphics called ggplot2!\nSo we’ll go ahead and load that up and let’s get graphing! We will use the subset data again but remember to plot everything for your projects.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.2\n\n\nWarning: package 'tibble' was built under R version 4.1.2\n\n\nWarning: package 'tidyr' was built under R version 4.1.2\n\n\nWarning: package 'readr' was built under R version 4.1.2\n\n\nWarning: package 'stringr' was built under R version 4.1.2\n\n\nWarning: package 'forcats' was built under R version 4.1.2\n\n\nWarning: package 'lubridate' was built under R version 4.1.2\n\nlibrary(here)\n\n# from last class\n# download file\ndownload.file(\"https://ndownloader.figshare.com/files/2292169\",\n              here(\"portal_data.csv\"))\n\n# read file into R\nportal_data &lt;- read_csv(here('portal_data.csv'))\n\n# subset every 8th row\nsurveys_subset &lt;- portal_data[seq(1, 34786, 8), ]\n\n# save subsetted dataframe into a new .csv in your file folder\nwrite.csv(surveys_subset, file = here(\"survey_subset.csv\"))\n\n# if you already have surveys_subset.csv in your file folder, you only have to run this\nsurveys_subset &lt;- read_csv(here('survey_subset.csv'))\n\n# always good to inspect your data frame to make sure there are no errors\nhead(surveys_subset)\n\n# A tibble: 6 × 14\n   ...1 record_id month   day  year plot_id species_id sex   hindfoot_length\n  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n1     1         1     7    16  1977       2 NL         M                  32\n2     2       588     2    18  1978       2 NL         M                  NA\n3     3      1453    11     5  1978       2 NL         M                  NA\n4     4      3000     5    18  1980       2 NL         F                  31\n5     5      5558     3    29  1982       2 NL         M                  33\n6     6      6500     8    16  1982       2 NL         F                  33\n# ℹ 5 more variables: weight &lt;dbl&gt;, genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;,\n#   plot_type &lt;chr&gt;\n\nstr(surveys_subset)\n\nspc_tbl_ [4,349 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ ...1           : num [1:4349] 1 2 3 4 5 6 7 8 9 10 ...\n $ record_id      : num [1:4349] 1 588 1453 3000 5558 ...\n $ month          : num [1:4349] 7 2 11 5 3 8 11 9 12 9 ...\n $ day            : num [1:4349] 16 18 5 18 29 16 13 30 8 7 ...\n $ year           : num [1:4349] 1977 1978 1978 1980 1982 ...\n $ plot_id        : num [1:4349] 2 2 2 2 2 2 2 2 2 2 ...\n $ species_id     : chr [1:4349] \"NL\" \"NL\" \"NL\" \"NL\" ...\n $ sex            : chr [1:4349] \"M\" \"M\" \"M\" \"F\" ...\n $ hindfoot_length: num [1:4349] 32 NA NA 31 33 33 32 32 32 32 ...\n $ weight         : num [1:4349] NA 218 218 87 211 152 158 173 160 135 ...\n $ genus          : chr [1:4349] \"Neotoma\" \"Neotoma\" \"Neotoma\" \"Neotoma\" ...\n $ species        : chr [1:4349] \"albigula\" \"albigula\" \"albigula\" \"albigula\" ...\n $ taxa           : chr [1:4349] \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ...\n $ plot_type      : chr [1:4349] \"Control\" \"Control\" \"Control\" \"Control\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ...1 = col_double(),\n  ..   record_id = col_double(),\n  ..   month = col_double(),\n  ..   day = col_double(),\n  ..   year = col_double(),\n  ..   plot_id = col_double(),\n  ..   species_id = col_character(),\n  ..   sex = col_character(),\n  ..   hindfoot_length = col_double(),\n  ..   weight = col_double(),\n  ..   genus = col_character(),\n  ..   species = col_character(),\n  ..   taxa = col_character(),\n  ..   plot_type = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt;"
  },
  {
    "objectID": "lec05-data-visualization.html#plotting-with-ggplot2",
    "href": "lec05-data-visualization.html#plotting-with-ggplot2",
    "title": "6  Data visualization in ggplot",
    "section": "6.2 Plotting with ggplot2",
    "text": "6.2 Plotting with ggplot2\nggplot2 is a plotting package that makes it simple to create complex plots from data frames. The name ggplot2 comes from its inspiration, the book A Grammar of Graphics, and the main goal is to allow coders to distill complex data structure and express their desired graphical outcome in a concise manner instead of telling the computer every detail about what should happen. For example, you would say “colour my data by species” instead of “go through this data frame and plot any observations of species1 in blue, any observations of species2 in red, etc”. Thanks to this functional way of interfaces with data, various plot elements interact seamlessly, publication-quality plots can be created with minimal amounts of adjustment and tweaking.\nggplot2 graphics are built step by step by adding new elements, or layers. Adding layers in this fashion allows for extensive flexibility and customization of plots. To build a ggplot, we need to:\n\nUse the ggplot() function and bind the plot to a specific data frame using the data argument\n\nRemember, if the arguments are provided in the right order then the names of the arguments can be omitted.\n\nDefine aesthetics (aes), by selecting the variables to be plotted and the variables to define the presentation such as plotting size, shape colour, etc.\n\n\nggplot(surveys_subset, aes(x = weight, y = hindfoot_length))\n\n\n\n\n\nAdd geoms – geometrical objects as a graphical representation of the data in the plot (points, lines, bars). ggplot2 offers many different geoms. We will use a few common ones today, including:\n\ngeom_point() for scatter plots, dot plots, etc.\ngeom_line() for trend lines, time-series, etc.\ngeom_histogram() for histograms\n\n\nTo add a geom to the plot use + operator. Because we have two continuous variables, let’s use geom_point() first:\n\nggplot(surveys_subset, aes(x = weight, y = hindfoot_length)) +\n  geom_point()\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNote: Notice that triangle-! warning sign above the plot? ggplot is telling you that it wasn’t able to plot all of your data. Typically this means that there are NAs in the data, or that some data points lie outside of the bounds of the axes. Can you figure what it is in this instance?\nThe + in the ggplot2 package is particularly useful because it allows you to modify existing ggplot objects. This means you can easily set up plot “templates” and conveniently explore different types of plots. The + sign used to add layers must be placed at the end of each line containing a layer. If, instead, the + sign is added in the line before the other layer, ggplot2 will not add the new layer and R will return an error message.\nThe above plot can be generated with code like this:\n\n# Assign plot to a variable\nsurveys_plot &lt;- ggplot(surveys_subset, aes(x = weight, y = hindfoot_length))\n\n# Draw the plot\nsurveys_plot + geom_point()\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n\nAnything you put in the ggplot() function can be seen by any geom layers that you add (i.e., these are universal plot settings). This includes the x and y axis you set up in aes(). You can also specify aesthetics for a given geom independently of the aesthetics defined globally in the ggplot() function, which is particularly handy when you are building complex plots layering data from different data frames.\n\n6.2.1 Building plots iteratively\nBuilding plots with ggplot is typically an iterative process – we sequentially add more layers and options until we are satisfied. Typically, the process starts with defining the dataset we’ll use, laying the axes, and choosing a geom, as we just did:\n\nggplot(surveys_subset, aes(x = weight, y = hindfoot_length)) +\n    geom_point()\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThen, we start modifying this plot to extract more information from it. For instance, we can add the argument for transparency (alpha) to reduce overplotting:\n\nggplot(data = surveys_subset, aes(x = weight, y = hindfoot_length)) +\n    geom_point(alpha = 0.2)\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n\nBased on the hindfoot length and the weights, there appears to be 4 clusters in this data. Potentially, one of the categorical variables we have in the data could explain this pattern. Colouring the data points according to a categorical variable is an easy way to find out if there seems to be correlation. Let’s try colouring this points according to plot_type.\n\nggplot(surveys_subset, aes(x = weight, y = hindfoot_length, colour = plot_type)) +\n    geom_point(alpha = 0.2)\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n\nIt seems like the type of plot the animal was captured on correlates well with some of these clusters, but there are still many that are quite mixed. Let’s try to do better! This time, the information about the data can provide some clues to which variable to look at. The plot above suggests that there might be 4 clusters, so a variable with 4 values is a good guess for what could explain the observed pattern in the scatter plot.\nBoth dplyr and ggplot2 are developed within “the tidyverse” and can use the pipes, but you may not be able to pipe in base R functions or functions from different packages.\n\nsurveys_subset %&gt;%\n    summarize_all(n_distinct) \n\n# A tibble: 1 × 14\n   ...1 record_id month   day  year plot_id species_id   sex hindfoot_length\n  &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt;           &lt;int&gt;\n1  4349      4349    12    31    26      24         37     3              44\n# ℹ 5 more variables: weight &lt;int&gt;, genus &lt;int&gt;, species &lt;int&gt;, taxa &lt;int&gt;,\n#   plot_type &lt;int&gt;\n\n# `n_distinct` is a function that counts unique values in a set of vectors\n\nRemember that there are still NA values here, that’s why there appears to be three sexes although there is only male and female. There are four taxa so that could be a good candidate, let’s see which those are.\n\nsurveys_subset %&gt;%\n    distinct(taxa)\n\n# A tibble: 4 × 1\n  taxa   \n  &lt;chr&gt;  \n1 Rodent \n2 Bird   \n3 Rabbit \n4 Reptile\n\n\nIt seems reasonable that these taxa contain animals different enough to have diverse weights and length of their feet. Lets use this categorical variable to colour the scatter plot.\n\nggplot(surveys_subset, aes(x = weight, y = hindfoot_length, colour = taxa)) +\n    geom_point(alpha = 0.2)\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n\nOnly rodents? That was unexpected… Let’s check what’s going on.\n\nsurveys_subset %&gt;%\n    group_by(taxa) %&gt;%\n    tally()\n\n# A tibble: 4 × 2\n  taxa        n\n  &lt;chr&gt;   &lt;int&gt;\n1 Bird       57\n2 Rabbit      7\n3 Reptile     3\n4 Rodent   4282\n\n\nDefinitely mostly rodents in our data set…\n\nsurveys_subset %&gt;%\n    filter(!is.na(hindfoot_length)) %&gt;% # control by removing `!`\n    group_by(taxa) %&gt;%\n    tally()\n\n# A tibble: 1 × 2\n  taxa       n\n  &lt;chr&gt;  &lt;int&gt;\n1 Rodent  3929\n\n\n…and it turns out that only rodents have had their hindfeet measured! Rats.\nLet’s remove all animals that did not have their hindfeet measured, including those rodents that did not. We’ll also remove animals whose weights weren’t measured.\n\nsurveys_hf_wt &lt;- surveys_subset %&gt;%\n    filter(!is.na(hindfoot_length) & !is.na(weight))\n\nsurveys_hf_wt %&gt;%\n    summarize_all(n_distinct)\n\n# A tibble: 1 × 14\n   ...1 record_id month   day  year plot_id species_id   sex hindfoot_length\n  &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt;           &lt;int&gt;\n1  3844      3844    12    31    26      24         23     3              41\n# ℹ 5 more variables: weight &lt;int&gt;, genus &lt;int&gt;, species &lt;int&gt;, taxa &lt;int&gt;,\n#   plot_type &lt;int&gt;\n\n\nMaybe the genus of the animals can explain what we are seeing.\n\nggplot(surveys_hf_wt, aes(x = weight, y = hindfoot_length, colour = genus)) +\n    geom_point(alpha = 0.2)\n\n\n\n\nNow this looks good! There is a clear separation between different genus, but also significant spread within genus. For example, in the weight of the green Neotoma observations. There are also two clearly separate clusters that are both coloured in olive green (Dipodomys). Maybe separating the observations into different species would be better?\n\nggplot(surveys_hf_wt, aes(x = weight, y = hindfoot_length, colour = species)) +\n    geom_point(alpha = 0.2)\n\n\n\n\nGreat! Together with the genus plot, this definitely seem to explain most of the variation we see in the hindfoot length and weight measurements. It is still a bit messy as it appears like we have around four clusters of data points, maybe three colours of points that really stood out, but there are 21 species in the legend. Let’s investigate!\n\nsurveys_subset %&gt;%\n    filter(!is.na(hindfoot_length) & !is.na(weight)) %&gt;%\n    group_by(species) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n))\n\n# A tibble: 21 × 2\n   species          n\n   &lt;chr&gt;        &lt;int&gt;\n 1 merriami      1213\n 2 penicillatus   377\n 3 ordii          347\n 4 baileyi        346\n 5 megalotis      305\n 6 torridus       265\n 7 spectabilis    262\n 8 flavus         182\n 9 eremicus       151\n10 albigula       125\n# ℹ 11 more rows\n\n\nWhen we look at the number of observations for each species, it seems like most species are fairly well represented. However some have rarely been sampled, and there is a big drop from 104 to 19 observations. Let’s include only species with more than 100 observations and see what happens.\n\nsurveys_abun_species &lt;- surveys_subset %&gt;%\n    filter(!is.na(hindfoot_length) & !is.na(weight)) %&gt;%\n    group_by(species) %&gt;%\n    mutate(n = n()) %&gt;% # add count value to each row\n    filter(n &gt; 100) %&gt;%\n    select(-n)\n\nsurveys_abun_species\n\n# A tibble: 3,791 × 14\n# Groups:   species [12]\n    ...1 record_id month   day  year plot_id species_id sex   hindfoot_length\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n 1     4      3000     5    18  1980       2 NL         F                  31\n 2     5      5558     3    29  1982       2 NL         M                  33\n 3     6      6500     8    16  1982       2 NL         F                  33\n 4     7      8657    11    13  1983       2 NL         F                  32\n 5     8      9605     9    30  1984       2 NL         F                  32\n 6     9     11215    12     8  1985       2 NL         F                  32\n 7    10     11879     9     7  1986       2 NL         F                  32\n 8    11     12729     4    26  1987       2 NL         M                  32\n 9    12     13434     9    27  1987       2 NL         M                  33\n10    14     17230     2    25  1990       2 NL         M                  33\n# ℹ 3,781 more rows\n# ℹ 5 more variables: weight &lt;dbl&gt;, genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;,\n#   plot_type &lt;chr&gt;\n\n\nStill has 3791 observations, so only 558 observations were removed.\n\nggplot(surveys_abun_species, aes(x = weight, y = hindfoot_length, colour = species)) +\n    geom_point(alpha = 0.2)\n\n\n\n\nThe plot is now cleaner – there are less species and so less colours, and the clusters are now more distinct.\n\n6.2.1.1 Challenge\nCreate a scatter plot of hindfoot_length against species with the weight showing in different colours. Hint: If you are confused about which variable to put on the x-axis, we generally describe a plot by saying plot the y variable against the x variable.\n\n\n6.2.1.2 Take home challenge\nHow would you improve the readability of this plot?\nAnswer: using jitter"
  },
  {
    "objectID": "lec05-data-visualization.html#split-apply-combine-plot",
    "href": "lec05-data-visualization.html#split-apply-combine-plot",
    "title": "6  Data visualization in ggplot",
    "section": "6.3 Split-apply-combine… plot!",
    "text": "6.3 Split-apply-combine… plot!\nIn this section, we will learn how to work with dplyr and ggplot together. Aided by the pipes (%&gt;%), we can create a powerful data exploration workflow using these two packages.\nLet’s calculate number of counts per year for each species. First, we need to group the data and count records within each group:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n)) # Adding arrange just to compare with histogram\n\n# A tibble: 259 × 3\n# Groups:   year [26]\n    year species      n\n   &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n 1  2002 baileyi    109\n 2  1985 merriami    81\n 3  1997 merriami    74\n 4  2001 baileyi     68\n 5  1982 merriami    64\n 6  1983 merriami    63\n 7  2000 baileyi     61\n 8  1998 merriami    57\n 9  1987 merriami    56\n10  1995 merriami    55\n# ℹ 249 more rows\n\n\nWe could assign this table to a variable, and then pass that variable to ggplot().\n\nyearly_counts &lt;- surveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    arrange(desc(n))\n\nggplot(yearly_counts, aes(x = n)) +\n    geom_histogram()\n\n\n\n\nRemember that a histogram plots the number of observations based on a variable, so you only need to specify the x-axis in the ggplot() call.\nTo change up how the n variable is binned on the x-axis, you can adjust the number of bins in your plot!\n\nggplot(yearly_counts, aes(x = n)) +\n    geom_histogram(bins=10)\n\n\n\n\nCreating an intermediate variable would be preferable for time consuming calculations, because you would not want to do that operation every time you change the plot aesthetics.\nIf it is not a time consuming calculation or you would like the flexibility of changing the data summary and the plotting options in the same code chunk, you can pipe the output of your split-apply-combine operation to the plotting command:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = n)) +\n        geom_histogram()\n\n\n\n\nWe can perform a quick check that the plot corresponds to the table by colouring the histogram by species:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = n, fill = species)) + \n        geom_histogram()\n\n\n\n\nNote2: Here we are using fill to assign colours to species rather than colour. In general colour refers to the outline of points/bars or whatever it is you are plotting and fill refers to the colour that goes inside the point or bar. If you are confused, try switching out fill for colour to see what looks best!\nLet’s explore how the number of each genus varies over time. Longitudinal data can be visualized as a line plot with years on the x axis and counts on the y axis:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n)) +\n        geom_line()\n\n\n\n\nUnfortunately, this does not work because we plotted data for all the species together as one line. We need to tell ggplot to draw a line for each species by modifying the aesthetic function to include group = species:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n, group = species)) +\n        geom_line()\n\n\n\n\nWe will be able to distinguish species in the plot if we add colours (using colour also automatically groups the data):\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n, colour = species)) + # `colour` groups automatically\n        geom_line()"
  },
  {
    "objectID": "lec05-data-visualization.html#faceting",
    "href": "lec05-data-visualization.html#faceting",
    "title": "6  Data visualization in ggplot",
    "section": "6.4 Faceting",
    "text": "6.4 Faceting\nggplot has a special technique called faceting that allows the user to split one plot into multiple subplots based on a variable included in the dataset. This allows us to examine the trends associated with each grouping variable more closely. We will use it to make the plot above more readable:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n)) + \n        geom_line() +\n        facet_wrap(~species)\n\n\n\n\nNow we would like to split the line in each plot by the sex of each individual measured. To do that we need to make counts in the data frame grouped by year, species, and sex:\n\nsurveys_abun_species %&gt;%\n    group_by(year, species, sex) %&gt;%\n    tally()\n\n# A tibble: 479 × 4\n# Groups:   year, species [259]\n    year species      sex       n\n   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n 1  1977 eremicus     M         1\n 2  1977 flavus       F         2\n 3  1977 flavus       M         2\n 4  1977 megalotis    F         1\n 5  1977 merriami     F        12\n 6  1977 merriami     M        11\n 7  1977 ordii        F         2\n 8  1977 ordii        M         1\n 9  1977 penicillatus F         1\n10  1977 spectabilis  F         2\n# ℹ 469 more rows\n\n\nWe can make the faceted plot by splitting further by sex using colour (within a single plot):\n\nsurveys_abun_species %&gt;%\n    group_by(year, species, sex) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n, colour = sex)) +\n        geom_line() +\n        facet_wrap(~species)\n\n\n\n\nThere are several observations where sex was not recorded. Let’s filter out those values.\n\nsurveys_abun_species %&gt;%\n    filter(!is.na(sex)) %&gt;%\n    group_by(year, species, sex) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n, color = sex)) +\n        geom_line() +\n        facet_wrap(~species)\n\n\n\n\nIt is possible to specify exactly which colors1 to use and to change the thickness of the lines to make the them easier to distinguish.\n\nsurveys_abun_species %&gt;%\n    filter(!is.na(sex)) %&gt;%\n    group_by(year, species, sex) %&gt;%\n    tally() %&gt;%\n    ggplot(aes(x = year, y = n, colour = sex)) +\n        geom_line(size = 1) +\n        scale_colour_manual(values = c(\"black\", \"orange\")) +\n        facet_wrap(~species) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nNot sure what colours would look good on your plot? The R Community got you covered! Check out these awesome color palettes where nice-looking color combos come predefined. We especially recommend the viridis color palettes. These palettes are not only pretty, they are specifically designed to be easier to read by those with colorblindness.\nTia hinted at a Studio Ghibli colour palette during the first class, so let’s use it! You’ll need to download the ghibli package and call library().\n\n# install.packages(\"ghibli\")\nlibrary(ghibli)\n\nWarning: package 'ghibli' was built under R version 4.1.2\n\n# check out their palettes\n# display palettes w/ names\npar(mfrow=c(9,3))\nfor(i in names(ghibli_palettes)) print(ghibli_palette(i))\n\n\n\nsurveys_abun_species %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  group_by(year, species, sex) %&gt;%\n  tally() %&gt;%\n  ggplot(aes(x = year, y = n, color = sex)) +\n        geom_line(size = 1) +\n  # make sure to read the package documentation so that you know how it works!\n  scale_colour_ghibli_d(\"SpiritedMedium\", direction = -1) +\n  facet_wrap(~species)\n\n\n\n\nLastly, let’s change the x labels so that they don’t overlap, and remove the gray background so that the lines can stand out more. To customize the non-data components of the plot, we will pass some theme statements2 to ggplot.\n\nsurveys_abun_species %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  group_by(year, species, sex) %&gt;%\n  tally() %&gt;%\n  ggplot(aes(x = year, y = n, color = sex)) +\n  geom_line(size = 1) +\n  scale_colour_ghibli_d(\"SpiritedMedium\", direction = -1) +       \n  facet_wrap(~species) +\n  theme_bw() +\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(angle=30, hjust=1))\n\n\n\n\nI like to use the classic theme_classic because it also gets rid of grid lines\n\nsurveys_abun_species %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  group_by(year, species, sex) %&gt;%\n  tally() %&gt;%\n  ggplot(aes(x = year, y = n, colour = sex)) +\n  geom_line(size = 1) +\n  scale_colour_ghibli_d(\"SpiritedMedium\", direction = -1) +       \n  facet_wrap(~species) +\n  theme_classic() +\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(angle=30, hjust=1))\n\n\n\n\n\n6.4.0.1 Challenge\n\nRemember the histogram coloured according to each species? Starting from that code, how could we separate each species into its own subplot?\n\nUse the filtered data frame (surveys_abun_species) for these two questions.\n\nCreate a plot to determine to show changes in average weight over years. Which year was the average weight of the animals the highest?\nCreate a plot to show differences in yearly trends across species. Is the yearly trend the same for all species?"
  },
  {
    "objectID": "lec05-data-visualization.html#why-visualize",
    "href": "lec05-data-visualization.html#why-visualize",
    "title": "6  Data visualization in ggplot",
    "section": "6.5 Why visualize?",
    "text": "6.5 Why visualize?\nThe Datasaurus Dozen dataset is a handful of datasets that complement the dplyr package. Aside from functions, packages can also import objects.\n\nlibrary(tidyverse)\nlibrary(dplyr)\n\n# install.packages(\"datasauRus\")\nlibrary(datasauRus)\n\nWarning: package 'datasauRus' was built under R version 4.1.2\n\n\n\nsummary(datasaurus_dozen)\n\n   dataset                x               y           \n Length:1846        Min.   :15.56   Min.   : 0.01512  \n Class :character   1st Qu.:41.07   1st Qu.:22.56107  \n Mode  :character   Median :52.59   Median :47.59445  \n                    Mean   :54.27   Mean   :47.83510  \n                    3rd Qu.:67.28   3rd Qu.:71.81078  \n                    Max.   :98.29   Max.   :99.69468  \n\nhead(datasaurus_dozen)\n\n# A tibble: 6 × 3\n  dataset     x     y\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 dino     55.4  97.2\n2 dino     51.5  96.0\n3 dino     46.2  94.5\n4 dino     42.8  91.4\n5 dino     40.8  88.3\n6 dino     38.7  84.9\n\ntable(datasaurus_dozen$dataset)\n\n\n      away   bullseye     circle       dino       dots    h_lines high_lines \n       142        142        142        142        142        142        142 \nslant_down   slant_up       star    v_lines wide_lines    x_shape \n       142        142        142        142        142        142 \n\n\nThere are 13 different datasets in this one object. We will use tidyverse functions to take a look at the object, grouped by the datasets.\n\ndatasaurus_dozen %&gt;% \n    group_by(dataset) %&gt;% \n    summarize(\n      mean_x    = mean(x),\n      mean_y    = mean(y),\n      std_dev_x = sd(x),\n      std_dev_y = sd(y),\n      corr_x_y  = cor(x, y)\n    )\n\n# A tibble: 13 × 6\n   dataset    mean_x mean_y std_dev_x std_dev_y corr_x_y\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 away         54.3   47.8      16.8      26.9  -0.0641\n 2 bullseye     54.3   47.8      16.8      26.9  -0.0686\n 3 circle       54.3   47.8      16.8      26.9  -0.0683\n 4 dino         54.3   47.8      16.8      26.9  -0.0645\n 5 dots         54.3   47.8      16.8      26.9  -0.0603\n 6 h_lines      54.3   47.8      16.8      26.9  -0.0617\n 7 high_lines   54.3   47.8      16.8      26.9  -0.0685\n 8 slant_down   54.3   47.8      16.8      26.9  -0.0690\n 9 slant_up     54.3   47.8      16.8      26.9  -0.0686\n10 star         54.3   47.8      16.8      26.9  -0.0630\n11 v_lines      54.3   47.8      16.8      26.9  -0.0694\n12 wide_lines   54.3   47.8      16.8      26.9  -0.0666\n13 x_shape      54.3   47.8      16.8      26.9  -0.0656\n\n\nAll of the datasets have roughly the same mean and standard deviation along both the x and y axis.\nLet’s take a look at how the data looks in a graphical sense. We will use filter to extract the rows belonging to one dataset and then pipe that directly into a ggplot.\n\ndatasaurus_dozen %&gt;% \n  filter(dataset == \"circle\") %&gt;% \n  ggplot(aes(x=x, y=y)) +\n  geom_point()\n\n\n\n\nRemember that tidyverse’s data wranging packages use the pipe %&gt;% to move the previous output to the next line, where as ggplot uses the plus sign +\nTry editing the code above to display different datasets. Notice how different distributions of data can all give similar statistical summaries - so it’s always a good choice to visualize your data rather than relying on just numbers!\nIf we wanted to take a look at all of the datasets at once, we can also use the facet_wrap() function\n\ndatasaurus_dozen %&gt;% \n  #filter(dataset == \"circle\") %&gt;% remove filter \n  ggplot(aes(x=x, y=y, color = dataset)) + # Add color \n  geom_point() + \n  facet_wrap(~dataset, ncol = 3) + \n  theme_void() + \n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "lec05-data-visualization.html#generalizable-plots",
    "href": "lec05-data-visualization.html#generalizable-plots",
    "title": "6  Data visualization in ggplot",
    "section": "6.6 Generalizable plots",
    "text": "6.6 Generalizable plots\nRemember when we edited our code to test out multiple datasets in the datasaurus dozen object? Perhaps you copy and pasted the code several time and changed the column name? This is not optimal because if you need to change the code in one instance (for example changing the x-axis label), you’ll need to revisit ever instance that you copy and pasted to code to. This approach leads you vulnerable to errors when copy and pasting.\nOne way to make your code robust is to bring all the factors that need editing to the start of the data. This may seem cumbersome for such a simple example where we are only changing the dataset name, but we’ll return to this concept later with more complicated examples.\nLet’s grab the code we used to make one plot earlier and modify it to be more generalizable\n\nunique(datasaurus_dozen$dataset)\n\n [1] \"dino\"       \"away\"       \"h_lines\"    \"v_lines\"    \"x_shape\"   \n [6] \"star\"       \"high_lines\" \"dots\"       \"circle\"     \"bullseye\"  \n[11] \"slant_up\"   \"slant_down\" \"wide_lines\"\n\n\n\ndataset_name &lt;- \"dino\" # new addition\n\ndatasaurus_dozen %&gt;% \n  filter(dataset == dataset_name) %&gt;% # change object name \n  ggplot(aes(x=x, y=y)) +\n  geom_point()\n\n\n\n\nOnce we have converted our code to a generalized format, we can convert it into a more versatile custom function!\nRemember that curly brackets are used for inputting multiple lines of code. It is generally attached to the function that proceeds it.\n\ndino_plot &lt;- function(data_name) {\n  \n  datasaurus_dozen %&gt;% \n    filter(dataset == data_name) %&gt;% # change object name \n    ggplot(aes(x=x, y=y)) +\n    geom_point()\n}\n\ndino_plot(\"circle\")\n\n\n\ndino_plot(\"dino\")\n\n\n\ndino_plot(\"star\")"
  },
  {
    "objectID": "lec05-data-visualization.html#export-plots",
    "href": "lec05-data-visualization.html#export-plots",
    "title": "6  Data visualization in ggplot",
    "section": "6.7 Export plots",
    "text": "6.7 Export plots\nLet’s save our star-shaped plot.\n\ndino_star &lt;- datasaurus_dozen %&gt;% \n  filter(dataset == \"star\") %&gt;% \n  ggplot(aes(x=x, y=y)) +\n  geom_point()\n\nThere are multiple ways we can save a plot. Using a point-and-click method, you can display your plot in the Viewer and save straight from that panel. Run your ggplot object in the Console. Once it pops up in the Viewer, click “Export”, and save your plot! You can also readjust the width and height of your plot, so you can have a look at the size of it before saving it to any directory.\nAnother way to save a plot is by using R Graphics, which will save your plot to your working directory. Remember that you should be using relative paths!\n\ngetwd()\n\n[1] \"/Users/vicki.zhang/Documents/UofT/PhD/Teaching/2023-2024/eeb313.github.io\"\n\nlibrary(here)\nhere()\n\n[1] \"/Users/vicki.zhang/Documents/UofT/PhD/Teaching/2023-2024/eeb313.github.io\"\n\n\nUse the function png() to save your file. Make sure to run all three lines together, including dev.off(), which ensures that your graphics device is closed - otherwise, R will try to keep saving your figures!\n\npng(file = \"figures/dino_star1.png\", bg = \"transparent\")\ndino_star\ndev.off()\n\nquartz_off_screen \n                2 \n\n\nFinally, you can save your figures using ggsave(), from ggplot2.\n\nggsave(\"figures/dino_star2.png\", dino_star, width = 6, height = 8)\n\nParts of this lesson material were taken and modified from Data Carpentry under their CC-BY copyright license. See their lesson page for the original source."
  },
  {
    "objectID": "lec05-data-visualization.html#footnotes",
    "href": "lec05-data-visualization.html#footnotes",
    "title": "6  Data visualization in ggplot",
    "section": "",
    "text": "There are so many colors to chose from in R. Check out the R Color doc to find your gem.↩︎\nThe amount of control over various plot elements in ggplot is truly astonishing. Check out the complete list of themes here. Have fun!↩︎"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html",
    "href": "lec06-exploratory-data-analysis.html",
    "title": "7  Exploratory data analysis",
    "section": "",
    "text": "8 So what does this mean for our predictions?\nRight. The reason we collected this data in the first place!\nNote: preparing data for analysis is generally the most time-consuming part of any project. Establishing what you need to do with your data in order to test your hypotheses & thoroughly exploring your data and its properties are extremely useful steps in this process."
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#lesson-preamble",
    "href": "lec06-exploratory-data-analysis.html#lesson-preamble",
    "title": "7  Exploratory data analysis",
    "section": "7.1 Lesson preamble:",
    "text": "7.1 Lesson preamble:\n\n7.1.1 Lesson objectives:\n\nImplications of (not) understanding your data\n\nHow did you collect your data?\nWhat are the properties of your data?\n\nExploring and asking questions about your data with graphing/visualization\nUsing insights from exploratory analysis to clean up data:\n\nDealing with unusual values/outliers\nDealing with missing values (NAs)\n\n\n7.1.2 Lesson outline:\nTotal lesson time: 2 hours\n\nData properties, initial predictions (15 min)\nPlotting and exploring data (45 min)\nDealing with unusual values (15 min)\nRe-connecting with our predictions (30 min)\nDealing with missing values (15 min)"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#introduction",
    "href": "lec06-exploratory-data-analysis.html#introduction",
    "title": "7  Exploratory data analysis",
    "section": "7.2 Introduction",
    "text": "7.2 Introduction\nExploratory data analysis is your exciting first look at your data! It’s a chance to develop a better understanding of the variables in your data set and the relationships between them. You can check your assumptions, find outliers, and possible errors. But THEN you’ll get to ask your questions! Yay!!\nYou need to understand your data you before you analyze it.\n\nWhat kind of data is it?\nWhat variation is present in my data?\nAre there any data points with values beyond the limits I anticipated?\nDo you notice any patterns?\n\nThe patterns you see can lead you to exciting new questions you may not have anticipated!"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#setup",
    "href": "lec06-exploratory-data-analysis.html#setup",
    "title": "7  Exploratory data analysis",
    "section": "7.3 Setup",
    "text": "7.3 Setup\nWe’ll use what you’ve learned in past lectures about summarizing and visualizing data with dplyr and ggplot to get to know some data!\n\nlibrary(tidyverse)\n\n\ndownload.file(\"https://uoftcoders.github.io/rcourse/data/pseudo.ara.busco\", \n              \"pseudo.ara.busco\")\ndownload.file(\"https://uoftcoders.github.io/rcourse/data/pseudo.LTRs\", \n              \"pseudo.LTRs\")\ndownload.file(\"https://uoftcoders.github.io/rcourse/data/pseudoMol_Kdist.txt\",\n              \"pseudoMol_Kdist.txt\")\n\nWe’re going to load the genomic data we have on the frequency of genes, the frequency of a type of repetitive element (LTRs stands for Long Terminal Repeat - there’s some more info on them coming up in the ‘predictions’ section), and the approximate evolutionary age of those repetitive elements.\n\ngeneDensity &lt;- read_tsv(\"pseudo.ara.busco\", \n                        col_names = c(\"chromosome\", \"start\", \"end\", \"winNum\", \n                                      \"numElements\", \"numBases\", \"winSize\", \n                                      \"density\"))\n\nRows: 48952 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): chromosome\ndbl (7): start, end, winNum, numElements, numBases, winSize, density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nltrDensity &lt;- read_tsv(\"pseudo.LTRs\", \n                       col_names = c(\"chromosome\", \"start\", \"end\", \"winNum\", \n                                     \"numElements\", \"numBases\", \"winSize\", \n                                     \"density\"))\n\nRows: 48952 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): chromosome\ndbl (7): start, end, winNum, numElements, numBases, winSize, density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nltrAge &lt;- read_tsv(\"pseudoMol_Kdist.txt\", col_names=TRUE)\n\nRows: 42339 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): chrom\ndbl (3): start, end, K2P\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe’re using “read_tsv” because the columns in this file are separated by tabs instead of commas or white space. The LTR age data (pseudoMol_Kdist.txt) already has column names, but the other two data sets need some more information"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#what-is-my-data-actually",
    "href": "lec06-exploratory-data-analysis.html#what-is-my-data-actually",
    "title": "7  Exploratory data analysis",
    "section": "7.4 What is my data, actually?",
    "text": "7.4 What is my data, actually?\nBefore we do anything else, we have to think about where this came from & whether the data is appropriate for the kinds of questions we might have.\nThis data describes a two of the genetic units (we’ll call them “elements”) that live in one plant genome: a set of highly conserved genes and one type of transposon (a “selfish” gene that makes copies of itself at the expense of its host genome). The chromosomes have been broken down into 1Mb pieces (“windows”) that overlap each other. In each window, we know the number and size (base pairs occupied) of the conserved genes and transposons.\n\n7.4.1 Predictions\nIt’s always good to lay out your hypotheses first. It can help you figure out how you need to assemble your data in order to test those predictions effectively.\n\nIn areas where gene density is high, LTR density is low\n\nLTRs are a type of transposable element, aka “genomic parasite”\n\nThey make copies of themselves at the expense of their host genome\nThey make up a large portion of plant genomes (can be &gt;40%!)\nThe host genome wants to prevent them from replicating\n\nCertain regions of a chromosome are more tightly wound up with histones\n\nThis makes them less accessible to molecular machinery\nIf polymerases aren’t likely to access a region, the region can’t be expressed\nIf a region is unexpressed, you don’t want genes there!!\nLTRs tend to accumulate in these regions\n\nMore accessible, active regions of a chromosome have higher gene content\n\nThese regions can be expressed effectively!\nLTRs that insert into these regions have a worse impact on the host\n\nOther factors like recombination rate and methylation also support this pattern\n\nIn areas where gene density is high, LTR age will be high (old, not transposing anymore)\n\nThere won’t be many new deleterious LTR insertions\n\nFew young LTRs\n\nThe LTRs that are present in those regions can’t have lethal effects\n\nIf they’re there, their effects are unlikely to have terrible effects on fitness\nSome transposable elements have been “domesticated” by their hosts\nThis all contributes to the likelihood that LTRs present can/have persisted\nLTRs present are more likely to be older\n\n\nThe sex chromosome (LG_X) will have higher LTR density\n\nLarger proportions of sex chromosomes are less accessible\nSex chromosomes experience lower rates of recombination relative to autosomes\n\nAlso correlated with higher transposon density and lower gene density\n\nThese trends are more true for non-recombining Y chromosomes than X chromosomes\n\nRecombination can occur between the two X chromosomes in females\n\n\n\n\n\n7.4.2 First Peek\nFirst, let’s just take a quick look at the gene density data set and ask ourselves what we’re dealing with. On a very basic level, what kind of variables do we have?\nWhat is one way to view a data frame?\n\n#head(geneDensity) # prints the first 6 rows\n#tail(geneDensity) #prints the last 6 rows\nglimpse(geneDensity) #prints number of rows and columns, column names, types, and several entries\n\nRows: 48,952\nColumns: 8\n$ chromosome  &lt;chr&gt; \"LG_N\", \"LG_N\", \"LG_N\", \"LG_N\", \"LG_N\", \"LG_N\", \"LG_N\", \"L…\n$ start       &lt;dbl&gt; 0, 20000, 40000, 60000, 80000, 100000, 120000, 140000, 160…\n$ end         &lt;dbl&gt; 1000000, 1020000, 1040000, 1060000, 1080000, 1100000, 1120…\n$ winNum      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ numElements &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ numBases    &lt;dbl&gt; 2499, 2499, 10158, 11583, 11583, 11583, 11583, 11583, 1158…\n$ winSize     &lt;dbl&gt; 1e+06, 1e+06, 1e+06, 1e+06, 1e+06, 1e+06, 1e+06, 1e+06, 1e…\n$ density     &lt;dbl&gt; 0.002499, 0.002499, 0.010158, 0.011583, 0.011583, 0.011583…\n\n\nWhat are your first impressions of the data?\nWhich variables will be relevant for testing our predictions?\nSomething to note here: in this data set, there are a number of things that will be clearly"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#basic-variable-categories",
    "href": "lec06-exploratory-data-analysis.html#basic-variable-categories",
    "title": "7  Exploratory data analysis",
    "section": "7.5 Basic Variable Categories",
    "text": "7.5 Basic Variable Categories\nCommon variable types:\n\nIndependent vs dependent\nContinuous vs discrete\nQualitative: categorical/nominal, ranked/ordinal, dichotomous\nQuantitative: interval, ratio\n\nThis matters because the type of data tells us the appropriate way to visualize it:\n\nQualitative data: pie charts or bar charts\nQuantitative data: histograms, box plots"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#visualizing-your-data",
    "href": "lec06-exploratory-data-analysis.html#visualizing-your-data",
    "title": "7  Exploratory data analysis",
    "section": "7.6 Visualizing Your Data",
    "text": "7.6 Visualizing Your Data\n\n7.6.1 Describing Patterns in Histograms\nFor a given variable, you’re generally looking at the range of values and where the majority of the data lies. This gives you an idea of the distribution of your data. As you’re aware, many statistical tests make assumptions about the distribution of your input data - it’s very important to make note of the shapely properties of your data.\n\nAverage (mean, median, mode)\nRange (max, min)\nSkewness: how symmetrical is your data around the average?\n\nClassic bell curve has a skew of zero\nIf your data isn’t symmetrical, that can give you important info!\nSkewed distributions aren’t likely to be normally distributed\n\nKurtosis: how sharp is your central peak?\n\nIf your distribution is basically flat, its kurtosis is negative\nIf your distribution has a huge spike, its kurtosis will be positive\n\n\n\n\n7.6.2 Qualitative Data with Histograms\nHistograms are great for qualitative data because they visualize the number of times a given value appears in your data."
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#quantitative-data-with-histograms",
    "href": "lec06-exploratory-data-analysis.html#quantitative-data-with-histograms",
    "title": "7  Exploratory data analysis",
    "section": "7.7 Quantitative Data with Histograms",
    "text": "7.7 Quantitative Data with Histograms\nHistograms provide an important view into continuous data, providing that you tell ggplot how to group your data into discrete bins. Here, we can look at our data’s values for gene density. This density is a measurement of the number of base pairs in a 1Mb window that are part of a gene divided by the total number of base pairs in the window (1 000 000).\n\nggplot(geneDensity, aes(density)) +\n  geom_histogram(binwidth = 0.01) + \n  labs(title = \"Distribution of gene density values\",\n       x = \"Gene density\", y = \"Count (bin size = 0.01)\") # Adding labels helps!\n\n\n\n\nWhat are some words you’d use to describe this distribution?\nDoes this change\n\n7.7.1 Binning Quantitative Data\nWhen you’re subsetting continuous data into discrete bin widths, it’s important to try out different values because different bin sizes can give vastly different impressions of your data’s distribution.\n\nggplot(geneDensity, aes(density)) +\n  geom_histogram(binwidth = 0.001) +  # Teeny tiny bins\n  labs(title = \"Distribution of gene density values\",\n       x = \"Gene density\", y = \"Count (bin size = 0.001)\")\n\n\n\nggplot(geneDensity, aes(density)) +\n  geom_histogram(binwidth = 0.1) +  # Huge bins! (for this data)\n  labs(title = \"Distribution of gene density values\",\n       x = \"Gene density\", y = \"Count (bin size = 0.1)\")\n\n\n\n\nIt’s also interesting to see whether your data’s distribution is different for any of the categories you’re looking at. Is there greater variation in height among women vs humans as a whole? (Do be careful with this, because looking for patterns by poking your data into a bunch of different subsets will basically guarantee you’ll find a pattern, whether or not it’s biologically relevant.)\n\n\n7.7.2 Histogram for One Chromosome\nLet’s see whether the gene density on one of the autosomes (how about LG_2) fits the general pattern.\n(Based on our initial hypotheses, would you predict that it would?)\nIt is important to consider how your predictions may affect the way you filter your data, so be mindful about tweaking parameters (like bin width) to fit the output you expect!\n\ngeneDensity %&gt;%\n  filter(chromosome == \"LG_2\") %&gt;%\n  ggplot(aes(density)) +\n  geom_histogram(binwidth = 0.01) +\n  labs(title = \"Distribution of gene density values on LG_2\",\n       x = \"Gene density\", y = \"Count\")\n\n\n\n\nThe range for the x axis is much smaller! The maximum gene density here (~12%) is much smaller than the highest value in the full genome data set (~40/50%).\n(Why might this be?)\nOne of the aspects of your data that you can’t visualize well with a histogram is whether there are any values that exceed the limits you expected for your data."
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#scatterplots-box-plots",
    "href": "lec06-exploratory-data-analysis.html#scatterplots-box-plots",
    "title": "7  Exploratory data analysis",
    "section": "7.8 Scatterplots & Box plots",
    "text": "7.8 Scatterplots & Box plots\n\n7.8.1 More info! Less bias!\nWith quantitative data, we can get more information by looking at scatterplots and box plots. Not only are they immune to bin size bias, they can help us find outliers and let us make initial visual comparisons of averages across categories.\n\n\n7.8.2 Visualize raw data as a scatterplot\nWe know that “chromosome” is a categorical, independent variable appropriate for our X axis and that “density” is a continuous, dependent variable that will be appropriate for the Y.\n\nggplot(geneDensity, aes(x = chromosome, y = density)) +\n  geom_point() +\n  labs(title = \"Comparison of gene density across chromosomes\",\n       x = \"Chromosome\", y = \"Gene density\")\n\n\n\n\nAlready, we can see that there different maximum gene density values on each chromosome. Because the points are overlapping, it’s hard to evaluate what the average or skewness might be for any of the categories.\n\n\n7.8.3 Boxplots for better comparisons\nBecause boxplots display the median and quartile limits, it’s much easier to evaluate the properties of the distribution.\n\nggplot(geneDensity, aes(x = chromosome, y = density)) +\n  geom_boxplot() +\n  labs(title = \"Comparison of gene density across chromosomes\",\n       x = \"Chromosome\", y = \"Gene density\")\n\n\n\n\nThere’s definitely a value that jumps out immediately. It’s stretching the scale of the Y axis so that it’s hard to effectively compare the medians of each of the chromosomes.\nBefore we officially decide what to do with this outlier, we’ll visually set it aside for now by re-scaling our Y axis, which we’ve already learned how to do!\n\nggplot(geneDensity, aes(x = chromosome, y = density)) +\n  geom_boxplot() +\n  ylim(0, 0.125) + #other methods possible\n  labs(title = \"Comparison of gene density across chromosomes\",\n       x = \"Chromosome\", y = \"Gene density\")\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nLook at that handy warning! It lets us know that one value was thrown out: “removed 1 rows”. This view helps us to get a better general understanding of how this categorical “chromosome” value might relate to gene density. However! It’s important not to throw out data unless you have good reason to believe it doesn’t belong in the data set.\nBonus note: you can use the coord_cartesian function instead of ylim. It won’t warn you if any of your data points are beyond the limits of the axes, though.\n\n\n7.8.4 What about the other variables?\n\nThis data describes a few of the genetic “bits” (we generally call them “elements”) live in one plant genome. The chromosomes have been broken down into 1Mb pieces (“windows”) that overlap each other and the contents of each window have been averaged. We’ve got information on the density of conserved genes and one type of transposon for each window. Additionally, we have the evolutionary age for those transposons.\n\nAverage number of genes in bins along chromosomes.\nDefinitely more interesting to compare across the categories built into our data (here, chromosomes) to see how the gene density looks in each one separately. We can see whether the global pattern is present in each category. But how can we get all that info in one graph??\nFirst step is to ask ourselves what we currently have in our data. If our category for comparison is chromosome, what independent variables are shared among them that could facilitate comparison of the dependent gene density variable?\n\nhead(geneDensity)\n\n# A tibble: 6 × 8\n  chromosome  start     end winNum numElements numBases winSize density\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 LG_N            0 1000000      1           1     2499 1000000 0.00250\n2 LG_N        20000 1020000      2           1     2499 1000000 0.00250\n3 LG_N        40000 1040000      3           2    10158 1000000 0.0102 \n4 LG_N        60000 1060000      4           2    11583 1000000 0.0116 \n5 LG_N        80000 1080000      5           2    11583 1000000 0.0116 \n6 LG_N       100000 1100000      6           2    11583 1000000 0.0116 \n\n\nStart, end, and winNum would all be reasonable proxies for position along the chromosome.\n\ngeneDensity %&gt;%\n  filter(chromosome == \"LG_2\") %&gt;%\n  ggplot(aes(x = start, y = density)) +\n  geom_point() +\n  labs(title = \"Comparison of gene density along LG_2\",\n       x = \"Chromosomal position (bp)\", y = \"Gene density\")\n\n\n\n\nThis gives us an overview of how many of the conserved genes are found in which region of this LG_2 chromosome.\nTo be able to compare all the chromosomes at the same time, we can split our graph into “facets” so there’s one per chromosome, as you’ve learned how to in the last lecture.\n\nggplot(geneDensity, aes(x=start, y=density)) +\n  geom_point() +\n  labs(title=\"Comparison of gene density across chromosomes\",\n       x=\"Chromosomal position (bp)\", y=\"Gene density\") +\n  facet_wrap( vars(chromosome) )\n\n\n\n\nBecause not all of the chromosomes are the same length, the data appears more squished in some of the panels. We can adjust that by telling facet wrap to scale the X axis per-panel instead of globally.\nIf we want to be able to visually compare the densities across chromosomes, we should not allow the Y axis to scale freely. We can, however, set a limit for the Y axis values, as we’ve done before.\nUse different command for scaling the Y axis\n\nggplot(geneDensity, aes(x = start, y = density)) +\n  geom_point() +\n  coord_cartesian( ylim = c(0,0.13) ) +\n  labs(title = \"Comparison of gene density across chromosomes\",\n       x = \"Chromosomal position (bp)\", y = \"Gene density\") +\n  facet_wrap( vars(chromosome), scales = \"free_x\" )\n\n\n\n\nCool, eh?? The chromosomes have very different patterns! The range and distribution of values differs considerably!\nWhat are some reasons for gene density to change along a chromosome?\n\nCentromeres are mostly made up of repeats - very low gene content\n\nCentromeres can be in the middle or near one end of a chromosome\nWhere do you think the centromeres are in these chromosomes?\n\nCertain regions of a chromosome are more tightly wound up with histones\n\nMakes them less accessible to molecular machinery\nIf polymerases don’t reach a region, that region can’t be expressed\nIf a region is unexpressed, you don’t want genes there!\nCentromeres are generally one of these ‘inactive’ regions\n\nMore accessible, active regions of a chromosome have higher gene content\n\nThese regions are generally along chromosome arms\n\n\n\n\n7.8.5 Boxplot augmentations\nThere are a few additional things we can do that might make boxplots even more informative for our data.\n\nViolin plots - boxplots but with curves instead of boxes\nAdding a scatterplot behind the boxplot\nAdding “jitter” to scatterplots so the points are offset\n\nAdditionally, you can make the points more transparent (change the alpha value)\n\nYou can also add a trend line to help you visualize potential relationships\n\n\nggplot(geneDensity, aes(x = chromosome, y = density)) +\n  geom_point(alpha = 0.1, position = \"jitter\") +\n  geom_violin() + \n  labs(title = \"Comparison of gene density across chromosomes\",\n       x = \"Chromosome\", y = \"Gene density\")\n\n\n\n\nMaking the points more transparent gives us a better idea of what density values are most common. You can see this at the bottom of the graph, where the points don’t look transparent at all - so many data points!!"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#challenge",
    "href": "lec06-exploratory-data-analysis.html#challenge",
    "title": "7  Exploratory data analysis",
    "section": "7.9 Challenge!",
    "text": "7.9 Challenge!\nHow could you visualize the LTR data across chromosomes? Don’t forget to use axis labels.\nWhat is the range of LTR density for the LG_2 chromosome?\n\nggplot(ltrDensity, aes(x = chromosome, y = density)) +\n  geom_violin() + #boxplot also valid\n  geom_point(alpha = 0.01, position = \"jitter\") +\n  labs(title = \"Comparison of LTR density across Chromosomes\",\n       x = \"Chromosome\", y = \"LTR density\")\n\n\n\nltrDensity %&gt;%\n  group_by(chromosome) %&gt;%\n  summarize(mean = mean(density), median = median(density), \n            n = n(), max = max(density))\n\n# A tibble: 5 × 5\n  chromosome  mean median     n   max\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 LG_2       0.324  0.331 11234 0.488\n2 LG_4       0.303  0.320  6520 0.973\n3 LG_7       0.331  0.339 15938 0.503\n4 LG_N       0.318  0.332  8551 0.476\n5 LG_X       0.307  0.315  6709 0.477\n\n\nNow it’s time to start thinking about what to do with rebellious outliers!"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#outliers",
    "href": "lec06-exploratory-data-analysis.html#outliers",
    "title": "7  Exploratory data analysis",
    "section": "7.10 Outliers",
    "text": "7.10 Outliers\n\n7.10.1 But why are you like this?\nThere could be many reasons why your data has values that exceed the limits you expected it would have. It basically comes down to error, whether in your data or in the expectation you had for its limits. Consider error carefully.\n\nIncorrect prediction of what the limits should be\n\nMaybe your study system has different qualities than literature spp.\n\nSystematic error is predictable and affects a measurement’s accuracy\n\nIncorrectly calibrated lab equipment (pH meter, thermometer, etc.)\nGenomics - your gene annotation can be biased by repetitive elements\nCan be very difficult to compensate for this kind of error\n\nRandom error affects measurement precision (think significant figures)\n\nWriting down measurements incorrectly in your notes\nA lab scale can only weigh samples to a given decimal point\nSimple human fallibility when it comes to assessing measurements\nTake multiple measurements and average the results to compensate\n\nCommon sources\n\nThis (exploratory data analysis) is a great time to look for issues!\nError in previous analysis steps (code that produced LTR age estimates)\nErroneous assumptions about your sample sites or methods\n\nDon’t just throw out a point because you can ascribe it to error\n\nIMPORTANT NOTE: if you do end up removing any data, you MUST disclose the removal and your justification for the removal. Your reasons could be great, but you need to make sure your audience has access to those reasons.\nThis consideration of weird values brings up an interesting point: you’re doing these checks because your values are different than what you expected. It’s important to think about analytical ‘controls’ to look for potential errors even when your data looks the way you expect it to! Steps towards this can be as simple as sharing your code publicly.\n\n\n7.10.2 Let’s take a look!\nWe had that weird really high gene density value on the LG_X chromosome. Let’s look at what’s happening there.\n\nfilter(geneDensity, density &gt; 0.13)\n\n# A tibble: 1 × 8\n  chromosome     start       end winNum numElements numBases winSize density\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 LG_X       134160000 134165151   6709           1     2655    5151   0.515\n\n\nWhat do the other variables tell us about this data point?\nThis data point has a really high winNum, so it’s likely located near the end of the chromosome. But importantly, our windows are supposed to be 1Mb in size (1 000 000 value in the winSize column). The winSize value for this outlier is tiny in comparison!!\n\n\n7.10.3 Wholesome thoughts about your data\nAverages – how was your data collected & what biases might be inherent? The data I’m showing you is a pretty clear example of how important (and difficult) it is to understand what the variables mean in your data sets. What might cause outliers in the kind of data you’re interested in?\nThat last look showed us that it’s definitely very important to consider our data as a whole: to think not only about the variables relevant to our hypotheses, but the way in which it was collected (how that was reflected in the other properties of the data).\nSo. Let’s try to understand more about the interaction between gene density and window size in the rest of our data. Visualization time!\n\nggplot(geneDensity, aes(x = start,y = winSize, colour = chromosome)) +\n  geom_point() +\n  labs(title = \"Window sizes along the chromosome\",\n       x = \"Chromosomal position (bp)\", y = \"Window size (bp)\")\n\n\n\n\nIt looks like all of the chromosomes get this trail-off in window size near their ends. This is not what we expected!! All of the squish at the end is basically just error from a previous analysis.\n\n\n7.10.4 Challenge!\nCreate a category based on window size: density as either belonging in a “small window” or a “normal window”. We can create a new “winCategory” variable using mutate() and assign the value of “small” to windows with winSize less than 1Mb and “normal” to all the other windows (which we expect will have a value of 1Mb).\n\ngeneDensity2 &lt;- geneDensity %&gt;%\n  mutate( winCategory = case_when(winSize&lt;1000000 ~ \"small\",\n                                  TRUE ~ \"normal\") ) %&gt;%\n  group_by(winCategory, chromosome)\n\nsummarize(geneDensity2,\n          mean = mean(density), median = median(density), n = n(), \n          max = max(density), sd = sd(density), .groups = \"keep\")\n\n# A tibble: 10 × 7\n# Groups:   winCategory, chromosome [10]\n   winCategory chromosome    mean  median     n    max     sd\n   &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 normal      LG_2       0.0104  0.00481 11184 0.119  0.0168\n 2 normal      LG_4       0.0123  0.00730  6470 0.0900 0.0146\n 3 normal      LG_7       0.00909 0.00284 15888 0.123  0.0151\n 4 normal      LG_N       0.0111  0.00464  8501 0.0690 0.0143\n 5 normal      LG_X       0.0119  0.00449  6659 0.0960 0.0175\n 6 small       LG_2       0.0443  0.0472     50 0.0964 0.0299\n 7 small       LG_4       0       0          50 0      0     \n 8 small       LG_7       0.0443  0.0370     50 0.122  0.0233\n 9 small       LG_N       0       0          50 0      0     \n10 small       LG_X       0.0358  0.0226     50 0.515  0.0705\n\n\nWhat can we take away from this table?\nThe n values are considerably larger for the normal-sized windows group. LG_4 and LG_N had 0 gene density in their small windows but have some of the highest median gene densities in the normal-sized windows.\nThe standard deviation of the small windows is much higher. Is that what we would expect for that data category? Perhaps most importantly for our purposes, the mean and median are quite different. These smaller windows have considerably different values.\nWhat does this look like in an actual plot? This is going to take a bit of black magic in the form of two separate calls to geom_boxplot(). The first will use all the windows (setting it to colour values by ‘all’) and the second will actually create (and colour) different box plots based on their winCategory value.\n\nggplot(geneDensity2, aes(x = chromosome, y = density, colour = winCategory)) +\n  geom_boxplot( aes(x = chromosome, y = density, colour = \"all\") ) + \n  geom_boxplot() +\n  ylim(0, 0.125) +\n  labs(title=\"Visualizing gene density across window size and chromosome\",\n       x=\"Chromosome\", y=\"Gene density\", colour=\"Window\\nCategory\")\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\nRemoved 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nThe small window values seem quite different than the global gene density results!\nWhy do you think this might be? Looking back on the summaries, we can see that there aren’t many data points in the ‘small’ window category.\nIn conclusion!!\nThese small windows do seem to contain interesting information, but they are over-weighted given the amount of information they’re based on. Based on the analysis conducted to create the windows, it might be appropriate to discard the small windows on the ends of the chromosomes. Each windowed data point is weighted equally, even though these smaller windows contain less information, which creates a bias.\nWhat do you think is the most appropriate way to deal with this data?\nIs there a way to weight the gene density by window size?"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#how-do-we-explore-these-questions",
    "href": "lec06-exploratory-data-analysis.html#how-do-we-explore-these-questions",
    "title": "7  Exploratory data analysis",
    "section": "8.1 How do we explore these questions?",
    "text": "8.1 How do we explore these questions?\nWe need to relate gene density, LTR density, and LTR age. Do we have this data?\nIs it currently in a form where we can make comparisons?\nBased on the properties of our gene and LTR density data sets, what are the shared “units”? Essentially, what are we trying to compare within and among each chromosome?\n\nhead(geneDensity)\n\n# A tibble: 6 × 8\n  chromosome  start     end winNum numElements numBases winSize density\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 LG_N            0 1000000      1           1     2499 1000000 0.00250\n2 LG_N        20000 1020000      2           1     2499 1000000 0.00250\n3 LG_N        40000 1040000      3           2    10158 1000000 0.0102 \n4 LG_N        60000 1060000      4           2    11583 1000000 0.0116 \n5 LG_N        80000 1080000      5           2    11583 1000000 0.0116 \n6 LG_N       100000 1100000      6           2    11583 1000000 0.0116 \n\nas_tibble(ltrDensity)\n\n# A tibble: 48,952 × 8\n   chromosome  start     end winNum numElements numBases winSize density\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 LG_N            0 1000000      1         396   258184 1000000   0.258\n 2 LG_N        20000 1020000      2         400   274748 1000000   0.275\n 3 LG_N        40000 1040000      3         390   271692 1000000   0.272\n 4 LG_N        60000 1060000      4         390   271540 1000000   0.272\n 5 LG_N        80000 1080000      5         386   268414 1000000   0.268\n 6 LG_N       100000 1100000      6         380   268519 1000000   0.269\n 7 LG_N       120000 1120000      7         396   285314 1000000   0.285\n 8 LG_N       140000 1140000      8         396   284053 1000000   0.284\n 9 LG_N       160000 1160000      9         396   270360 1000000   0.270\n10 LG_N       180000 1180000     10         388   267071 1000000   0.267\n# ℹ 48,942 more rows\n\n\nThe basic “unit” in this data is the 1Mb window. Because this is shared across the two data sets, we can use it to join them together. Excellent!\nWhat about the LTR age data set?\n\nas_tibble(ltrAge)\n\n# A tibble: 42,339 × 4\n   chrom    start      end     K2P\n   &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 LG_2  61204840 61212964 0.0202 \n 2 LG_2  61219360 61221366 0.0538 \n 3 LG_2  61236795 61241286 0.00514\n 4 LG_2  61241905 61249226 0.00100\n 5 LG_2  61268475 61269268 0.0808 \n 6 LG_2  61337049 61348048 0.00472\n 7 LG_2  61365365 61372328 0.0260 \n 8 LG_2  61382447 61389344 0.0617 \n 9 LG_2  61396331 61407449 0.00800\n10 LG_2  61413022 61414052 0.0984 \n# ℹ 42,329 more rows\n\n\nThis data was prepared differently, so it doesn’t have the same ‘window’ units. It does contain chromosomal position information, however, which we can use to make some preliminary comparisons.\n\n8.1.1 Actual wrangling\nWe’re also going to pull out only the variables we now know we’ll need (what’s shared among the data sets and what will be used to try and test our predictions), just because of how large this data frame will be. It’s not a good idea to do this before looking at all the variables together.\n\nsimpleGeneDensity &lt;- geneDensity %&gt;%\n  mutate(elementType = \"gene\") %&gt;%\n  select(chromosome, start, elementType, density)\n\nsimpleLTRdensity &lt;- ltrDensity %&gt;%\n  mutate(elementType = \"LTR\") %&gt;%\n  select(chromosome, start, elementType, density)\n\nhead(simpleLTRdensity)\n\n# A tibble: 6 × 4\n  chromosome  start elementType density\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 LG_N            0 LTR           0.258\n2 LG_N        20000 LTR           0.275\n3 LG_N        40000 LTR           0.272\n4 LG_N        60000 LTR           0.272\n5 LG_N        80000 LTR           0.268\n6 LG_N       100000 LTR           0.269\n\n\nAt this point, are these data “long” or “wide”? #throwback\n\n\n8.1.2 ? Knowledge Check Challenge\nJoin the two data sets (simpleLTRdensity and simpleGeneDensity) into one data frame called “densities”. As a bonus, try mutating the start variable so that it’s measured in 10kb increments instead of 1bp. This will just make our X axis labels are easier to interpret.\n\ndensities &lt;- full_join(simpleLTRdensity, simpleGeneDensity,\n                       by = c(\"chromosome\", \"start\", \"elementType\", \"density\")) %&gt;%\n  mutate(start = start / 10000) %&gt;%\n  group_by(chromosome, elementType)\n  \nhead(densities)\n\n# A tibble: 6 × 4\n# Groups:   chromosome, elementType [1]\n  chromosome start elementType density\n  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 LG_N           0 LTR           0.258\n2 LG_N           2 LTR           0.275\n3 LG_N           4 LTR           0.272\n4 LG_N           6 LTR           0.272\n5 LG_N           8 LTR           0.268\n6 LG_N          10 LTR           0.269\n\nrm(simpleGeneDensity, simpleLTRdensity)\n\nWe’ve got two independent categorical variables, an independent numerical variable, and a dependent numerical variable. It’s beautiful."
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#is-gene-density-high-when-ltr-density-is-low-hyp-1",
    "href": "lec06-exploratory-data-analysis.html#is-gene-density-high-when-ltr-density-is-low-hyp-1",
    "title": "7  Exploratory data analysis",
    "section": "8.2 Is gene density high when LTR density is low? (hyp #1)",
    "text": "8.2 Is gene density high when LTR density is low? (hyp #1)\nWhat variables do we want to plot?\n\nChromosome\nStart position (bp)\nElement type\nElement density\n\n\n\n8.2.1 Challenge\nOf the plot types we’ve used so far, what would you use to try and compare gene densities along the chromosomal positions on each chromosome?\n\nggplot(densities, aes(x = density, fill = elementType)) +\n  geom_histogram( binwidth = 0.03 ) +\n  facet_wrap( vars(chromosome), scales = \"free_y\" ) +\n  coord_cartesian(xlim = c(0,0.6)) +\n  labs(x = \"Element Density\", y = \"Count\", fill = \"Element\\nType\",\n       title = \"Element densities among chromosomes\")\n\n\n\n\nPoking at the histogram shows us some interesting things about differences in the frequencies of LTRs and genes. Gene values have extremely high kurtosis near 0. LG_4 may have the highest median/mode LTR density.\nThe Y axis can be free-scaled here because all of the counts are based on the size of their chromosome. We don’t want one chromosome to seem like it has a much higher LTR count just because it has more windows (greater n) than the other chromosomes.\nThis was an interesting plot, but it compares densities across chromosomes more than it looks at differences in LTR/gene patterns within chromosomes.\n\nggplot(densities, aes(x = start,y = density,colour = elementType)) +\n  geom_point(alpha = 0.3) +\n  facet_wrap( vars(chromosome), scales = \"free_x\" ) +\n  ylim(0, 0.5) +\n  labs(title = \"Element densities along chromosomes\",\n       x = \"Chromosomal position (10kb)\", y = \"Element density\",\n       colour = \"Element\\nType\")\n\nWarning: Removed 5 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThis looks like the kind of information we want! If we squint, we can almost see that increases in gene density seem to correlate with decreases in LTR density.\nIf you can remember how to add a smooth line to show broad patterns, this will be the easiest view.\n\nggplot(densities, aes(x = start,y = density,colour = elementType)) +\n  geom_smooth() +\n  ylim(0, 0.4) +\n  facet_wrap( vars(chromosome), scales = \"free_x\" ) +\n  labs(title = \"Element densities along chromosomes\",\n       x = \"Chromosomal position (10kb)\", y = \"Element density\",\n       colour = \"Element\\nType\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 3700 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 6 rows containing missing values (`geom_smooth()`).\n\n\n\n\n\nBroadly, we can see that when the LTR density plummets, gene density smudges upward."
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#is-gene-density-high-when-ltr-age-is-high-hyp-2",
    "href": "lec06-exploratory-data-analysis.html#is-gene-density-high-when-ltr-age-is-high-hyp-2",
    "title": "7  Exploratory data analysis",
    "section": "8.3 Is gene density high when LTR age is high? (hyp #2)",
    "text": "8.3 Is gene density high when LTR age is high? (hyp #2)\nLet’s take a look at the LTR age data.\n\nhead(ltrAge)\n\n# A tibble: 6 × 4\n  chrom    start      end     K2P\n  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 LG_2  61204840 61212964 0.0202 \n2 LG_2  61219360 61221366 0.0538 \n3 LG_2  61236795 61241286 0.00514\n4 LG_2  61241905 61249226 0.00100\n5 LG_2  61268475 61269268 0.0808 \n6 LG_2  61337049 61348048 0.00472\n\n\nSome of the variable names are different (K2P is our measure of age) but are really familiar to the other data we’ve been analyzing (chrom instead of chromosome). Let’s see if the LTR age data looks anything like our gene and LTR density data.\n\nggplot(ltrAge, aes(x = start,y = K2P)) +\n  geom_point(alpha = 0.1) +\n  facet_wrap( vars(chrom), scales = \"free_x\" ) +\n  labs(title = \"LTR age along chromosomes\",\n       x = \"Chromosomal position\", y = \"LTR age (K2P)\")\n\n\n\n\nThese clouds of points are really hard to understand. We can try using geom_smooth to see if it can reveal what’s going on in these clouds. The one argument we’ll make note of right now is n, which tells geom_smooth how many points along the x it should be using to make its average. Because of how big our data is, we’ll give it a smaller value so it doesn’t take forever to plot.\n\nggplot(ltrAge, aes(x = start,y = K2P)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(n = 50) + # try different values (think histogram bin widths)\n  facet_wrap( vars(chrom), scales = \"free_x\" ) +\n  labs(title = \"LTR age along chromosomes\",\n       x = \"Chromosomal position\", y = \"LTR age (K2P)\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nWell, what can we take away from this visualization? Not much. It seems pretty clear that the average LTR is old (darkness at the bottom of the clouds). The few younger LTRs near the top of the plots might have useful information for us, given that LTR “reproduction” (transposition) is generally rare."
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#discretizing-our-ages",
    "href": "lec06-exploratory-data-analysis.html#discretizing-our-ages",
    "title": "7  Exploratory data analysis",
    "section": "8.4 Discretizing our ages",
    "text": "8.4 Discretizing our ages\nWe can try binning LTRs based on their age, to see if the youngest LTRs are able to be “born” in gene-dense regions. Note: don’t categorize numerical data like this without considering what information you’re losing!\n\nggplot(ltrAge, aes( x = start / 10000,\n                    y = K2P,\n                    colour = cut(K2P, 4,\n                               labels = c(\"youngest\", \"young\",\n                                          \"old\", \"oldest\")) \n                    )) +\n  geom_point(alpha = 0.5) +\n  #geom_smooth() + #just aren't enough points in the younger categories\n  facet_wrap( vars(chrom), scales = \"free_x\" ) +\n  labs(title = \"LTR age along chromosomes\",\n       x = \"Chromosomal position (10Mb)\", y = \"LTR age (K2P)\",\n       colour = \"Age Class\")\n\n\n\n\nThere are few young LTRs, but their positions match with the pattern of our overall TE density plot. The positions of the young LTRs aren’t close to gene-dense regions on the autosomes, though interestingly there are a few on the right-hand side of LG_X, which was the most gene-dense region of our sex chormosome!\nThat was cool. Definitely not a statistical test of correlations between LTR age and gene density, but the fact that sub-sections of this data behave quite differently is really interesting to see!"
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#missing-values-in-gene-density-comparison",
    "href": "lec06-exploratory-data-analysis.html#missing-values-in-gene-density-comparison",
    "title": "7  Exploratory data analysis",
    "section": "8.5 Missing values in gene density comparison",
    "text": "8.5 Missing values in gene density comparison\nYou might remember that in last lecture, when you encountered missing values in your data, you could replace them with 0. If we want to put our LTR age data into the same kind of windows that the gene density data is in, we’re going to have some windows without any age data. Do you think that setting LTR age to 0 would be a good way to handle the windows with missing age data?\n\nwindowedAges &lt;- ltrAge %&gt;%\n  mutate(chromosome = chrom,            # get the \"chromosome\" col name to match\n         age = K2P,                     # might as well give a better name\n         winNum = floor(start/20000)) %&gt;% # bin the start (20Mb) & round \n  select(chromosome, winNum, age)\n  \ngenesPlusAges &lt;- geneDensity %&gt;%\n  select(\"chromosome\", \"winNum\", \"density\") %&gt;%\n  full_join(windowedAges, by = c(\"chromosome\", \"winNum\") )\n\nas_tibble(genesPlusAges)\n\n# A tibble: 57,617 × 4\n   chromosome winNum density      age\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 LG_N            1 0.00250  0.0122 \n 2 LG_N            2 0.00250  0.0508 \n 3 LG_N            3 0.0102   0.0181 \n 4 LG_N            4 0.0116   0.111  \n 5 LG_N            5 0.0116  NA      \n 6 LG_N            6 0.0116   0.0343 \n 7 LG_N            6 0.0116   0.00505\n 8 LG_N            7 0.0116   0.0277 \n 9 LG_N            8 0.0116   0.0318 \n10 LG_N            9 0.0116   0.0629 \n# ℹ 57,607 more rows\n\n\nJust for the record, this is not a great way to window data.\nBut! Look what happens when you change the join method! Got more missing data… Maybe it’ll be easier to understand if we plot it.\n\nfindMissing &lt;- genesPlusAges %&gt;%\n  mutate(noDense = is.na(density),\n         noAges = is.na(age))\n\nggplot(findMissing) +\n  geom_point( aes(x = winNum, y = density, colour = noAges) ) +\n  coord_cartesian( ylim = c(0,0.15) ) +\n  facet_wrap( vars(chromosome), scales = \"free_x\" ) +\n  labs(title = \"Missing gene density data\",\n       x = \"Window number\", y = \"Gene density\", colour = \"Missing ages!\")\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\nggplot(findMissing) +\n  geom_point( aes(x = winNum, y = age, colour = noDense) ) +\n  facet_wrap( vars(chromosome), scales = \"free_x\" ) +\n  labs(title = \"Missing LTR age data\",\n       x = \"Window number\", y = \"LTR age\", colour = \"Missing densitiy!\")\n\nWarning: Removed 15278 rows containing missing values (`geom_point()`).\n\n\n\n\n\nOoph. We’re only missing gene density data for 3 windows, but we’re missing out on LTR age data for around 15k windows… Is there anything we can do about it?\nAssigning a value of 0 to our LTR ages is asserting we have different data than we actually have: that we know there’s an LTR in that position and that it’s an incredibly recent insertion. But what if we replace the missing age points with the mean LTR age?\n\nfindMissing &lt;- genesPlusAges %&gt;%\n  mutate(noDense = is.na(density),\n         noAges = is.na(age),\n         rplAge = replace_na( age,mean(age,na.rm=TRUE) ),\n         rplDensity = replace_na( density,mean(density,na.rm=TRUE) ))\n\nsum( is.na(findMissing$rplDensity) )\n\n[1] 0\n\nsum( is.na(findMissing$rplAge) )\n\n[1] 0\n\nggplot(findMissing) +\n  geom_point(alpha = 0.5,\n             aes(x = winNum, y = rplAge,\n                 colour = cut(rplAge, 3, labels = c(\"youngest\", \"middle\", \"oldest\")))) +\n  geom_col(aes(x = winNum, y = rplDensity)) +\n  ylim(c(0,0.2)) +\n  facet_wrap(vars(chromosome), scales = \"free_x\" ) +\n  labs(title = \"Gene density and LTR age along chromosomes\",\n       x = \"Chromosomal position (window number)\", y = \"Altered ages\",\n       colour = \"Age Class\")\n\nWarning: Removed 1 rows containing missing values (`position_stack()`).\n\n\nWarning: Removed 21 rows containing missing values (`geom_col()`).\n\n\n\n\n\nNo more warnings about missing data! Woo! We can feel whole again! On the other hand, replacing these values didn’t actually help us improve our understanding of our age-density prediction…\nWe should actually plot a window’s LTR age against its gene density! First, let’s look at the general pattern with the adjusted ages (when we replaced NA with the mean value for the variable).\n\nggplot(findMissing) +\n  geom_point(alpha = 0.5,\n             aes(x = rplDensity, y = rplAge,\n                 colour = cut(rplAge, 3, labels = c(\"youngest\", \"middle\", \"oldest\")))) +\n  coord_cartesian( xlim = c(0, 0.13) ) +    # being mindful of our outlier\n  facet_wrap( vars(chromosome), scales=\"free_x\" ) +\n  labs(title=\"Gene density vs LTR age post-adjustment\",\n       x=\"Gene density (adjusted)\", y=\"LTR age (adjusted)\",\n       colour=\"Age Class\")\n\n\n\n\nThat’s pretty dang cool! We do see a general downward trend - as gene density increases (going right along the X axis), LTR age tends to decrease! We can even see some interesting differences among the chromosomes.\nLG_7 is our biggest chromosome and it seems to have a lot of the young (blue) LTRs. Why do you think that might be? Generally, as chromosome length increases, recombination rate decreases. Recombination rate is one of the properties we know correlates with the accessibility of genomic regions. So, we might predict that longer chromosomes have a larger proportion of ‘inactive’ regions and a greater number of TEs.\nMoving on!\nLet’s judge our past selves! Let’s see the effects of our decision to replace the missing age data with an average. Was it foolish or as wise as the time we joined a dragon boat team?\n\nggplot(findMissing) +\n  # This time, we'll colour the points based on whether the age should be NA\n  geom_point(alpha=0.5, aes(x=rplDensity,y=rplAge, colour=noAges)) +\n  coord_cartesian( xlim=c(0,0.13) ) +    # being mindful of our outlier\n  facet_wrap( vars(chromosome), scales=\"free_x\" ) +\n  labs(title=\"Gene density vs LTR age post-adjustment\",\n       x=\"Gene density (adjusted)\", y=\"LTR age (adjusted)\",\n       colour=\"Agelessss\")\n\n\n\n\nNot too bad! Not mechanistically motivated, but it does show that replacing missing data with the value we would expect (basically the definition of the mean) can have fewer consequences than replacing it with 0 in some contexts."
  },
  {
    "objectID": "lec06-exploratory-data-analysis.html#does-the-sex-chromosome-lg_x-have-higher-ltr-density-hyp-3",
    "href": "lec06-exploratory-data-analysis.html#does-the-sex-chromosome-lg_x-have-higher-ltr-density-hyp-3",
    "title": "7  Exploratory data analysis",
    "section": "8.6 Does the sex chromosome (LG_X) have higher LTR density? (hyp #3)",
    "text": "8.6 Does the sex chromosome (LG_X) have higher LTR density? (hyp #3)\nWe want to compare LTR densities across chromosomes. How would you do this?\n\n8.6.1 Challenge!\nBox plot is a good base line. Ask for types of improvements that could be made to a basic box plot.\n\nggplot(ltrDensity, aes(x = chromosome, y = density)) +\n  geom_boxplot() +\n  labs(title = \"LTR densities among chromosomes\",\n       x = \"Chromosome\", y = \"LTR density\")\n\n\n\n\nWhat unusual things do you notice about this plot?\nDoes this initial exploration lead us to think that the sex chromosome, with its reduced rate of recombination, has accumulated more LTRs?\nBased on what we know about the gene density data, what would you suggest we might need to do with this LTR density data?\nThere is one extreme outlier again - let’s see what happens if we colour the data based on window size.\n\nltrDensity %&gt;%\n  ggplot( aes(x = chromosome, y = density, colour = winSize==1000000) ) +\n  geom_boxplot() +\n  ylim(c(0,0.6)) +\n  labs(title = \"LTR densities among chromosomes\",\n       x = \"Chromosome\", y = \"LTR density\", colour = \"Normal\\nWindow\\nSize\")\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nThere are fewer outliers, but the pattern remains the same. GOOD SIGN! :)"
  },
  {
    "objectID": "lec07-intro-to-stats.html#lesson-preamble",
    "href": "lec07-intro-to-stats.html#lesson-preamble",
    "title": "8  Introduction to inference",
    "section": "8.1 Lesson preamble",
    "text": "8.1 Lesson preamble\n\n8.1.1 Lesson objectives\n\nDevelop understanding of random variables, probability distributions, and likelihood.\nUnderstand how to simulate realizations of of a random variable.\nDevelop familiarity with maximum likelihood estimation and inference.\nBecome familiar with hypothesis testing: null and alternate hypotheses, test statistics, power, significance, p-values, and the duality of hypothesis tests and confidence intervals.\n\n8.1.2 Lesson outline\n\nRandom variables, probability distributions, and likelihood\n\nInterpretations of probability, sources of uncertainty\nDiscrete vs continuous RVs\nSimulating random variables in R\nMeans, variances, and other moments\n\nlikelihood estimation and inference\n\nUnderstanding the likelihood function\nMathematical and numerical optimization\n\nHypothesis testing: theory and examples\n\n\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.2\n\n\nWarning: package 'tibble' was built under R version 4.1.2\n\n\nWarning: package 'tidyr' was built under R version 4.1.2\n\n\nWarning: package 'readr' was built under R version 4.1.2\n\n\nWarning: package 'stringr' was built under R version 4.1.2\n\n\nWarning: package 'forcats' was built under R version 4.1.2\n\n\nWarning: package 'lubridate' was built under R version 4.1.2\n\ntheme_set(theme_bw())\n### see other themes here: https://ggplot2.tidyverse.org/reference/ggtheme.html"
  },
  {
    "objectID": "lec07-intro-to-stats.html#whats-chance-got-to-do-got-to-do-with-it",
    "href": "lec07-intro-to-stats.html#whats-chance-got-to-do-got-to-do-with-it",
    "title": "8  Introduction to inference",
    "section": "8.2 What’s chance got to do … got to do … with it?",
    "text": "8.2 What’s chance got to do … got to do … with it?\nStatistics is the study of decision making under uncertainty. Randomness that is inherit in many physical processes (especially those in ecology and evolution) makes it difficult to, given data, choose between alternative explanations for how the world works. But probability theory provides us with tools to combat this uncertainty and to come to principled, informed conclusions.\nIn this lecture, we will introduce key concepts in probability and statistics. These concepts (e.g., likelihood) form the backbone for future lectures and will be invaluable as you complete your projects."
  },
  {
    "objectID": "lec07-intro-to-stats.html#random-variables-and-probability-distributions",
    "href": "lec07-intro-to-stats.html#random-variables-and-probability-distributions",
    "title": "8  Introduction to inference",
    "section": "8.3 Random variables and probability distributions",
    "text": "8.3 Random variables and probability distributions\nConsider an experiment which gives rise to a set \\(\\Omega\\) of outcomes. This set of outcomes is called the sample space. The space of all events is formed by considering all possible combinations of outcomes (or some combinations but not all for technical reasons). Suppose the sample space is \\(\\Omega = \\{1,2,3,4\\}\\), so that the experiment might be rolling a four-sided die. One event is observing 2 or 3 upon rolling the die: this event is denoted \\(\\{2,3\\}\\). (This is only one of many such events!). Probability allows us to assign to each event (\\(\\{2,3\\}, \\{\\}, \\{1,2,3,4\\}\\), etc.) a value between zero and one capturing how likely that event is to occur under the experiment we have preformed. We also need certain conditions on this function to be met:\n\nThe probability of any event must be \\(\\geqslant 0\\). Negative probabilities make no sense!\nThe probability of the event \\(\\Omega\\) must be \\(=1\\). Something must have happened!\nCountably many mutually exclusive events \\(A_1,A_2,\\dots,\\) must satisfy the following:\n\n\\[\\Pr (A_1 \\text{ or } A_2 \\text{ or } A_3 \\text{ or } \\cdots) = \\sum_{i=1}^\\infty \\Pr (A_i)\\]\n\\(A_1,\\dots,A_n,\\dots\\) are mutually exclusive if \\(A_i, A_j\\) do not share outcomes for all \\(i \\neq j\\). (The events \\(A_1 = \\{1,2\\}\\) and \\(A_2 = \\{3,4\\}\\) are distinct, while the events \\(A_1 = \\{1,3\\}\\) and \\(A_2 = \\{3,4\\}\\) are not..)\nIn the case of rolling a four-sided die, the probability of all events can be calculated (due to the extra conditions) by specifying what the probability of each outcome. Suppose \\(\\Pr(\\{1\\}) = 0.2, \\Pr(\\{2\\}) = 0.3, \\Pr(\\{3\\}) = \\Pr(\\{4\\}) = 0.25\\), i.e., the die is not fair but not horribly biased to one side.\nA random variable is the (uncertain) outcome of a random experiment; more precisely, a RV is a function from the sample space to the real numbers. In the above example, the experiment is the roll of a four-sided die and the outcomes are 1,2,3, and 4. The random variable, which we call \\(X\\), has an associated probability distribution: \\(\\Pr(X = 1) = 0.2, \\Pr(X = 2) = 0.3, \\Pr(X = 3) = \\Pr(X = 4) = 0.25\\). Capital letters are often used to denote random variables and lower case letters to denote the values they can assume (i.e., elements of their range). We can use this probability distribution to simulate realizations (draws) of \\(X\\), which correspond to repeatedly and independently preforming the experiment:\n\nN &lt;- 100 # = number of experiments (realizations)\nrealizations_N_die_rolls &lt;- sample(c(1,2,3,4), \n                                   size = N, \n                                   prob = c(0.2,0.3,0.25,0.25), \n                                   replace = T)\nrealizations_N_die_rolls\n\n  [1] 4 1 2 4 4 4 3 2 4 1 2 1 2 3 1 4 3 1 4 2 3 3 2 1 4 4 1 2 3 2 4 2 4 4 4 2 4\n [38] 3 1 4 3 4 4 4 2 2 2 2 4 2 1 3 3 2 4 3 2 4 2 1 4 4 3 4 2 4 4 3 2 2 4 3 4 2\n [75] 4 1 3 2 2 2 3 2 1 2 4 3 4 4 4 4 4 3 2 3 4 3 2 3 2 3\n\nrealizations_N_die_rolls &lt;- data.frame(value = realizations_N_die_rolls)\nggplot(realizations_N_die_rolls, \n       aes(x = value, y = ..count../N)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis example gives rise to a probability distribution on the set \\(\\{1,2,3,4\\}\\) and, in fact, all of its subsets. Discrete probability distributions like this show up quite often, but so do continuous ones.\nFor example, a uniform distribution assigns equal weight to all real numbers between, say, \\(0\\) and \\(1\\). If \\(X \\sim \\text{Uniform}(0,1)\\), i.e., \\(X\\) is a random variable with a uniform distribution, then \\(\\Pr(a_1 &lt; X &lt; b_1) = (b_1 - a_1).\\). We can simulate realizations of a uniform random variable using\n\nN &lt;- 10000\nrealizations_N_uniform &lt;- runif(n = N, min = 0, max = 1)\n\nrealizations_N_uniform &lt;- data.frame(value = realizations_N_uniform)\nggplot(realizations_N_uniform, aes(x = value, y = ..count../N)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nIn general, the distribution of a discrete random variable \\(X\\) (which models an experiment with a countable number of outcomes \\(1,2,3.\\dots\\)) is completely determined by the probabilities \\(\\Pr(X = k)\\). These probabilities form the probability mass function for \\(X\\), i.e., a function which returns how much “mass” is assigned to each outcome. The probabilities of more complex events can be formed by taking sums.\nThe below code chunk simulates \\(n\\) realizations of a random variable that is 0 with probability \\(1-p\\) and 1 with probability \\(p\\). Such a random variable is said to have a Bernoulli distribution, or be Bernoulli.\n\nbernouli &lt;- function(p, n){\n  return(sample(c(0,1), size = n, prob = c(1-p, p), replace = T))\n}\n\nbernouli(0.1, 100) ### 100 realizations of a Bernouli(p = 0.1) RV\n\n  [1] 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\nThe distribution of a continuous random variable \\(Y\\) is defined by a function \\(f(y|\\theta)\\) called its probability density. The probability of an event \\(A\\) (for example, observing a number between 0.3 and 0.7) is\n\\[\\Pr(Y \\in A) = \\int_A f(y|\\theta) \\text{d} y.\\]\nFor example, if \\(Y \\sim \\text{Uniform}(a,b)\\),\n\\[\\Pr(a &lt; Y &lt; b) = \\int_a^b \\frac{1}{b-a} \\text{d} y = \\frac{1}{b-a}(b-a) = 1.\\]\nImportantly, it is convenient to use a common notation for the distribution of discrete and continuous random variables. Since sums are special kinds of integrals, we write \\(X \\sim f(x|\\theta)\\) even if \\(X\\) is discrete. A list of important probability distribution functions can be found here."
  },
  {
    "objectID": "lec07-intro-to-stats.html#means-variances-and-other-moments",
    "href": "lec07-intro-to-stats.html#means-variances-and-other-moments",
    "title": "8  Introduction to inference",
    "section": "8.4 Means, variances, and other moments",
    "text": "8.4 Means, variances, and other moments\nRandom variables model outcomes from uncertain experiments, and their properties can tell us about what to expect (or how confident we are that certain outcomes will be realized). One way to characterize the behavior of a RV is by considering its mean, variance, and higher moments.\nThe mean (or expected value) of \\(X \\sim f(x|\\theta)\\) is a measure of central tendency:\n\\[E(X) = \\int_\\Omega f(x|\\theta) x \\text{d} x.\\]\nThe mean of a random variable with a Uniform(0,\\(n\\)) distribution is\n\\[\\int_0^n \\frac{1}{n} x \\text{d} x = \\frac{1}{n} \\int_0^n x \\text{d} x = \\frac{1}{n} \\frac{x^2}{2} \\bigg\\lvert_0^n = \\frac{1}{n} \\frac{n^2}{2} = \\frac{n}{2}.\\]\nLet \\(E(X) = \\mu\\). The variance of \\(X \\sim f(x|\\theta)\\) is \\(V(X) = E[(X-\\mu)^2] = E(X^2) - \\mu^2.\\) Variance provides a measure of how far \\(X\\) is, on average, from its mean.\n\n8.4.1 Higher moments\nThe mean and the variance turn out to be two (very important) measures of central tendency and spread, respectively. Higher ``moments” of a random variable \\(X\\) characterize the skewness and other properties of \\(X\\); these moments are given by \\(E(X^n)\\) for \\(n = 1,2,\\dots\\)."
  },
  {
    "objectID": "lec07-intro-to-stats.html#simulating-important-random-variables",
    "href": "lec07-intro-to-stats.html#simulating-important-random-variables",
    "title": "8  Introduction to inference",
    "section": "8.5 Simulating important random variables",
    "text": "8.5 Simulating important random variables\nWe will return to this topic in a later lecture, but simulation of random variables turns out to be an important tool in scientific computing, which allows us to tackle problems that would be difficult to solve otherwise.\nIn R, simulating realizations from well-known random variables is very easy.\n\nrunif(n = 10, min = -1, max = 1) \n\n [1] -0.88248866  0.77108732 -0.40209328  0.11477758  0.79295663  0.04308083\n [7]  0.69427487  0.80266205 -0.04979734 -0.50694866\n\n## simulates n = 10 realizations of a Uniform(-1,1) RV\n\nrnorm(n = 100, mean = 0, sd = 1) \n\n  [1]  0.591054326 -0.525879627  0.978692513 -0.145729789  0.239024604\n  [6]  1.432246938 -0.266750418 -1.685394502 -0.771830054  0.326986455\n [11] -0.370978147 -0.441274244 -0.300116351 -0.290424652  0.076397398\n [16]  0.322436722  0.160385149 -0.952599422  2.672326039 -0.336913261\n [21] -0.168802504 -0.809612278  0.911578989 -1.195535740  1.375745764\n [26] -0.035282129 -0.376523636  1.171572498  0.987117080 -1.477884341\n [31]  0.696401335 -0.930899301  1.160831183 -0.277132150 -1.384601282\n [36] -1.282769837  0.475964741 -1.196723769 -1.067382412 -0.826815455\n [41]  0.581504831  1.803356787 -0.967488841 -0.613819676  1.758592394\n [46]  1.441045829 -0.443364568  0.002226038 -0.255550133 -0.677160425\n [51] -0.065793066 -0.270207518 -2.499945378  0.567973518 -0.454815228\n [56] -0.312012414  0.080638733 -0.675297966  0.300746336  1.525663379\n [61] -0.539993581 -0.009324013  1.161080534  0.757832828  0.026779701\n [66]  1.044932461  0.074625596 -2.383958908 -0.574617382  1.296932724\n [71]  0.448824031 -0.809778201  0.402179692 -1.402702866 -0.514567834\n [76] -0.346628038 -0.837916124 -0.730869647  0.526799117 -1.502824494\n [81] -0.835280932  0.038935752  2.332465931  0.487234555 -0.676925887\n [86] -0.159479619  0.687216105 -1.845916410  0.817515712  0.787890141\n [91]  1.474987083 -0.255309571 -1.375507153  1.127476640  0.009125361\n [96]  1.746304671  0.167926252 -1.415058015  0.385028142  1.220930947\n\n## simulates n = 100 realizations of a Normal(mean = 0, variance = 1) RV\n## sd stands for standard deviation, and is the sqrt of the variance\n\nThe syntax for simulating from well-known random variables (distributions) is r followed by the name of the distribution. For example, if \\(X \\sim \\text{Exponential}(\\lambda = 3)\\), then we can simulate realizations of \\(X\\) by calling rexp() and specifying rate = 3. The same syntax works for discrete random variables. If \\(X \\sim \\text{Poisson}(\\lambda = 0.1)\\), then rpois(lambda = 0.1) does the trick!\nImportantly, this also provides a means to simulate from RVs with probability distributions that we cannot write down easily. For example, if \\(X \\sim \\text{Normal}(0,1)\\), then we can simulate from \\(X^2 + 6\\) as follows:\n\nX &lt;- rnorm(n = 20, mean = 0, sd = 1) \nX^2 + 6\n\n [1] 12.821837  6.470534  6.088460  7.538307  6.639093  9.578919  7.587246\n [8]  6.551278 10.602757  6.727429  7.461080  6.880326  6.217859  8.016961\n[15]  6.367063  9.494384  7.819442  6.547326  6.135061  8.590569"
  },
  {
    "objectID": "lec07-intro-to-stats.html#all-about-likelihood",
    "href": "lec07-intro-to-stats.html#all-about-likelihood",
    "title": "8  Introduction to inference",
    "section": "8.6 All about likelihood!",
    "text": "8.6 All about likelihood!\nSo far we have seen:\n\nRandom variables model experiments with uncertain outcomes and come in many flavors. They are the main workhorse of probability and statistics.\nFunctions in base R allow us to conveniently and easily simulation realizations (i.e., draws) of many random variables. When the probability distribution of a random variable may be hard to write down, transformations of other random variables may provide a means to generate random numbers.\nThe mean, variance, and higher moments are useful summaries of the central tenancy of and variability in a random experiment or process.\n\nNow, we turn our attention to the central problem of statistics: determining what processes and parameters gave rise to data (estimation), and quantifying uncertainty in those estimates (inference). Estimation and inference based on the likelihood function is the basis/foundation for most statistical procedures used in the sciences (including Analysis of Variance, fitting mixed models).\n\n8.6.1 The likelihood function\nThe idea is as follows. Given data \\(X_1,X_2,\\dots,X_n \\sim f(x|\\theta)\\), we want to estimate \\(\\theta\\), i.e., to determine what parameters were mostly likely to have given rise to the data (under the assumption \\(f\\) models the data generating process). We do this by maximizing the likelihood function\n\\[L(X_1,\\dots,X_n|\\theta) = f(X_1|\\theta) \\cdots f(X_n|\\theta) = \\prod_{i=1}^n f(X_i|\\theta),\\]\nwhich is formed the assumption \\(X_1,\\dots,X_n\\) are independent. (There are methods that accommodate for dependent data, but we will not get into them here.) The likelihood function is a function of the parameters \\(\\theta\\), but not of the data. It contains all of the information in the data about the parameters. The likelihood is formed by simply plugging in the data into the probability distribution function from which they jointly arose. When the data are independent, the joint probability distribution function is the product of the individual distribution functions.\nViewed as a function of \\(\\theta\\), the likelihood tells us how likely each set of parameter values is to have given rise to the data. The set of parameter values which jointly maximize \\(L\\) (i.e., have the highest likelihood of generating the observed data) is called the maximum likelihood estimator \\(\\hat{\\theta}_{\\text{MLE}}\\).\n\n\n8.6.2 An example of maximum likelihood estimation\nSuppose \\(X_1, X_2, \\dots, X_n \\sim \\text{Exponential}(\\lambda)\\). The probability distribution for an Exponential random variable is \\(f(x|\\lambda) = \\lambda e^{-\\lambda x}\\), where \\(x \\geqslant 0\\). The likelihood function for \\(\\lambda\\) is\n\\[L(X_1,\\dots,X_n|\\lambda) = \\prod_{i=1}^n \\lambda e^{-\\lambda X_i} = \\lambda^n e^{-\\lambda (X_1+\\dots+X_n)}.\\]\nThis appears difficult to maximize, but it turns out that it is quite easy if we take the log of the likelihood. The point at which the likelihood is greatest is not changed under this transformation (since log is an increasing function). The log-likelihood is a bit easier to deal with, and has some nice statistical properties.\nWith Exponential(\\(\\lambda\\)) data, the log-likelihood function is\n\\[\\ln L(X_1,\\dots,X_n|\\lambda) =  \\ln \\lambda^n e^{-\\lambda (X_1+\\dots+X_n)} = n \\ln \\lambda - \\lambda (X_1+\\cdots + X_n).\\]\nNow, it is possible to maximize this function with respect to \\(\\lambda\\) without too much hassle. Taking the derivative with respect to \\(\\lambda\\), setting it equal to zero, and solving for \\(\\lambda\\), one has\n\\[\\hat{\\lambda}_{\\text{MLE}} = \\frac{n}{(X_1+\\dots+X_n)} = \\frac{1}{\\overline{X}}.\\]\nThis shows that the “best” (maximum likelihood) estimator of \\(\\lambda\\) is the inverse of the sample mean! Importantly, the estimator is a function of the data and is thus random—this means the estimator has a distribution that is set by the distribution of the data.\nTo see this, we can do a quick simulation:\n\nexponential_MLE &lt;- function(n = 1000, lambda = 5){\n  data &lt;- rexp(n, rate = lambda)\n  return(1/mean(data)) ### this is our maximum likelihood estimator\n}\n\nexponential_MLE()\n\n[1] 4.770059\n\nestimator &lt;- c()\nN &lt;- 1000 ### number of n=100 exponential draws\n\nfor (i in 1:N){\n  estimator[i] &lt;- exponential_MLE(n = 1000)\n}\n\nestimator &lt;- data.frame(value = estimator)\n\nggplot(estimator, aes(x = value)) +  geom_histogram(aes(y = ..density..), \n                                                    fill = \"gray\", alpha = 0.5) + \n  geom_vline(xintercept = mean(estimator$value), color = \"red\") +\n  geom_vline(xintercept = mean(estimator$value) - sd(estimator$value), \n             color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = mean(estimator$value) + sd(estimator$value), \n             color = \"red\", linetype = \"dashed\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nImportantly, the distribution of \\(\\hat{\\lambda}_{\\text{MLE}}\\) is approximately normal for large \\(n\\), and has mean equal to the true value of \\(\\lambda\\). When R returns a confidence interval, it often makes use of the asymptotic distribution of the maximum likelihood estimator (even when the assumption of a large sample size is not satified).\n\n\n8.6.3 An aside: numerical evaluation of the likelihood\nOften, it is not possible to use calculus to maximize the likelihood function. Sometimes it is not even possible to write down a closed form for the likelihood! In this case, we can use numerical methods to evaluate and maximize the likelihood. The following code chunk illustrates how to do this when the data arise from a mixture of two Normal distributions: \\(X \\sim \\text{Normal}(\\mu,\\sigma^2)\\) with probability \\(p\\) and \\(X \\sim \\text{Normal}(\\nu,\\tau^2)\\) with probability \\(1-p\\). The probability distribution function for this data is\n\\[f(x|\\mu,\\sigma^2,\\nu,\\tau^2) = p \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-(x-\\mu)^2/2\\sigma^2} + (1-p) \\frac{1}{\\sqrt{2\\pi \\tau^2}} e^{-(x-\\nu)^2/2\\tau^2}.\\]\nIn the below code chunk, \\(p = 0.1\\), \\(\\mu = 0, \\nu = 10, \\sigma = 1, \\tau = 10\\). We assume that we know \\(p\\) and \\(\\sigma^2\\), and use numerical methods to evaluate the likelihood function across different combinations of parameters. The MLEs for \\(\\mu, \\nu,\\) and \\(\\tau\\) is found by determining where the log-likelihood assumes a maximum.\n\ndata &lt;- c(rnorm(n = 0.1*1000, mean = 0, sd = 1),\n          rnorm(n = 0.9*1000, mean = 10, sd = 10))\n\nreturn_LL_at_given_set_parameters &lt;- function(parameters){\n  # parameters is a vector with values at which evaluate the likelihood\n  # contains value for normal means and standard deviations\n  \n  likelihood_each_observation &lt;- \n    0.1*dnorm(data, mean = parameters$mean1, sd = parameters$sd1) +\n    0.9*dnorm(data, mean = parameters$mean2, sd = parameters$sd2) \n  # this is the prob. dist of the data, but will look different depending on the application\n  \n  LL_at_given_theta &lt;- sum(log((likelihood_each_observation)))\n  return(LL_at_given_theta)\n}\n\nLL &lt;- c()\nvalues_to_test &lt;- expand.grid(mean1 = c(-0.5,-0.1,0,0.1,0.5),\n                              mean2 = seq(-20,20,by=0.5),\n                              sd1 = 1, sd2 = seq(5,15,by=0.1))\n\nfor (i in 1:nrow(values_to_test)){\n  parameters &lt;- as.data.frame(values_to_test[i,])\n  LL[i] &lt;- return_LL_at_given_set_parameters(parameters)\n}\n\nlog_likelihood &lt;- \n  as.data.frame(cbind(LL, values_to_test)) %&gt;% subset(is.finite(LL))\n\n### returns which parameters gave rise to max likelihood\nMLE &lt;- log_likelihood %&gt;% subset(LL == max(LL)); MLE\n\n             LL mean1 mean2 sd1  sd2\n20954 -3682.241   0.1   9.5   1 10.1\n\nlog_likelihood %&gt;% subset(mean1 == MLE$mean1 & \n                         mean2 == MLE$mean2 & sd1 == MLE$sd1) %&gt;%\nggplot(aes(x = sd2, y = LL)) + geom_line() +\n  geom_vline(xintercept = MLE$sd2, color = \"red\") + \n  labs(x = \"tau\", \n       y = \"log-likelihood (fixing all other parameters at their MLEs)\")\n\n\n\n## this shows the *profile* likelihood, a slice of the full likelihood\n## mu, sigma, and nu are fixed at their MLEs"
  },
  {
    "objectID": "lec07-intro-to-stats.html#hypothesis-testing-theory",
    "href": "lec07-intro-to-stats.html#hypothesis-testing-theory",
    "title": "8  Introduction to inference",
    "section": "8.7 Hypothesis testing: theory",
    "text": "8.7 Hypothesis testing: theory\nOften, the objective of a study is not to estimate a parameter but to decide which of two (or more) contradictory claims about the parameter is consistent with the data. This part of statistics is called hypothesis testing. Hypothesis testing is intimately connected to the construction and interpretation of confidence intervals which quantify estimation uncertainty.\n\n8.7.1 Null and alternative hypotheses\nA statistical hypothesis is an assertion about the values of one or more parameters, or the form of a probability distribution that is used to model the data.\nTwo contradictory hypotheses of the first kind are\n\\[H_0: \\theta = \\theta_0\\] \\[H_1: \\theta \\neq \\theta_0\\]\nThe first hypothesis is called the null hypothesis and may correspond to an expectation we have about the parameter (from, e.g., prior data). The second hypothesis is called the alternative hypothesis. The data are used to make a principle conclusion about if the null hypothesis is consistent with the data; if so, we reject the alternative hypothesis and, if not, we reject the null hypothesis as an explanation for the data generative process. In the following sections, we will describe the process for conducting such a hypothesis test.\n\n\n8.7.2 Test statistics\nSuppose we have data \\(X_1,\\dots,X_n \\sim f(x|\\theta)\\) and wish to test the above hypotheses, i.e., to decide if \\(\\theta \\neq \\theta_0\\). We do this by constructing a test statistic, i.e., function of the data, and assessing if the realized value of statistic is consistent with its distribution under the null hypothesis. Is the value of the statistic, at some level of significance, different from what we would expect if the null hypothesis were true?\nMany choices of test statistic are possible, but the likelihood ratio is one that is commonly used:\n\\[\\lambda_{LR} = -2 (\\ln L(\\theta_0)-\\ln L(\\hat{\\theta}_{\\text{MLE}})).\\]\nThe statistic is based on the likelihood function, and its asymptotic distribution (as the sample size becomes large) under the null hypothesis is known. \\(\\lambda_{LR}\\) has an approximate \\(\\chi^2\\) distribution under \\(H_0\\). (Importantly, one must know the approximate distribution of a test statistic to preform a hypothesis test.) Given the value and distribution of our test statistic under the null hypothesis, we can determine which two competing hypothesis is consistent with the data.\n\n\n8.7.3 How do we do this?\nTo decide between the null and alternative hypothesis, given a test statistic and its distribution under \\(H_0\\), we must specify a significance level \\(\\alpha\\). The significance level measures how likely we are to reject the null hypothesis, given that it is true: \\(\\alpha = P(\\text{reject } H_0 | H_0)\\). The significance level is chosen before data collection, and is typically set to 0.05 or smaller. By a similar token, the power of a statistical test is defined as the probability of rejecting the alternative hypothesis, given it is true: \\(1-\\beta = \\Pr(\\text{reject } H_0 | H_1).\\) Many factors affect the power of a test, but a test based on the likelihood ratio test statistic is the uniformly most powerful among all alternatives to test the above hypothesis. In general, one can preform (and it is best practice to preform!) analyses ahead of data collection to ensure power at a certain level.\nWe conduct a hypothesis test at significance level \\(\\alpha\\) as follows:\n\nState the null and alternative hypothesis and significance level \\(\\alpha\\).\nCollect data, possibly with knowledge of the sample size required to achieve a certain power.\nCalculate the realized value \\(s\\) of a test statistic \\(S\\), e.g., \\(\\lambda_{LR}\\). The test statistic must have a known distribution under the null hypothesis. The likelihood ratio has a \\(\\chi^2\\) distribution under \\(H_0\\) above.\nCompute the probability of observing the realized value of the test statistic or something more extreme, given the null hypothesis is true, i.e., \\(p = \\Pr(S &gt; s | H_0)\\). This probability is called a \\(p\\) value. If \\(p &lt; \\alpha\\), we reject the null hypothesis at significance level \\(\\alpha\\) and, if not, we fail to reject \\(H_0\\).\n\nTo illustrate how this works, we will return to the previous example. A test of \\(H_0: (\\mu,\\sigma,\\nu,\\tau) = (0,1,10,10)\\) vs \\(H_1: (\\mu,\\sigma,\\nu,\\tau) \\neq (0,1,10,10)\\) at significance level \\(\\alpha = 0.05\\) can be preformed as follows.\n\nlikelihood_at_null &lt;- log_likelihood %&gt;% \n  subset(mean1 == 0 & mean2 == 10 & sd1 == 1 & sd2 == 10)\n\ntest_statistic &lt;- -2*(likelihood_at_null$LL - MLE$LL) \ntest_statistic ### realized value of the likelihood ratio test statistic\n\n[1] 1.390309\n\npchisq(test_statistic, df = 1, lower.tail = FALSE)\n\n[1] 0.238353\n\n### if p &lt; 0.05, then we reject H0; otherwise, we fail to reject H0\n\nSince we have designed \\(H_0\\) so that it is true (i.e., the values for the parameters are those used to generate the data), it should come as no surprise that we fail to reject the null hypothesis in favour of \\(H_1\\).\nIn contrast, consider the test \\(H_0: (\\mu,\\sigma,\\nu,\\tau) = (0,1,10,5)\\) vs \\(H_1: (\\mu,\\sigma,\\nu,\\tau) \\neq (0,1,10,5)\\). In this case, \\(p &lt; \\alpha\\), so that we have sufficient evidence to reject the null hypothesis.\n\nlikelihood_at_null &lt;- log_likelihood %&gt;% \n  subset(mean1 == 0 & mean2 == 10 & sd1 == 1 & sd2 == 5)\n\ntest_statistic &lt;- -2*(likelihood_at_null$LL - MLE$LL) \ntest_statistic ### realized value of the likelihood ratio test statistic\n\n[1] 1435.57\n\npchisq(test_statistic, df = 1, lower.tail = FALSE)\n\n[1] 3.918456e-314\n\n### if p &lt; 0.05, then we reject H0; otherwise, we fail to reject H0\n\n\n\n8.7.4 Confidence intervals\nOne way to think of a \\(100(1-\\alpha)\\%\\) confidence interval is as the set of values \\(\\theta_0\\) such that we fail to reject the hypothesis \\(\\theta = \\theta_0\\) at significance level \\(\\alpha\\). (There are other ways to define and understand confidence intervals but they are somewhat oblique.) Depending on the application, it may be more convenient to do a hypothesis test or to construct a confidence interval. Unlike hypothesis tests, confidence intervals quantify the uncertainty around point estimates like the MLE. There are many ways to construct confidence intervals, but below we show one that involves finding which \\(\\theta_0\\) are such that \\(\\lambda_{LR} -2(\\ln L(\\theta_0)-\\ln L(\\hat{\\theta}_{\\text{MLE}})) &lt; \\chi^2_c\\), where \\(\\chi^2_c\\) a cutoff based on \\(100(1-\\alpha)\\%\\)-ile for a \\(\\chi^2\\) distribution with one degree of freedom.\n\ncutoff &lt;- qchisq(0.95,df=1)/2 \n# cutoff for admissible values based on 95%-ile for a chi-squared dist df=1\n\nlog_likelihood %&gt;% subset(mean1 == MLE$mean1 & \n                         mean2 == MLE$mean2 & sd1 == MLE$sd1) -&gt; LL_plot\n\nLL_plot %&gt;% subset(abs(LL - MLE$LL) &lt; cutoff) -&gt; values_inconfidence_interval\n\nc(min(values_inconfidence_interval$sd2),\n  max(values_inconfidence_interval$sd2)) ### confidence interval!\n\n[1]  9.7 10.5\n\nLL_plot %&gt;% ggplot(aes(x = sd2, y = LL)) + geom_line() +\n  geom_vline(xintercept = MLE$sd2, color = \"red\") +\n  geom_vline(xintercept = min(values_inconfidence_interval$sd2), \n              color = \"red\", linetype = \"dashed\") +\n   geom_vline(xintercept = max(values_inconfidence_interval$sd2), \n              color = \"red\", linetype = \"dashed\") +\n  labs(x = \"tau\", y = \"log-likelihood (all other parameters at their MLEs)\")\n\n\n\n\nThe limits for the \\(95\\%\\) confidence interval for \\(\\tau\\) are given by the dashed red lines. As before, the maximum likelihood estimate for \\(\\tau\\) (with all other parameters fixed at their MLEs) is represented by the solid red line."
  },
  {
    "objectID": "lec07-intro-to-stats.html#the-t-test-as-a-special-case-of-the-lrt",
    "href": "lec07-intro-to-stats.html#the-t-test-as-a-special-case-of-the-lrt",
    "title": "8  Introduction to inference",
    "section": "8.8 The t-test as a special case of the LRT",
    "text": "8.8 The t-test as a special case of the LRT\nMost of the tests that are presented in introductory statistics courses are special cases of the likelihood ratio test, or approximations to the LRT. Consider the t-test: \\(H_0: \\mu = \\mu_0\\) vs \\(H_1: \\mu \\neq \\mu_0\\). The \\(t\\)-test assumes the data \\(X_1,\\dots,X_n\\) are independent and normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nUnder these assumptions, the likelihood ratio test statistic is of the form\n\\[\\lambda_{LR} = \\bigg( 1 + \\frac{t^2}{n-1} \\bigg)^{-n/2},\\]\nwhere\n\\[t = \\frac{\\sqrt{n} (\\overline{x} - \\mu_0)}{S}\\]\nand\n\\[S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\overline{x})^2.\\]\n\\(S^2\\) is an unbiased estimator of the population variance. (The ML estimator is biased, so it is corrected and the unbiased estimator is used in calculating the test statistic.) The corresponding LR is based on the statistic \\(t\\), which has an approximate \\(t\\) distribution with \\(n-1\\) degrees of freedom.\nA \\(t\\) test assesses if there is evidence to reject \\(H_0\\) based on the realized value of \\(t\\) and knowledge of its approximate (asymptotic) distribution. As before, this involves calculating the realized value of the statistic and determining the probability of observing data more extreme under the null hypothesis; if this value is \\(&lt; \\alpha\\), the null hypothesis is rejected at significance \\(\\alpha\\). Importantly, any test based on the statistic \\(t\\) can be formulated in terms of the likelihood ratio; in this way, the \\(t\\) test is a special case of the LRT.\nThe t.test function implements several variants of the \\(t\\)-test in R:\n\ndata &lt;- rnorm(100, mean = 0, sd = 1) # change mean to 0.25, 0.3\nt.test(data, mu = 0.25, conf.level = 0.01)\n\n\n    One Sample t-test\n\ndata:  data\nt = -1.9921, df = 99, p-value = 0.04912\nalternative hypothesis: true mean is not equal to 0.25\n1 percent confidence interval:\n 0.06135010 0.06371506\nsample estimates:\n mean of x \n0.06253258 \n\n\n\n8.8.1 Tests of the equality of group means\nWith a one-sample \\(t\\)-test, you’re asking whether the mean of your sample differs significantly from a mean value that you expect it to have. You might want to use this if you have an expected population mean value from the literature. You may have even amassed your own dataset that you think is representative of a population’s true mean & you want to compare a newly collected sample. To determine if there are differences between group means, a two-sample \\(t\\)-test can be used:\n\nobs1 &lt;- rnorm(100, mean = 0, sd = 1)\nobs2 &lt;- rnorm(1000, mean = 1, sd = 1) # note difference in sample size\n\nt.test(obs1, obs2)\n\n\n    Welch Two Sample t-test\n\ndata:  obs1 and obs2\nt = -8.4782, df = 118.06, p-value = 7.578e-14\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.1665348 -0.7247809\nsample estimates:\nmean of x mean of y \n0.0994738 1.0451317 \n\n### alternatively, one can use the following syntax:\n\ndata &lt;- as.data.frame(rbind(cbind(obs = obs1, ID = 1), \n                            cbind(obs = obs2, ID = 2)))\n\nt.test(obs~ID, data) ### the syntax will be the same for linear models!\n\n\n    Welch Two Sample t-test\n\ndata:  obs by ID\nt = -8.4782, df = 118.06, p-value = 7.578e-14\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -1.1665348 -0.7247809\nsample estimates:\nmean in group 1 mean in group 2 \n      0.0994738       1.0451317 \n\n# anova(lm(obs~ID, data))\n\nWhen the goal of an investigation is to test if mean values of a dependent variable are different between categories that were paired by design, a paired \\(t\\)-test is used. An example where this would be appropriate is an experiment in which Daphnia abundance was sampled before and after exposure to a pathogen.\nThe syntax for a paired \\(t\\)-test is as follows:\n\nt.test(obs1, obs2, paired=TRUE) ## sample sizes must be equal here\n\nIn the next assignment, you will be asked to apply content from this lecture, and to read some notes on other tests (chi square, permutation, etc.) that are commonly used in ecology and evolution. Be on the lookout for these notes, which will be on Quercus and eeb313.github.io soon."
  },
  {
    "objectID": "lec08-linear-models.html#lesson-preamble",
    "href": "lec08-linear-models.html#lesson-preamble",
    "title": "9  Linear models",
    "section": "9.1 Lesson preamble:",
    "text": "9.1 Lesson preamble:\n\n9.1.1 Lesson objectives\n\nUnderstand the logic of simple and multiple linear regression, including the assumptions that are placed on the data, parameters, and errors.\nUnderstand the meaning of regression coefficients and how they are estimated.\nLearn how confidence intervals and \\(p\\)-values associated to the regression coefficients are calculated and used to test hypotheses.\nUnderstand how to implement linear models (including ANOVA) in R.\nDevelop familiarity with generalized linear models and some important examples (logistic, Poisson, negative binomial regression).\n\n9.1.2 Lesson outline\n\nLinear models: theory and examples\n\nStructure and assumptions, including interpretation of the effects\nLikelihood-based estimation and inference\nTransformations\nDummy variables, interactions between covariates, etc.\nAnalysis of Variance\n\nGeneralized linear models\n\nNon-normal errors, link functions\nEstimation and inference: even more likelihood!\nLogistic regression\nPoisson, negative binomial regression\n\n\n\n\nlibrary(tidyverse)\nlibrary(car)\nsurveys_subset &lt;- read_csv(\"survey_subset.csv\")\ntheme_set(theme_bw())"
  },
  {
    "objectID": "lec08-linear-models.html#linear-models-why-we-care",
    "href": "lec08-linear-models.html#linear-models-why-we-care",
    "title": "9  Linear models",
    "section": "9.2 Linear models: why we care",
    "text": "9.2 Linear models: why we care\nLinear models are at the heart of statistical practice in the physical, life, and social sciences! Linear regression actually refers to a family of modeling approaches that attempt to learn how the mean and/or variance of a response variable \\(\\boldsymbol{y} = (y_1,\\dots,y_n)\\) depend on (linear) combinations of variables \\(\\boldsymbol{x}_i = (x_{i1},\\dots,x_{in})\\) called predictors. In this lecture, we will discuss various forms of the linear model and assumptions placed on the data to make estimation and inference of the relationships between variables tractable. We will see how the likelihood function forms the basis for this estimation/inference, and how extensions of multiple regression (generalized linear models and mixed models!) can be understood in a likelihood framework. Our goal will be to become familiar with how these models work and how they are fit to data in R."
  },
  {
    "objectID": "lec08-linear-models.html#theory-likelihood-estimation-and-inference-for-linear-models",
    "href": "lec08-linear-models.html#theory-likelihood-estimation-and-inference-for-linear-models",
    "title": "9  Linear models",
    "section": "9.3 Theory: likelihood estimation and inference for linear models",
    "text": "9.3 Theory: likelihood estimation and inference for linear models\nA linear model takes the form\n\\[ y_i = \\beta_0 + \\beta_1 x_{1i} + \\cdots + \\beta_p x_{pi} + \\varepsilon_{i}, \\]\nwhere \\(i=1,\\dots,n\\) correspond to observations of a random variable \\(Y\\) which we call the response. The goal of regression is to explain and to predict the behavior of the response variable by adding together effects of \\(p\\) covariates \\(x_1,\\dots,x_p\\), which are also called predictors. Importantly, the phrase “linear model” is somewhat deceptive in that the model can be used to describe many kinds of functional relationships in the data: for example, we could use \\(x_2 = x_1^2\\) to model higher order effects of \\(x_1\\) on the response. In general, the linear model above specifies how the realizations \\(y_1,\\dots,y_n\\) of a random variable \\(Y\\) depend on the additive effects of one or more non-random covariates/predictors. We will discuss random and mixed effects next class, but the machinery used to estimate the effect sizes \\(\\beta_1,\\dots,\\beta_p\\) and error variance are very similar to the model in which the \\(x\\)s are assumed to be fixed.\nTo make estimation and inference tractable, the errors \\(\\varepsilon_i\\) are assumed to be Normal with mean zero and variance \\(\\sigma_i^2\\). Equivalently, we could assume the data \\(y_1,\\dots,y_n\\) are 1) independent and 2) Normal with mean(s) \\(\\beta_0 + \\beta_1 x_{1i} + \\cdots + \\beta_p x_{pi}\\) and variance(s) \\(\\sigma_1^2,\\dots,\\sigma_n^2\\). Although the normality assumption is not strictly necessary (nor is the assumption of equal variance across observations), they are commonly made and allow us to easily estimate the effect sizes and error variance. Under the assumption of normality and constant error variance \\(\\sigma_1^2 = \\dots = \\sigma_n^2 = \\sigma^2\\), the likelihood can be written as\n\\[ L(\\beta_0,\\beta_1,\\dots,\\beta_p,\\sigma^2) = \\prod_{i=1}^n (2 \\pi \\sigma^2)^{-1/2} e^{-(y_i - \\beta_0 - \\beta_1 x_{1i} - \\dots - \\beta_p x_{pi})^2/2\\sigma^2}. \\]\nEstimators of the regression coefficients \\(\\beta_1,\\dots,\\beta_p\\) and \\(\\sigma^2\\) are found by maximizing the likelihood. In fact, the estimators found by maximum likelihood methods are exactly those which minimize the distance from the model \\(y = \\beta_0 + \\beta_1 x_{1} + \\dots + \\beta_p x_{p}\\) from the data. The line (and in higher dimensions, plane) given by this equation is such that the the sum of squared departures of the line from the data is as small is it can be. Other notions of distance turn out to give rise to interesting extensions of the linear model, such as ridge and LASSO regression, which are beyond the scope of the course.\n\n9.3.1 Simple linear regression: \\(p=1\\)\nSimple linear regression is a special case of the general linear model (above), and corresponds to the case there is only one predictor, i.e., \\(p=1\\).\n\n\n9.3.2 The matrix vesion of the general linear model\nWhat is often used in theory and practice is a matrix version of the above model:\n\\[ \\boldsymbol{y} = \\begin{bmatrix}\n     y_{1} \\\\\n     y_{2} \\\\\n     \\vdots \\\\\n     y_{n}\n     \\end{bmatrix} = \\begin{bmatrix}\n     1 & x_{11} & x_{21} & \\cdots & x_{p1} \\\\\n     1 & x_{12} & x_{22} & \\cdots & x_{p2} \\\\\n     \\vdots & \\vdots & \\vdots & & \\vdots \\\\\n     1 & x_{1n} & x_{2n} & \\cdots & x_{pn}\n     \\end{bmatrix} \\begin{bmatrix}\n     \\beta_{1} \\\\\n     \\beta_{2} \\\\\n     \\vdots \\\\\n     \\beta_{p}\n     \\end{bmatrix} + \\begin{bmatrix}\n     \\varepsilon_{1} \\\\\n     \\varepsilon_{2} \\\\\n     \\vdots \\\\\n     \\varepsilon_{n}\n     \\end{bmatrix} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}.\n  \\]\nHere, \\(\\boldsymbol{y} = (y_1,\\dots,y_n)'\\) is a vector of measurements for the response, \\(\\boldsymbol{x_i} = (x_{i1},\\dots,x_{in})'\\) is a vector of measurements for the \\(k\\)th predictor, and \\(\\boldsymbol{\\varepsilon} = (\\varepsilon_1,\\dots,\\varepsilon_n)'\\) is a vector of measurement errors. The \\('\\) symbol corresponds to transposition, which involves interchanging the rows and columns of a vector or matrix.1\nWhen all the errors are modeled as above (i.e., as Normal with constant variance and mean zero), the ML estimator for the vector of regression coefficients is \\(\\boldsymbol{\\hat{\\beta}_{\\text{MLE}}} = (\\boldsymbol{X'X})^{-1} \\boldsymbol{X'} \\boldsymbol{y}\\).\n\n9.3.2.1 What do the regression coefficients mean?\nThe interpretation of the regression coefficients is as follows: \\(\\beta_j\\) describes how much the response is expected to change, all else constant, if we increase \\(x_j\\) by exactly one unit. Importantly, the fitted regression coefficients measure the effect (slope) of increasing \\(x_1,\\dots,x_j\\) on the response under the assumption of normal data — this distinction between the theoretical and fitted coefficients is important to keep mind.\n\n\n9.3.2.2 Categorical predictors\nBefore we dive into implementing linear models in R, it is important to mention how the linear model accommodates discrete predictors like sex (or genotype, ID, race). To deal with categorical predictors, we define the model in terms of a baseline and to interpret the regression coefficients relative to this baseline. This involves coding “dummy variables” \\(x_1,\\dots,x_{k-1}\\) for all but one the values (\\(1,2,\\dots,k\\)) the predictor can take one, so that \\(x_{ji} = 1\\) for observations where the categorical variable is \\(=j\\) and \\(=0\\) otherwise."
  },
  {
    "objectID": "lec08-linear-models.html#practice-fitting-linear-models-with-lm",
    "href": "lec08-linear-models.html#practice-fitting-linear-models-with-lm",
    "title": "9  Linear models",
    "section": "9.4 Practice: fitting linear models with lm",
    "text": "9.4 Practice: fitting linear models with lm\nTo illustrate how regression models are fitted (via maximum likelihood) in R, we will use the survey dataset from a couple classes ago. We begin by regressing weight on hindfoot length:\n\nmodel &lt;- lm(weight ~ hindfoot_length, data=surveys_subset)\nsummary(model) # assuming normality, homogeneity of variance\n\n\nCall:\nlm(formula = weight ~ hindfoot_length, data = surveys_subset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-60.342 -13.126  -4.018   3.351 221.135 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -33.2179     1.3575  -24.47   &lt;2e-16 ***\nhindfoot_length   2.5651     0.0441   58.16   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.19 on 3842 degrees of freedom\n  (505 observations deleted due to missingness)\nMultiple R-squared:  0.4682,    Adjusted R-squared:  0.4681 \nF-statistic:  3383 on 1 and 3842 DF,  p-value: &lt; 2.2e-16\n\n\nR returns the following after fitting a linear model via lm():\n\nDescriptive statistics for the “residuals” \\(\\varepsilon_i = y_i - \\widehat{\\beta_0} - \\widehat{\\beta_1} x_i\\), which tell us about how much variability there is in the data relative to the linear model specified and fitted.\nThe regression coefficients minimizing the sum of squared departures from the data (i.e., the ML estimates) and \\(95\\%\\) confidence intervals for each. The CIs are expressed as standard errors, since the estimators have an approximate Normal distribution. A joint confidence region for the coefficients can also be found using, e.g., the LRT statistic.\nA suite of test statistics! The \\(t\\) statistics and their \\(p\\) values are associated to the test \\(H_0: \\beta_i = 0\\) vs \\(H_1: \\beta_i \\neq 0\\). Significance codes specify the level \\(\\alpha\\) at which we have evidence to reject the null hypothesis for each coefficient.\nMeasures of goodness-of-fit: the multiple \\(R^2\\) and the adjusted \\(R^2\\). These explain the proportion of variance that are explained by the model. The latter measures the proportion of variance explained by the linear model upon adjusting for sample size and \\(\\#\\) of predictors.\n\n\nggplot(surveys_subset, aes(x = hindfoot_length, y = weight)) + \n  geom_point(aes(color = as.factor(sex)), size = 2) +\n  stat_smooth(method = \"lm\", se = F, color = \"gray\") + labs(color = \"sex\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 505 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n### are the data normal?\n\nggplot(surveys_subset, aes(x = weight)) + geom_histogram() ## oh no!\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 316 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n9.4.1 Transformations\nOften, data are non-normal! This is an unfortunate fact of life. It is sometimes possible, however, to use the machinery of regression if there is a suitable transformation of the data which makes it normal, e.g., log(), sqrt(). Right-skewed data (like above) may be normalized using log or root transformations (e.g. square root, third-root, etc.), with greater roots required for increasingly right-skewed data. Left-skewed data could be normalized with power transformations (e.g. squared, 3rd power, etc.).\n\nggplot(surveys_subset, aes(x = log(weight))) + geom_histogram() ## better!\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 316 rows containing non-finite values (`stat_bin()`).\n\n\n\n\nmodel_logtrasnformed &lt;- lm(log(weight)~hindfoot_length, data=surveys_subset)\nsummary(model_logtrasnformed)\n\n\nCall:\nlm(formula = log(weight) ~ hindfoot_length, data = surveys_subset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8285 -0.2135 -0.0473  0.1488  1.9612 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     1.5655038  0.0203235   77.03   &lt;2e-16 ***\nhindfoot_length 0.0647424  0.0006603   98.06   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3922 on 3842 degrees of freedom\n  (505 observations deleted due to missingness)\nMultiple R-squared:  0.7145,    Adjusted R-squared:  0.7144 \nF-statistic:  9615 on 1 and 3842 DF,  p-value: &lt; 2.2e-16\n\n\nAs before, there is evidence to reject the null hypothesis that hindfoot length has no effect on weight (or, in this case, its log transformation) at significance level \\(\\alpha = 0.05\\). This is because the \\(p\\)-value that is associated to the coefficent of hindfoot length is \\(&lt; \\alpha\\). We can also use what is returned by lm() to predict what the response will be if we observe new data (hindfoot lengths).\n\nnew_hindfoot_length_obs &lt;- data.frame(hindfoot_length = seq(0,100,0.1))\npredicted_values &lt;- predict.lm(object = model_logtrasnformed, \n                               newdata = new_hindfoot_length_obs)\n\nggplot(cbind(logweight = predicted_values, new_hindfoot_length_obs), \n       aes(x = hindfoot_length, y = logweight)) + geom_line(size = 1, color = \"gray\") + \n  geom_point(data = surveys_subset, inherit.aes = F,  size = 2, \n             aes(x = hindfoot_length, y = log(weight), color = as.factor(sex))) + \n  labs(color = \"sex\", y = \"log(weight)\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 505 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n9.4.1.1 Challenge\nRegress weight on sex. Be sure to specify that sex is a factor in the call to lm(). What value of sex is used as a baseline? Is there a significant effect of sex on weight?\n\nmodel &lt;- lm(weight~as.factor(sex), data=surveys_subset)\nsummary(model) \n\n\nCall:\nlm(formula = weight ~ as.factor(sex), data = surveys_subset)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-38.14 -22.14  -6.99   5.01 227.86 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      42.9896     0.8403  51.160   &lt;2e-16 ***\nas.factor(sex)M  -0.8537     1.1651  -0.733    0.464    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 36.89 on 4014 degrees of freedom\n  (333 observations deleted due to missingness)\nMultiple R-squared:  0.0001337, Adjusted R-squared:  -0.0001154 \nF-statistic: 0.5368 on 1 and 4014 DF,  p-value: 0.4638\n\n\n\n\n\n9.4.2 Syntax for multiple regression, interactions\nOne can regress on several explanatory variables simultaneously as follows:\n\nmodel &lt;- lm(weight~hindfoot_length+as.factor(sex), data=surveys_subset)\nsummary(model, type = 3)\n\n\nCall:\nlm(formula = weight ~ hindfoot_length + as.factor(sex), data = surveys_subset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.736 -12.860  -3.985   3.519 222.859 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -31.55118    1.41051 -22.369  &lt; 2e-16 ***\nhindfoot_length   2.57297    0.04414  58.287  &lt; 2e-16 ***\nas.factor(sex)M  -3.64249    0.84599  -4.306 1.71e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.15 on 3832 degrees of freedom\n  (514 observations deleted due to missingness)\nMultiple R-squared:  0.4702,    Adjusted R-squared:  0.4699 \nF-statistic:  1700 on 2 and 3832 DF,  p-value: &lt; 2.2e-16\n\n\nInteractions between covariates are modeled by introducing additional covariates of the form \\(x_i x_j\\). An interaction occurs when an independent variable has a different effect on the outcome depending on the values of another independent variable. They are super important! To estimate the coefficients associated to an interaction, * is used in the call to lm:\n\nmodel &lt;- lm(weight~hindfoot_length*as.factor(sex), data=surveys_subset)\n\nWe interpret the coefficient of the interaction term as we do all other regression coefficients. Per unit change in \\(x_i x_j\\) (here, the interaction between hindfoot length and sex), the associated regression coefficient measures the change in the response (weight).\n\nsurveys_subset %&gt;% subset(! is.na(sex)) %&gt;%\n  ggplot(aes(x = hindfoot_length, y = weight, color = as.factor(sex))) + geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 285 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 285 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n9.4.2.1 Challenge\nIs there a significant interaction between hindfoot length and sex on weight?\n\n\n\n9.4.3 The relationship between linear models and ANOVA\nAnalysis of Variance (ANOVA) is a statistical technique that is used to analyze variation in observations within and between categories, and to test if two or more population means are equal. Importantly, ANOVA is mathematically equivalent to regression with covariates that are all categorical. This means the assumptions of linear regression apply when preforming an ANOVA.\nANOVA can be implemented in R as follows:\n\nmodel &lt;- lm(weight~as.factor(sex), data=surveys_subset)\nanova(model) # must wrap the anova() around a lm()/model\n\nAnalysis of Variance Table\n\nResponse: weight\n                 Df  Sum Sq Mean Sq F value Pr(&gt;F)\nas.factor(sex)    1     730  730.48  0.5368 0.4638\nResiduals      4014 5461759 1360.68               \n\n\nBriefly, ANOVA tests differences between means by decomposing the total sum of squares (variance in the observations) into the variance due to the level under investigation (e.g., sex) and its factors (M, F). The \\(F\\)-statistic for each level (excluding the residuals) is the mean square, divided by the residual mean square, and should be \\(\\sim 1\\) if the corresponding effects are \\(=0\\). We reject the null hypothesis (there is no differences in group means) for values of \\(F\\) that are inconsistent with its distribution under the null hypothesis; recall from last time that the \\(p\\)-value measures the probability of observing data more extreme than the calculated value of \\(F\\) from the data, under the null hypothesis (i.e., the factors at some level have no effect). If \\(p &lt; \\alpha\\), we reject the null hypothesis at significance level \\(\\alpha\\).\n\n9.4.3.1 ANOVA with two or more covariates\n\nmodel &lt;- lm(weight~hindfoot_length*as.factor(sex), data=surveys_subset)\n\n# wrap Anova(), not summary()\nAnova(model, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: weight\n                                Sum Sq   Df   F value  Pr(&gt;F)    \n(Intercept)                     203901    1  298.3134 &lt; 2e-16 ***\nhindfoot_length                1151284    1 1684.3645 &lt; 2e-16 ***\nas.factor(sex)                      48    1    0.0709 0.79010    \nhindfoot_length:as.factor(sex)    1951    1    2.8544 0.09121 .  \nResiduals                      2618537 3831                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# compare p-values with summary(), which are slightly different\nanova(model)\n\nAnalysis of Variance Table\n\nResponse: weight\n                                 Df  Sum Sq Mean Sq   F value    Pr(&gt;F)    \nhindfoot_length                   1 2312681 2312681 3383.5236 &lt; 2.2e-16 ***\nas.factor(sex)                    1   12677   12677   18.5473 1.699e-05 ***\nhindfoot_length:as.factor(sex)    1    1951    1951    2.8544   0.09121 .  \nResiduals                      3831 2618537     684                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInstead of wrapping summary() around our linear model object, we use the Anova() function from the cars package to test the significance of our predictors. A Type 1 ANOVA (sequential sum of squares) will test the main effect of variable A, followed by the main effect of variable B after the main effect of A, followed by the interaction effect of AB. Since it tests factor A without controlling for factor B, the order that you specify your independent variables will matter! A Type 2 ANOVA (hierarchical sum of squares) tests for the main effect of variable A and variable B, but it does not assume that there is an interaction. If you have a factorial design, you should not be using a Type 2 ANOVA. Finally, a Type 3 ANOVA (marginal sum of squares) tests for the presence of a main effect while controlling for the other main effect and interaction; that is, it tests the effect of variable A if all other variables are already considered. This approach is therefore valid in the presence of significant interactions.\nThe type of ANOVA you use will matter when you have more than one independent variable, and especially if you have unbalanced data. By default, R uses type II sums of squares. Above, we are use Type 3 sums of squares as we care about the interaction between hindfoot length and sex, and we don’t prioritize the effects of one variable.\n\n\n\n9.4.4 A fun application!\nIn quantitative genetics, regression is used to estimate the strength of directional, stabilizing, or disruptive selection on continuous traits (e.g., beak length) controlled by a large number of genes of small additive effect. Without getting into the weeds, the regression of relative fitness (or some proxy for fitness) on trait value provides an estimate of the selection differential \\(S\\), defined as the co-variance between fitness and the trait. The observation linear models could be used to estimate selection on on or more quantitative traits (possibly correlated) was operationalized in 1983 by Lande & Arnold. When trait measurements are normalized, the slope of the regression equals the strength of directional selection on the trait. Regression has been applied in a wide range of plant and animal taxa to estimate selection and, when there are multiple traits under investigation, the relative importance of direct vs indirect selection on evolution.\n\ngenerate_data_directionalselection &lt;- function(n = 100, values){\n  \n  trait_data &lt;- sort(sample(size = n, x = values, replace = T))\n  fitness_data &lt;- 1 - dexp(trait_data, rate = 1) + rnorm(n, mean = 0, sd = 0.05)\n  \n  return(\n    data.frame(\n      trait = trait_data, fitness = fitness_data/mean(fitness_data)\n      )\n  )\n}\n\ndata &lt;- generate_data_directionalselection(values = seq(0,3,0.01))\n\nmodel1 &lt;- lm(fitness~scale(trait), data)\n# scale function transforms trait data so that mean=0, variance=1\n\nsummary(model1) \n\n\nCall:\nlm(formula = fitness ~ scale(trait), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.36874 -0.11840  0.03626  0.13023  0.30442 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.00000    0.01590   62.90   &lt;2e-16 ***\nscale(trait)  0.37040    0.01598   23.18   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.159 on 98 degrees of freedom\nMultiple R-squared:  0.8457,    Adjusted R-squared:  0.8442 \nF-statistic: 537.3 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n### fitness changed by ~50% per sd change in trait value!\n\nggplot(data, aes(x = scale(trait), y = fitness)) + \n  geom_point(color = \"gray\") +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = F) +\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), se = F,\n              color = \"red\") +\n  labs(x = \"standardized trait value\", y = \"relative fitness\")\n\n\n\nmodel2 &lt;- lm(fitness~scale(trait)+scale(trait^2), data)\nsummary(model2)\n\n\nCall:\nlm(formula = fitness ~ scale(trait) + scale(trait^2), data = data)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.157196 -0.059319  0.000927  0.042577  0.199743 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     1.000000   0.007575  132.02   &lt;2e-16 ***\nscale(trait)    0.885050   0.029141   30.37   &lt;2e-16 ***\nscale(trait^2) -0.533167   0.029141  -18.30   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07575 on 97 degrees of freedom\nMultiple R-squared:  0.9653,    Adjusted R-squared:  0.9646 \nF-statistic:  1351 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nHere, the blue (red) regression line is the linear (quadratic) individual selection surface, estimated by regressing fitness on trait value (and its square) simulated under a model of directional selection.\n\n9.4.4.1 Other applications…\nRegression has also been used in quantitative genetics to estimate the hertiability of traits (i.e., the proportion of variance in the trait that is explained by the additive action of genes), and to preform Genome-Wide Association Studies (GWAS) and uncover the genetic basis of complex traits. One form of GWAS involves regressing phenotype (e.g., disease status) on the existence/non-existence of genetic differences (most often, SNPs) among individuals sampled, and identifying those which have a significant effect on phenotype."
  },
  {
    "objectID": "lec08-linear-models.html#generalized-linear-models-theory-and-examples",
    "href": "lec08-linear-models.html#generalized-linear-models-theory-and-examples",
    "title": "9  Linear models",
    "section": "9.5 Generalized linear models: theory and examples",
    "text": "9.5 Generalized linear models: theory and examples\nSo far we have seen how regression can be used to fit linear models when the distribution of the data is approximately normal or when the data can be transformed so that this assumption is not violated. What if we were, say, interested in a binary response (e.g., disease status) and how it changes with a continuous predictor (e.g., age)? In this case, one can use a special kind of linear model called logistic regression to estimate the additive effect of predictor(s) on the binary response. Generalized linear models (GLMs) are useful when the response has a non-normal distribution, and transformation of the data is undesirable or impossible. A GLM takes the form\n\\[E(\\boldsymbol{y}|\\boldsymbol{X}) = g^{-1}(\\boldsymbol{X} \\boldsymbol{\\beta}),\\]\nwhere \\(g(\\cdot)\\) is a smooth and invertible link function taking the conditional expectation on the LHS to the linear predictor on the RHS. The link function for distributions in the overdispersed exponential family (including the Normal, Gamma, Exponential, Poisson, and Multinomial) are known. GLMs with these data distributions can be implemented in R by calling glm() with the appropriate family specified:\n\nresult_binary &lt;- glm(as.factor(sex)~weight, \n                     family=binomial, \n                     data=surveys_subset)\n\nsummary(result_binary)\n\n\nCall:\nglm(formula = as.factor(sex) ~ weight, family = binomial, data = surveys_subset)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.222  -1.214   1.134   1.143   1.204  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  0.1074131  0.0482240   2.227   0.0259 *\nweight      -0.0006272  0.0008560  -0.733   0.4638  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5560.8  on 4015  degrees of freedom\nResidual deviance: 5560.3  on 4014  degrees of freedom\n  (333 observations deleted due to missingness)\nAIC: 5564.3\n\nNumber of Fisher Scoring iterations: 3\n\n\nAbove are estimates of coefficients of a regression of sex on weight, as well as some details about the procedure used to fit those parameters. All GLMs are fitted via maximum likelihood, using using numerical optimization methods like iteratively reweighted least squares.\nUsing the cannonical link function, logistic regression can be formulated as follows:\n\\[ \\text{logit}(p) = \\log\\frac{p}{1-p} = \\boldsymbol{X} \\boldsymbol{\\beta} \\iff p = e^{\\boldsymbol{X} \\boldsymbol{\\beta}} = e^{\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p}. \\]\n\\(\\text{logit}(p)\\) is called the log-odds, which can be thought of as a likelihood the response takes on the value one. Under logistic regression, the log-odds ratio is modeled as a linear combination in the predictors \\((= \\boldsymbol{X} \\boldsymbol{\\beta})\\). Since the regression coefficients appear only through the logit-transformed proportions, their interpretation is somewhat different under logistic regression and other GLMs. Importantly, changing \\(x_j\\) by one unit, all else constant, results in change \\(\\beta_j\\) to the link-transformed response. This is how the effect sizes are interpreted for GLMs like the one fitted above.\n\nsurveys_subset %&gt;% filter(! is.na(sex)) %&gt;%\n  mutate(ID = ifelse(sex == \"M\", 0, 1)) %&gt;%\n  ggplot(aes(x = weight, y = ID)) + geom_point() +\n  geom_smooth(method = \"glm\", \n              method.args = list(family = \"binomial\"),\n              se = T\n              )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 104 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 104 rows containing missing values (`geom_point()`).\n\n\n\n\n\nHere are some common types of response variables and their corresponding distributions:\n\nCount data (positive integers only): Poisson distribution\nOver-dispersed count data (when the count data is more spread out than “expected”): negative binomial distribution\nBinary data (two discrete categories): binomial distribution\n\n\n9.5.1 Returning to the disease-age example…\nBelow we simulate binary disease data (0,1) for patients of several ages, assuming the the log-odds of disease is a linear function of age. We then fit a logistic regression to this data to determine the effect of age on disease risk. Next, we write a function to do a power analysis. That is, we determine the sample size that is required so that simulating the age-disease data repeatedly we identify a significant effect of age on disease status (i.e., reject the null hypothesis) at level \\(\\alpha = 0.01\\) at least \\(99\\%\\) of the time.\n\ndata_generator &lt;- function(sample_size = 100){\n  age &lt;- sample(size = sample_size, x = seq(0,100,0.1), replace = T)\n  linear_predictor &lt;- 0.8*scale(age)\n  prob &lt;- 1/(1+exp(-linear_predictor))\n  \n  disease_status &lt;- c()\n  \n  for (i in 1:length(prob)){\n  disease_status[i] &lt;- rbinom(n = 1, size = 1, prob = prob[i])\n  }\n  \n  return(data.frame(age = age, disease_status = disease_status))\n}\n\ndata &lt;- data_generator()\n\ndata %&gt;% pivot_longer(! age) %&gt;% \n  ggplot(aes(x = age, y = value)) + geom_point() + \n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\")\n              ) + labs(y = \"prob. of disease (i.e., disease status =1)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nmodel &lt;- glm(disease_status~scale(age), family = binomial, data = data)\nsummary(model)\n\n\nCall:\nglm(formula = disease_status ~ scale(age), family = binomial, \n    data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6236  -1.0219  -0.7782   1.0535   1.6736  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -0.08469    0.21035  -0.403  0.68722   \nscale(age)   0.65288    0.21870   2.985  0.00283 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.47  on 99  degrees of freedom\nResidual deviance: 128.78  on 98  degrees of freedom\nAIC: 132.78\n\nNumber of Fisher Scoring iterations: 4\n\npower_analysis_function &lt;- function(sample_size){\n  \n  sims &lt;- 1000\n  pvalues &lt;- c()\n  for (i in 1:sims){\n  data &lt;- data_generator(sample_size)\n  model &lt;- glm(disease_status~scale(age), family = binomial, data = data)\n  pvalues[i] &lt;- summary(model)$coefficients[2,4]\n  }\n  \n  power_estimate &lt;- length(which(pvalues &lt; 0.01))/length(pvalues)\n  \n  return(power_estimate)\n}\n\nsample_sizes &lt;- seq(100,200,10); power_estimates &lt;- c()\n\nfor (i in 1:length(sample_sizes)){\n  power_estimates[i] &lt;- power_analysis_function(sample_size = sample_sizes[i])\n}\n\nknitr::kable(cbind(n = sample_sizes, power = power_estimates))\n\n\n\n\nn\npower\n\n\n\n\n100\n0.895\n\n\n110\n0.933\n\n\n120\n0.938\n\n\n130\n0.956\n\n\n140\n0.971\n\n\n150\n0.989\n\n\n160\n0.986\n\n\n170\n0.991\n\n\n180\n0.997\n\n\n190\n1.000\n\n\n200\n0.997"
  },
  {
    "objectID": "lec08-linear-models.html#footnotes",
    "href": "lec08-linear-models.html#footnotes",
    "title": "9  Linear models",
    "section": "",
    "text": "In R, the function t() can be used to transpose a vector or matrix.↩︎"
  },
  {
    "objectID": "projects.html#option-1-hypothesis-driven-project",
    "href": "projects.html#option-1-hypothesis-driven-project",
    "title": "10  Project description",
    "section": "10.1 Option 1: Hypothesis-driven project",
    "text": "10.1 Option 1: Hypothesis-driven project\nGroups will formulate their own hypotheses based on their interests within ecology and evolution. Groups will test predictions borne out of their hypotheses with reproducible and quantitative analysis techniques (e.g., ANOVA). If your group has an idea for statistical analyses that are beyond the scope of the course, please let us know. We are happy to support any groups who want to learn new tools, but expect that these groups are ready to learn how these tools work on their own; we hope to equip you with enough understanding to learn new things independently. Finally, the work must be original – while we may be repurposing data, we will not be simply redoing analyses. Keep in mind also that any work you do as part of this course may not be submitted for credit in another course (such as a fourth-year research project) and vice versa. While you may not submit your work for this course for credit in another course, you are welcome to publish or present your work in an academic setting.\nA note about community/citizen science websites: since the data is community-controlled, it may not always be research quality. There may be incorrect species IDs, inaccurate geolocations or time of observations, or discrepancies in protocols. When working with community science data, make sure that the data is cleaned and wrangled so that it is reliable. Quality control is a good first step when working with data, as simple errors can exist in any dataset.\n\n10.1.1 What is a hypotheses? What is a prediction?\nA hypothesis is a testable and falsifiable statement that offers a possible explanation of a phenomenon based on background knowledge, preliminary observations, or logic.\nE.g., Primary productivity is an important driver of mammal species richness.\nA prediction is based on a hypothesis. It is meant to describe what will happen in a specific situation, such as during an experiment, if the hypothesis is correct.\nE.g., If primary productivity is an important driver of mammal species richness, then more mammalian species would be found in sites with more plant biomass (proxy for primary productivity) compared with sites with less plant growth."
  },
  {
    "objectID": "projects.html#option-2-modeling",
    "href": "projects.html#option-2-modeling",
    "title": "10  Project description",
    "section": "10.2 Option 2: Modeling",
    "text": "10.2 Option 2: Modeling\nGroups will develop a mathematical model to answer a question in ecology and/or evolution they find interesting. There are many reasons to develop models: they help clarify assumptions, generate predictions, nullify hypotheses, provide mechanistic explanations for observed data, and help us know what kinds of data to look for. New models almost always build on existing and well-studied ones (e.g., the Lotka-Volterra model). The fact models are simplifying representations of the real world is by design! The goal of building a model is to identify the key features that make a process interesting, represent the process mathematically (and, in doing so, clarify what assumptions are being made!), characterize the behavior of the model, and from this characterization draw conclusions about how the process being modeled works. Characterization of a model can involve mathematical analysis, simulation, and confrontation with data.\nThe key steps in this project are to 1) identify an interesting question in ecology or evolution, 2) develop (and likely revise) a model to address that question, 3) characterize the behavior of the model, and 4) draw biological conclusions from the model and its characterization.\nIf you are interested in modeling, let Vicki and Mete know as soon possible!"
  },
  {
    "objectID": "projects.html#option-3-simulation-study",
    "href": "projects.html#option-3-simulation-study",
    "title": "10  Project description",
    "section": "10.3 Option 3: Simulation study",
    "text": "10.3 Option 3: Simulation study\nSimilar to Option 1, groups that do a simulation study will formulate hypotheses and use reproducible and quantitative analysis techniques to test predictions borne out of those hypotheses. The difference is that students will simulate their own data, instead of using an existing dataset. One reason to do a simulation study is to see what kind of data would be needed to test a hypothesis in the field, e.g., how much data would be needed to find a significant association between response and predictor variables.\nIf you are interested in doing a simulation study, let Vicki and Mete know as soon possible!"
  },
  {
    "objectID": "projects.html#finding-a-topic",
    "href": "projects.html#finding-a-topic",
    "title": "10  Project description",
    "section": "10.4 Finding a topic",
    "text": "10.4 Finding a topic\nHere are some discussion questions to help you and your group work towards a research topic and set of hypotheses and predictions:\n\nWhat is a paper you read recently that you found really interesting?\nWhat is your favorite EEB course so far? Why did you like it?\nThinking about EEB professors, was there anyone whose work you are particularly interested in?\nBrowse through some recent issues of broad scope EEB journals such as Trends in Ecology and Evolution and Annual Review of Ecology, Evolution, and Systematics. Any articles catching your eyes?\nCheck out this paper. Any of those questions spark your interest?"
  },
  {
    "objectID": "projects.html#project-timeline-and-deliverables",
    "href": "projects.html#project-timeline-and-deliverables",
    "title": "10  Project description",
    "section": "10.5 Project timeline and deliverables",
    "text": "10.5 Project timeline and deliverables\nAs instructors, we are here to help your group work towards a project idea that you are excited about! We have included multiple check points and small assignments throughout the semester for you to get feedback on your project ideas and ask us questions.\n\n10.5.1 Project proposal\nDue Oct 3rd, worth 4% of final grade\nGood research takes time! The purpose of the proposal is to get your group started on this process early on so that you will have sufficient time to do your project justice. This will also serve as official documentation of your project development process. Your projects will likely evolve over time, and there can be many reasons for this. For instance, as you explore your data, you might be inspired to ask different questions, or you may need to refine your hypotheses due to limitations in the data. All of these are fine, in fact, it happens all the time in real research settings.\nInclude the following information in your proposal:\n\nOption 1: your hypotheses and predictions (point form or short paragraph) and data source (short paragraph). Include a citation, a brief description of how the data was collection, and which section of the dataset you plan to use in your analysis (e.g., which columns).\nOption 2: a question you want to answer using a mathematical model (short paragraph describing the problem and the value modeling may add). Be sure to include a description of the variables that you may want to track and the kind of model you envision using.\nOption 3: same as 1, except with a description of how to simulate the data.\n\n\n\n10.5.2 Mid-project update\nDue Nov 2nd, worth 6% of final grade\nThe purpose of the mid-project update is to ensure you are on track with your projects. By now, you should have completed your exploratory data analyses, modeling, or simulation. You should have also solidified your hypotheses, predictions, and analyses plan. Essentially, you should be ready to write the Methods section of your report!\nIncluded the following information in your mid-project review:\n\nOptions 1 and 3:\n\n\nYour hypotheses and predictions (point form or short paragraph). If these differ from the ones in your proposal, explain clearly the rationale for the change.\nA detailed description of your data (a paragraph), including how the data was collected or simulated, along with any manipulation(s) you performed to get your data ready for the analysis.\nYour analysis plan (a paragraph): describe the statistical test(s) that you will use to test each prediction, including how you will validate the assumptions of each test.\n\n\nOption 2:\n\n\nA detailed description of the question you want to answer, any previous work (modeling and otherwise), the model you have built to answer this question, and your modeling assumptions.\nDetailed descriptions of the model analysis and biological interpretations of the results so far.\nYour analysis plan (a paragraph): describe additional analysis that you will do and any assumptions you would like to relax.\n\n\n\n10.5.3 Presentation\nDue Dec 5th, worth 10% of final grade\nThe presentations will be held on the last day of class during regular class hours (Dec 5th, 2-4 pm). Each presentation will be 10 minutes long, followed by 2 minutes of questions from the audience. If you cannot make it to class for your presentation, please get in touch with us to make alternative arrangements no later than Dec 1st.\n\n\n10.5.4 Report\nDue Dec 8th, worth 20% of final grade\nThis report will be styled as a journal article, with these sections:\n\nAbstract\nIntroduction\nMethods (including “Data Description” and “Data Analysis” subsections)\nResults\nDiscussion\nReferences\nSupplementary material consisting of data and code required to reproduce analysis\n\nFor your sake (and ours), we are enforcing a two page limit (single spaced, excluding figures, tables, code, references, and appendices). Please use a standard font, size 12, with regular margins. One goal of this assignment is to write clearly and concisely – it is often clarifying to put your analyses in as few words as possible.\nFor the report, you are expected to:\n\nPut your research questions in the context of existing research and literature.\nHave clear and explicit objectives, hypotheses, and/or predictions.\nAdequately describe and properly cite the data source(s) you will analyze. If your project involves modeling, describe other modeling work that is relevant.\nDescribe your analysis in sufficient detail for others to understand.\nDiscuss the interpretation of your results and their implications.\n\nThe data and code associated with your report is expected to be entirely reproducible. Your supplementary files must include the following:\n\nA description of what every column/row in your submitted data file.\nA well-annotated R script or R notebook file. We must be able to run your code once you submit the project. This lesson on best practices for writing R code is a good starting place. Also check out this coding style guide and these simple rules on how to write code that is easy to read.\n\nHermann et al. 2016 is a great example of what we expect your code to look like. Refer to their supplementary materials for examples of how to describe your data set and how to annotate your code."
  },
  {
    "objectID": "projects.html#project-grading-rubric",
    "href": "projects.html#project-grading-rubric",
    "title": "10  Project description",
    "section": "10.6 Project grading rubric",
    "text": "10.6 Project grading rubric\n\n10.6.1 Project proposal\n4 marks total\nOption 1: Two marks each for 1) your hypotheses and associated predictions and 2) a description of your data source(s). Students are expected to demonstrate effort in formulating hypotheses and predictions, and identifying a suitable dataset.\nOption 2: Two marks each for 1) a clear description of the question or problem in ecology or evolution you would like to address using a model, and 2) a description of the kind of model you envision using, including what variables to track.\nOption 3: One mark for simulating realistic data using appropriate tools, and one mark for your hypotheses and associated predictions, and two marks for describing the appropriate analyses.\nThese components will be graded mostly on completion. The purpose of this assignment is to ensure you start early and are heading towards the right track.\n\n\n10.6.2 Mid-project update\n6 marks total\nOptions 1 and 3: Two marks are given to clearly stating hypotheses and predictions. In the case that these are different from the original submission in the proposal, the rationale for refinement needs to be clearly explained.\nEach of the following criteria are scored out of 2: 2 == excellent, 1.5 == good, 1 == acceptable, but needs improvement.\n\nData description\n\nThe data source(s) are sufficiently described, specifically, where was the obtained and how it was originally collected.\nThe data is sufficient described, including any initial observations from your exploratory data analyses.\nThe suitability of the data is justified.\nAny manipulations done to the data are thoroughly explained and well-justified.\n\nData analysis plan\n\nClearly lay out the statistical test(s) you will use to test each prediction.\nState how you will validating assumptions associated with each statistical test.\n\n\nOption 2: Each of the following criteria are scored out of 3: 3 == excellent, 2 == good, 1 == acceptable, but needs improvement.\n\nDescription of question, previous work, the model, modeling assumptions, and any predictions you have ahead of the analysis\n\nThe question you want to address and previous work in that direction (modeling or otherwise) is described in detail.\nThe relationship between the question/problem and modeling approach is clear and well-justified.\nModeling assumptions and choices (including limitations) are clear and well-motivated.\nPredictions for how the model will behave, what it might have to say about the question/problem, etc. are inclued and well thought out.\n\nAnalysis and analysis plan\n\nThe details of all analysis (mathematical or computational) are explained clearly.\nThe biological interpretations of results so far are clearly presented and their validity/applicability is discussed.\nClearly lay out plans for remaining analysis (e.g., relaxing model assumptions) and justify why they are reasonable.\n\n\n\n\n10.6.3 The presentation\n10 marks total\nEach of the following criteria are scored out of 3: 3 == excellent, 2 == adequate, 1 == needs improvement.\n\nContent – background and methods\n\nThe context for the study, along with hypotheses and predictions, are clearly set up.\nData source(s), manipulations, and statistical tests used are succinctly and adequately described.\nIf modeling, the relationship between the question/problem addressed and modeling approach is well-explained, and previous work (modeling or otherwise) is discussed.\n\nContent – results and conclusions\n\nResults are accurately described and interpreted, with particular attention to how they related to the hypotheses and predictions the group set out to test.\nThe conclusion to the study is succinct and clear.\n\nDelivery\n\nAll students participated in presenting the information.\nAll students spoke clearly and without jargon.\nThe presentation is well organized and ideas flowed naturally from one to the next.\nThe presentation is well rehearsed and is an appropriate length.\nFigures are easy to read (e.g., axis labels are big enough to read and are informative) and are explained thoroughly (e.g., x and y axis and what each data point is).\n\n\nThe final 1 mark will be assigned to the question period, and students will be assessed on whether they are able to answer questions thoughtfully.\n\n\n10.6.4 The report\n20 marks total\nEach of the following criteria are scored out of 4: 4 == excellent, 3 == good, 2 == acceptable, 1 == needs improvement.\n\nContent and concepts\n\nAuthors demonstrate a full understanding of the existing literature on the topic, and these concepts are critically integrated into their own insights.\nOptions 1 and 3: Hypotheses and predictions are clearly defined, and rational for choosing/simulating this data is justified.\nOption 2: The question, modeling approach, and relevant work are thoughtfully explained; the rationale for using the model (and its assumptions) is justified.\n\nCommunication\n\nWriting is succinct, clear, logical, and free of grammatical and spelling errors.\n\nAnalysis: see below.\nResults\n\nResults are accurately and sufficiently described.\nConclusions are supported by evidence.\nFigures and tables are clearly presented and are informative.\n\nCoding style and reproducibility\n\nData and code are well-organized and well-documented.\nThe analysis is easily reproducible.\n\n\nNote: marks for the 3rd criterion (Analysis) depend on if groups did a modeling or data-driven project:\nOptions 1 and 3: Statistical analysis\n\nStatistical tests chosen or modeling choices made are appropriate.\nAssumptions for each statistical test is validated.\nLimitations in the data and analysis are discussed.\n\nOption 2: Analysis of model\n\nCharacterization of the model is appropriate and explained in detail.\nImportantly, biological conclusions explained in detail and in terms of the processes described (or not described) by the model.\nLimitations of modeling assumptions are discussed, and extensions are proposed.\n\nPlease note that we are only going to be marking the two pages of your report. Please do not go over the page limit (with the exception of tables, figures, references, and appendices)."
  },
  {
    "objectID": "projects.html#tips-on-writingpresenting-a-research-project",
    "href": "projects.html#tips-on-writingpresenting-a-research-project",
    "title": "10  Project description",
    "section": "10.7 Tips on writing/presenting a research project",
    "text": "10.7 Tips on writing/presenting a research project\nWe know that students have very unique research interests and ideas, and we hope that your project encapsulates that! As instructors, we do not know everything, but we are excited to learn from you and your projects. Below are some tips that we have gathered that you may find helpful when preparing for the project presentation and writing your report.\n\nUse a title that summarizes your project/results clearly.\nDefine everything! Do not assume that we know about your question, study system, etc. For your presentations, adding some pictures will help when you are defining something.\nAfter introducing your study system, tell us clearly your hypothesis and prediction: “I hypothesize that there are more mosquitoes in the boreal forest because it is warmer. I predict this because insects have a thermal tolerance”. Then, after your methods, results, etc., remind us of your hypothesis again! For your presentation, you can even show the same slide you used for your hypothesis with a big red X or a big green checkmark. Assume we forgot and that we know nothing about the system.\nNEVER EVER USE THE WORD “prove”. Science cannot prove or disprove anything — the evidence can only support (or fail to support) how we think the world works.\nUse an appropriate font and font size. Also, use colours wisely (e.g., avoid red and blue together because of folks that are colourblind).\nA 10-minute presentation is about 10 slides (more or less depending on if you use animations). A note about animations: use “Appear”, not any of the fancy stuff. And no slide transitions!\nWe will ask questions after your presentation, but we are not trying to trick you — we just want more information. Give us your best answer, and remember that it’s okay to say “I don’t know, but I think that…” or “I can test this further by doing this”. At this point, you should know more about your projects than we do. Also, when preparing for the presentation, it useful to think about what questions listeners may have and try to answer them preemptively.\nPractice your presentation at least once with your group! It’ll get rid of any nerves you have if you already know the words you are going to say. It’ll also help you ensure that you speak louder and slower. We know you all will do great projects, and we are excited to hear about them!\n\nReading widely and often is one of the best ways to learn how to write well. Here are some papers which we think are clear, concise, and free of grammatical and logical flaws.\n\nViral zoonotic risk is homogenous among taxonomic orders of mammalian and avian reservoir hosts\nNonsystemic fungal endophytes increase survival but reduce tolerance to simulated herbivory in subarctic Festuca rubra\nEstimation of the strength of mate preference from mated pairs observed in the wild\nHumans introduce viable seeds to the Arctic on footwear\nEffects of environmental warming during early life history on libellulid odonates\nThe role of evolution in the emergence of infectious diseases\nCoevolution of parasite virulence and host mating strategies\nA rigorous measure of genome-wide genetic shuffling that takes into account crossover positions and Mendel’s second law\nThe role of divergent ecological adaptation during allopatric speciation in vertebrates"
  },
  {
    "objectID": "databases.html",
    "href": "databases.html",
    "title": "11  Some open-access databases",
    "section": "",
    "text": "Below are some resources that would be a good to look at if you are in search of data for the term project, or in search of a question in ecology or evolution:\n\nGenBank: annotated collection of all publicly available DNA/protein sequences. It is possible to download sequences manually, but command line tools can help to automate the process.\nPanTHERIA: database of ecology, life history, and geography of all extant and recently extinct mammal species. Includes body size, lifespan, litter size, and other trait data at the species level.\nGene Expression Omnibus: repository of gene expression, methylation, and annotated genomic data which are (like GenBank) most readily accessible using command line tools.\nContinuous Plankton Recorder Survey: data (going back to 1958!) on northern hemisphere plankton species, including the location (latitude, longitude) and date of sampling.\nRed-backed salamander abundance: abundance of red-backed salamanders from 4 sites in the Bruce Peninsula from 2004 to 2017.\nNorth American Bird Breeding Survey: repository containing information regarding the number of birds at multiple sites in North America. Many datasets of varying size that need to be linked together.\nMalaria Atlas Project: publicly available and up-to-date malaria prevalence and distribution data. Vector distribution, bednet coverage, etc. data also available.\n\nLet Vicki or Mete know if data from any of these resources interests you, or if you would like to discuss where data to answer questions you find interesting live (…or if they exist at all!)."
  },
  {
    "objectID": "tests-assignment-3.html#a-quick-review",
    "href": "tests-assignment-3.html#a-quick-review",
    "title": "12  More on hypothesis testing",
    "section": "12.1 A quick review",
    "text": "12.1 A quick review\nTo conduct a hypothesis test at significance level \\(\\alpha\\), one does the following:\n\nState the null and alternative hypothesis (\\(H_0\\) and \\(H_1\\), respectively) and significance level \\(\\alpha\\). The significance level is the probability of a false positive (rejecting the null hypothesis) given the null hypothesis holds. \\(\\alpha = 0.05\\) is the most common choice of significance level.\nCollect data \\(X_1,\\dots,X_n\\), possibly with knowledge of the sample size \\(n\\) required to achieve a certain power. Power is the probability of a true positive, given the null hypothesis is false.\nCalculate the realized value \\(s\\) of a test statistic \\(S\\) from the data. The test statistic must have a known distribution under the null hypothesis.\nCompute the probability of observing the realized value of the test statistic or something more extreme, given the null hypothesis is true, i.e., \\(p = \\Pr(S &gt; s | H_0)\\). This probability is called a \\(p\\) value. If \\(p &lt; \\alpha\\), we reject the null hypothesis at significance level \\(\\alpha\\) and, if not, we fail to reject \\(H_0\\).\n\nIn the intro to inference lecture, we applied the likelihood ratio test to simulated data. We also discussed \\(t\\) tests (a special case of the LRT). In Assignment 3, you were asked to preform a \\(t\\)-test to examine whether beaver’s body temperature differ by activity level. These are not the only tests you will see in ecology and evolution papers. In these notes, we describe two tests you may see in the literature."
  },
  {
    "objectID": "tests-assignment-3.html#the-chi-square-test",
    "href": "tests-assignment-3.html#the-chi-square-test",
    "title": "12  More on hypothesis testing",
    "section": "12.2 The chi square test",
    "text": "12.2 The chi square test\nSuppose we count the number of individuals in a population that carry an allele at a specific locus. The number of individuals with this allele could be modeled as a realization of Binomial distribution with \\(n\\) trials and success probability \\(p\\), i.e., \\(Y \\sim \\text{Binomial}(n,p)\\). (Binomial random variables count the number of successes in some number of trials; in the context of the example above, if any individual carries the allele of interest, we count that as a success.) A \\(\\chi^2\\) test determines which of the following hypothesis is consistent with the data: \\(H_0 \\colon p = p_0\\) vs \\(H_1 \\colon p \\neq p_0\\). In words, the null hypothesis is that the probability of carrying the allele of interest is \\(p_0\\) and the alternative hypothesis is that is not \\(p_0\\).1 To conduct a test of these hypotheses, one must calculate the value of the test statistic\n\\[S = \\frac{(Y-E(Y))^2}{\\text{Var}(Y)} = \\frac{(Y-np_0)^2}{np_0(1-p_0)}\\] \nusing the observed number of individuals that carry the allele of interest. Under the null hypothesis, \\(S\\) has an approximate \\(\\chi^2\\) distribution one degree of freedom. Simplifying this expression, one can write the test statistic in another way that is commonly used:\n\\[S = \\frac{(Y-np_0)^2}{np_0} + \\frac{((n-Y)-n(1-p_0))^2}{n(1-p_0)},\\] \nwhere the numerator in the first term corresponds to the \\(\\#\\) of individuals carrying the allele (\\(=Y\\)) minus the number of individuals that are expected to carry the allele under the null hypothesis (\\(=np_0)\\). The denominator is the number expected to carry the allele. Similarly, in the second term, we have the number of individuals sampled that do not carry the allele (\\(=n-Y\\)) minus the number of individuals that are expected to not carry the allele (\\(n(1-p_0)\\)).\nIf the probability of observing a test statistic more extreme than\n\\[s = \\frac{(y-np_0)^2}{np_0} + \\frac{((n-y)-n(1-p_0))^2}{n(1-p_0)},\\]\nis \\(&lt; \\alpha\\), then we reject the null hypothesis. (The reason being that we are unlikely to live in a world where the null hypothesis is true if the test statistic does not line up with what is expected under the null hypothesis.) Tables of \\(\\chi^2\\) cutoff values are sometimes used to determine the \\(p\\)-value (i.e., the probability of observing something more extreme than the test statistic that was calculated), but is more convenient to do the calculation in R.\nIn the following code chunk, we test \\(H_0 \\colon p \\neq 0.4\\) vs \\(H_1 \\colon p \\neq 0.4\\) by calculating the test statistic and \\(p\\) value. Data is simulated.\n\nn &lt;- 100 # this is the population (and sample) size\nY &lt;- rbinom(1, size = n, prob = 0.45) # prob is the true value of p in this case!\nY # this is the number individuals in the pop with the allele\n\n[1] 44\n\nexpected_carrying_allele &lt;- 100*0.4 \n# number individuals expected to carry the allele under H0: p = 0.4\n\nexpected_NOT_carrying_allele &lt;- 100*(1-0.4)\n# number individuals expected to NOT carry the allele under H0: p = 0.4\n\nS &lt;- (Y-expected_carrying_allele)^2/expected_carrying_allele +\n  ((n-Y)-expected_NOT_carrying_allele)^2/expected_NOT_carrying_allele\n\npchisq(S, df = 1, lower.tail = F) # since p &gt; 0.05, we fail to reject the null hypothesis\n\n[1] 0.4142162\n\n\nPlay around with this code to see when, depending on \\(p_0\\), the true value of \\(p\\) used to simulate the data, and the sample size, the \\(p\\)-value is \\(&lt; \\alpha = 0.05\\)."
  },
  {
    "objectID": "tests-assignment-3.html#permutation-tests",
    "href": "tests-assignment-3.html#permutation-tests",
    "title": "12  More on hypothesis testing",
    "section": "12.3 Permutation tests",
    "text": "12.3 Permutation tests\nMany ecological and evolutionary questions ask: is the observed pattern different than what we would expect by random chance? Permutation tests (sometimes called randomization tests) allow us to test whether the observed data are different from a random distribution generated by reordering our observed data. If the pattern is random, then it should be just as likely as any other pattern generated by reordering the data. If it is not random, then it should occur more or less frequently than we expect under this distribution. The key step of a permutation test is to generate a null distribution by shuffling the data. In the following example of a permutation test, we shuffle observations between groups to test if the group means are different.\nThe steps for a permutation test of the difference between group means is as follows:\n\nCalculate the value of a test statistic (e.g., the difference in means, \\(t\\) statistic) given data \\(X_1,\\dots,X_n\\) for the first group and \\(Y_1,\\dots,Y_n\\) for the second group.\nRandomly reshuffle observations among the treatment groups, each time calculating a test statistic.\nRepeat step (2) multiple times, generating a distribution of test-statistics.\nCalculate the proportion of times the actual test statistic is outside the distribution of test-statistics.\n\nWe will perform the above steps on simulated data. Imagine we venture to South America and collect 30 male and 30 female Hercules beetles. We brought the beetles back to the lab and measured the width of their bodies at the largest point. The question we are interested in is: Do male and female Hercules beetles differ in body width? We will simulate body width data that is normally distributed. Our test statistic will be the difference in group means.\n\ndf_males &lt;- data.frame(width = rnorm(30, mean=17, sd=3), sex = \"male\")\ndf_females &lt;- data.frame(width = rnorm(n=30, mean=16, sd=2), sex = \"female\")\n\ndf_body_widths &lt;- rbind(df_males, df_females) # combine data!\nhead(df_body_widths)\n\n     width  sex\n1 21.84220 male\n2 16.37205 male\n3 20.53999 male\n4 21.09151 male\n5 18.74234 male\n6 17.91990 male\n\ndf_body_widths %&gt;% ggplot(aes(x = as.numeric(width), \n                              fill = as.factor(sex))) + geom_histogram() +\n  labs(x = \"width\", y = \"count\", fill = \"sex\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nmean_males &lt;- mean(df_males$width)\nmean_females &lt;- mean(df_females$width)\ndiff_means_obs &lt;- mean_males - mean_females # test statistic w/out permutation\ndiff_means_obs\n\n[1] 1.691863\n\nreshuffled &lt;- df_body_widths\nreshuffled$width &lt;- sample(reshuffled$width, size = nrow(reshuffled), replace = F)\nhead(reshuffled)\n\n     width  sex\n1 14.71039 male\n2 17.23578 male\n3 15.24109 male\n4 11.85666 male\n5 14.34533 male\n6 14.79923 male\n\npermute_and_calculate_mean_diff &lt;- function(){\n  reshuffled &lt;- df_body_widths\n  reshuffled$width &lt;- sample(reshuffled$width, size = nrow(reshuffled), replace = F)\n  \n  mean_males_permuted &lt;- mean(reshuffled %&gt;% filter(sex == \"male\") %&gt;% pull(width))\n  mean_females_permuted &lt;- mean(reshuffled %&gt;% filter(sex == \"female\") %&gt;% pull(width))\n  \n  mean_diff_permuted &lt;- mean_males_permuted - mean_females_permuted\n  # test statistic after permutation\n  \n  return(mean_diff_permuted)\n}\n\npermute_and_calculate_mean_diff()\n\n[1] -0.1414876\n\nn_sims &lt;- 1000 # number of times to permute data to generated null distribution\ntest_stats &lt;- c()\n\nfor (i in 1:n_sims){\n  test_stats[i] &lt;- permute_and_calculate_mean_diff()\n}\n\nggplot() + geom_histogram(aes(x = test_stats), fill = \"gray\") +\n  geom_vline(xintercept = diff_means_obs, color = \"red\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n## red line corresponds to value of test statistic from the un-permuted data\n\nFinally, to get our \\(p\\)-value, we calculate the number of times the simulated mean difference exceeded the observed mean difference from our data. Because we are performing a two-tailed test, this amounts to determining the number of times the simulated mean difference is either greater or lesser than the observed difference. We can do this by asking how many times the absolute value of the simulated mean difference is greater or equal to the absolute value of the observed mean difference.\n\nlength(test_stats[(diff_means_obs &lt; test_stats)])/length(test_stats)\n\n[1] 0.012"
  },
  {
    "objectID": "tests-assignment-3.html#footnotes",
    "href": "tests-assignment-3.html#footnotes",
    "title": "12  More on hypothesis testing",
    "section": "",
    "text": "In population genetics, so-called Hardy-Weinberg equilibrium is the null hypothesis we wish to test. This is a model of evolution in which there is no mutation, selection, migration, recombination — really, there is nothing interesting happening. In this case, we have an expectation for the frequency of any allele at a given locus. Given count data, the \\(\\chi^2\\) test allows us to determine if the Hardy-Weinberg equilibrium is a reasonable description of the population and its evolution.↩︎"
  }
]